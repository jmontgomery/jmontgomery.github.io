yaxt = 'n')
}
tic <- c(0, 0.5, 1)
axis(side = 2, tic, labels = T)
legend('topleft', legend = c('Response Function',"Theta Hat"), lty=c(1,1), col=c("black", "blue"))
##Adding a legend and adjusting tic marks
}
return(plotting(raschObj, theta_hat, question))
}
)
raschPlot(demoRasch, T, 3)
current.code <- as.package("easyRasch")
library(MASS)
dgp.func <- function(n, p, q=0){
D = as.numeric(sample(n) <= (n/2)) # Treat half of sample
#For Logit Errors
e <- runif(n, 0, 1) # Generate random uniform(0,1) errors
e <- e/(1-e) # Errors expressed as odds
e <- log(e)*3 # Log odds
X <- rnorm(n) # Draw X random standard normal
latentr <- p*X + q*X*D + p*D + e # Response propensity ~ X + D + X:D
R <- latentr # Store response propensity as R
R[latentr <  1] <- 0 # Recode score < 1 as response
R[latentr >= 1] <- 1 # Recode score >= 1 as non-response
y.star <- 1 + 1*D + .5*X + 1*(D*X) + rnorm(n) # Y ~ D*X
data.sim <- as.data.frame(cbind(X, D, y.star, R)) # Store in data frame
data.sim$Y <- data.sim$y.star # Store responses as Y
data.sim$Y[data.sim$R==1] <- NA # Recode non-response as NA
data.sim$R.tilde <- 1 - data.sim$R # Store R.tilde as indicator where 1 == response
return(data.sim)
}
### Proposition 1 ###
Proposition1 <- function(Data) {
# Calculate response propensity: p(R = 1 | D, X)
pWeights <- glm(R.tilde ~ D*X,
family = binomial(link = logit),
data = Data)$fitted
# Estimate ATE: Y ~ D*X | p(D,X)
Model <- lm(Y ~ D*X, weights = 1/pWeights, data = Data)
return(list(ATE = Model$coefficients[2],
weights = pWeights))
}
### Proposition 2 ###
Proposition2 <- function(Data) {
# Subset data s.t. Response == 1
RespondentData <- Data[Data$R.tilde == 1, ]
# Calculate treatment propensity: p(D = 1 | X, R = 1)
PiWeights <- glm(D ~ X,
family = binomial(link = logit),
data = RespondentData)$fitted
# Replace pi(X) with 1-pi(X) where D == 0
PiWeights[RespondentData$D == 0] <- 1 - PiWeights[RespondentData$D == 0]
# Estimate ATE: Y ~ D | pi(X)
Model <- lm(Y ~ D, weights = 1/PiWeights, data = RespondentData)
return(list(ATE = Model$coefficients[2],
weights = PiWeights))
}
### Proposition 3 ###
Proposition3 <- function(Data) {
# Calculate response propensity: p(R = 1 | D, X)
pWeights <- glm(R.tilde ~ D*X,
family = binomial(link = logit),
data = Data)$fitted.values
# Subset data s.t. Response == 1
RespondentData <- Data[Data$R.tilde == 1, ]
# Calculate treatment propensity: p(D = 1 | X, R = 1)
PiWeights <- glm(D ~ X,
family = binomial(link = logit),
data = RespondentData)$fitted.values
# Replace pi(X) with 1-pi(X) where D == 0
PiWeights[RespondentData$D == 0] <- 1 - PiWeights[RespondentData$D == 0]
# Product of weights
ProductWeights <- pWeights[Data$R.tilde == 1]*PiWeights
# Estimate ATE: Y ~ D | pi(X)
Model <- lm(Y ~ D*X , weights = 1/ProductWeights, data = RespondentData)
return(list(ATE = Model$coefficients[2],
weights = ProductWeights, pWeights = pWeights, PiWeights=PiWeights ))
}
### Simulations for Propositions 1-3 Using dgp.func ###
nsim1 <- 1000 # set number of simulations
out1 <- matrix(NA, nsim1, 6) # empty matrix for estimate comparisons
weights1<-matrix(NA, nsim1, 6)
p.sim1 <- 1
for(i in 1:nsim1){ # iterate over simulations
data.tmp1 <- dgp.func(1000, p.sim1) # simulate data
out1[i,1] <- lm(y.star ~ D + X , data=data.tmp1)$coef[2] # OLS estimate under no attrition
out1[i,2] <- lm(Y ~ D , data=data.tmp1)$coef[2] # naive OLS estimate/No controls
out1[i,3] <- lm(Y ~ D + X, data=data.tmp1)$coef[2] # naive OLS estimate/With controls
thisProp1<-Proposition1(data.tmp1)
thisProp2<-Proposition2(data.tmp1)
thisProp3<-Proposition3(data.tmp1)
out1[i,4] <- thisProp1$ATE # Prop. 1 IPW OLS
out1[i,5] <- thisProp2$ATE # Prop. 2 IPW OLS
out1[i,6] <- thisProp3$ATE # Prop. 3 IPW OLS
weights1[i,1]<-max(thisProp1$weights)
weights1[i,2]<-min(thisProp1$weights)
weights1[i,3]<-max(thisProp2$weights)
weights1[i,4]<-min(thisProp2$weights)
weights1[i,5]<-max(thisProp3$weights)
weights1[i,6]<-min(thisProp3$weights)
rm(list=c("data.tmp1", "thisProp1", "thisProp2", "thisProp3"))
}
##Calculate errors in point estimation for each simulation
ateErrors<-matrix(NA, nrow=nsim1, ncol=5)
for(i in 1:5){
ateErrors[,i]<-out1[, (i+1)]-out1[,1]
}
colnames(ateErrors)<-c("Naive/NC", "Naive/C", "Prop 1.", "Prop 2.", "Prop 3.")
summary(ateErrors)
head(weights1) ## Shows minimum and maximum predicted probabilities
## Via assumption 4' and 5', should not be too close to 0 or 1
## You may need to remove simulations where this is violated
Results1<-summary(ateErrors)
par(mfrow=c(2,1), mar=c(1,2,1,.1), mgp=c(1,0,0), tcl=0)
boxplot(ateErrors, ylab="Bias", main="No attrition induced imballance in X")
abline(h=0)
Results1
#############################################################################
### Simulations for Propositions 1-3 Using dgp.func creating imbalance in X
## across Treatment/Control among non-attritted ###
#############################################################################
nsim2 <- 1000 # set number of simulations
out2 <- matrix(NA, nsim2, 6) # empty matrix for estimate comparisons
weights2<-matrix(NA, nsim2, 6)
p.sim2 <- 1
for(i in 1:nsim2){ # iterate over simulations
data.tmp2 <- dgp.func(1000, p.sim2, q=4) # simulate data
out2[i,1] <- lm(y.star ~ D + X , data=data.tmp2)$coef[2] # OLS estimate under no attrition
out2[i,2] <- lm(Y ~ D , data=data.tmp2)$coef[2] # naive OLS estimate/No controls
out2[i,3] <- lm(Y ~ D + X, data=data.tmp2)$coef[2] # naive OLS estimate/With controls
thisProp1<-Proposition1(data.tmp2)
thisProp2<-Proposition2(data.tmp2)
thisProp3<-Proposition3(data.tmp2)
out2[i,4] <- thisProp1$ATE # Prop. 1 IPW OLS
out2[i,5] <- thisProp2$ATE # Prop. 2 IPW OLS
out2[i,6] <- thisProp3$ATE # Prop. 3 IPW OLS
weights2[i,1]<-max(thisProp1$weights)
weights2[i,2]<-min(thisProp1$weights)
weights2[i,3]<-max(thisProp2$weights)
weights2[i,4]<-min(thisProp2$weights)
weights2[i,5]<-max(thisProp3$weights)
weights2[i,6]<-min(thisProp3$weights)
rm(list=c("data.tmp2", "thisProp1", "thisProp2", "thisProp3"))
}
## Example
dataTemp2<-dgp.func(1000, p.sim2, q=4)
summary(lm(X~D, data=dataTemp2)) # Random assigment
summary(lm(X~D, data=dataTemp2[dataTemp2$R.tilde==1,])) # Loss of randomness/imballance of X in observed
summary(lm(y.star~D, data=dataTemp2)) #True ATE
summary(lm(y.star~D+X, data=dataTemp2[dataTemp2$R.tilde==1,])) #True ATE|R=1
summary(lm(Y~D, data=dataTemp2)[dataTemp2$R.tilde==1,]) # Gives wrong answer
summary(lm(Y~D+X, data=dataTemp2)) # Gives wrong answer for ATE, but correct for ATE|R=1
Proposition3(dataTemp2)$ATE # All fixed
Proposition1(dataTemp2)$ATE # But so did this one
Proposition2(dataTemp2)$ATE # Gives same wrong answer as abvoe
rm(dataTemp2)
##Calculate errors in point estimation for each simulation
ateErrors2<-matrix(NA, nrow=nsim2, ncol=5)
for(i in 1:5){
ateErrors2[,i]<-out2[, (i+1)]-out2[,1]
}
colnames(ateErrors2)<-c("Naive/NC", "Naive/C", "Prop 1.", "Prop 2.", "Prop 3.")
head(weights2) ## Shows minimum and maximum predicted probabilities
## Via assumption 4' and 5', should not be too close to 0 or 1
## You may need to remove simulations where this is violated
Results2<-summary(ateErrors2)
boxplot(ateErrors2, ylab="Bias", main="With attrition induced imballance in X")
abline(h=0)
Results2
install.packages("bookdown")
install.packages("bookdown")
output: bookdown::
library(bookdown)
?bookdown
?gitbook
![Quantiative methods skills?](Graphics/napoleon.jpg)
knit_with_parameters('~/Dropbox/MathCamp/MathCamp2017/02Notation-And-Algebra.Rmd')
unlink('Dropbox/MathCamp/MathCamp2017/02Notation-And-Algebra_cache', recursive = TRUE)
?text
knitr::opts_chunk$set(echo = TRUE)
plot(x=2, y=3, xlim=c(0, 4), ylim=c(0,4), pch=19, cex=3
, xlab="X-axis", ylab="Y-axis", col="darkred")
abline(h=c(0,1,2,3,4),v=c(0,1,2,3,4), lty=2, col="gray40")
text(x=2.2, y=3.2, labels="(2, 3)", cex=1)
plot(x=2, y=3, xlim=c(0, 4), ylim=c(0,4), pch=19, cex=3
, xlab="X-axis", ylab="Y-axis", col="darkred")
abline(h=c(0,1,2,3,4),v=c(0,1,2,3,4), lty=2, col="gray40")
text(x=2.2, y=3.2, labels="(2, 3)", cex=1)
plot(x=c(2,3), y=c(1, 5), xlim=c(2, 3), ylim=c(0,4), pch=19, cex=3
, xlab="X-axis", ylab="Y-axis", col="darkred")
abline(a=-7, b=4)
text(x=c(2.2, 3.2), y=c(1.2, 5.2), labels="(2, 1)", cex=1)
library(png)
library(jpeg)
knitr::include_graphics("escalators.png", auto_pdf = TRUE, dpi = NULL)
str(knitr::include_graphics("escalators.png", auto_pdf = TRUE, dpi = NULL))
a<-(knitr::include_graphics("escalators.png", auto_pdf = TRUE, dpi = NULL)
)
a
str(a)
knitr::include_graphics("escalators.png", auto_pdf = TRUE, dpi = NULL)
unlink('Dropbox/MathCamp/MathCamp2017/03Functions_cache', recursive = TRUE)
library(devtools)
install_github("erossiter/catSurv")
data(warps)
data(warp)
?data
data()
data(ChickWeight)
str(ChickWeights)
data(ChickWeights)
boxplot(weight~Diet)
boxplot(weight~Diet, data=ChickWeight)
ChickWeight$weight[ChickWeight$Diet==1]
meanDiet1<-mean(ChickWeight$weight[ChickWeight$Diet==1])
text(1,meanDiet1, "Mean")
data(ChickWeights)
boxplot(weight~Diet, data=ChickWeight)
meanDiet1<-mean(ChickWeight$weight[ChickWeight$Diet==1])
text(1,meanDiet1, "Mean")
points(1,meanDiet1, col="red", pch=19)
?text
text(1,meanDiet1, "Mean", pos=1)
text(1,meanDiet1, "Mean", pos=2)
data(ChickWeights)
boxplot(weight~Diet, data=ChickWeight)
meanDiet1<-mean(ChickWeight$weight[ChickWeight$Diet==1])
text(1,meanDiet1, "Mean", pos=2)
points(1,meanDiet1, col="red", pch=19)
(63-78)/11
(65-78)/11
(67-78)/11
(63-78)/11
(67-78)/11
pnorm((63-78)/11)
pnorm((63-78)/11, lower.tail=FALSE)
pnorm(63)
pnorm(-1)
pnorm(-1, lower.tail=FALSE)
install.packages("swirl")
library("swirl")
install_course_github("jmontgomery", "QPMSwirl")
swirl()
library("swirl")
install_course_github("jmontgomery", "QPMSwirl")
uninstall_all_courses(force = FALSE)
install_course_github("jmontgomery", "QPMSwirl")
swirl()
uninstall_all_courses(force = FALSE)
install_course_github("jmontgomery", "QPMSwirl")
swirl()
bye()
library("swirl")
install_course_github("jmontgomery", "QPMSwirl")
uninstall_all_courses(force = FALSE)
install_course_github("jmontgomery", "QPMSwirl")
bye()
uninstall_all_courses(force = FALSE)
install_course_github("jmontgomery", "QPMSwirl")
swirl()
bye()
uninstall_all_courses(force = FALSE)
install_course_github("jmontgomery", "QPMSwirl")
swirl()
bye()
uninstall_all_courses(force = FALSE)
install_course_github("jmontgomery", "QPMSwirl")
swirl()
-1
uninstall_all_courses(force = FALSE)
by()
bye()
uninstall_all_courses(force = FALSE)
install_course_github("jmontgomery", "QPMSwirl")
swirl()
bye()
uninstall_all_courses(force = FALSE)
uninstall_all_courses(force = FALSE)
install_course_github("jmontgomery", "QPMSwirl")
swirl()
bye()
install_course_github("jmontgomery", "QPMSwirl")
swirl()
-1
uninstall_all_courses(force = FALSE)
install_course_github("jmontgomery", "QPMSwirl")
swirl()
-1
uninstall_all_courses(force = FALSE)
install_course_github("jmontgomery", "QPMSwirl")
swirl()
-1
library("swirl")
uninstall_all_courses(force = FALSE)
install_course_github("jmontgomery", "QPMSwirl")
swirl()
-1
oos<-read.csv(file = "https://jmontgomery.github.io/index.html")
oos<-read.csv(file = url("https://jmontgomery.github.io/index.html"))
oos<-read.csv(file = url("https://jmontgomery.github.io/index.html"))
oos<-read.csv(file = url("https://jmontgomery.github.io/ProblemSets/incumbents.csv"))
lowess(voteshare~presvote, data=oos)
?lowess
lowess(oos$voteshare~oos$presvote)
myFit<-lowess(oos$voteshare~oos$presvote)
plot(myFit)
myFit<-lowess(oos$voteshare~oos$presvote, delta=.1)
?lowess
myFit<-lowess(oos$voteshare~oos$presvote, delta=1)
?lowess
plot(myFit)
myFit<-lowess(oos$voteshare~oos$presvote, delta=.3)
plot(myFit)
myFit<-lowess(oos$voteshare~oos$presvote, delta=.1)
plot(myFit)
plot(myFit, type="l")
myFit<-lowess(oos$voteshare~oos$presvote, delta=.05, )
plot(myFit, type="l")
points(oos$voteshare, oos$presvote)
points(oos$presvote, oos$voteshare)
myFit<-lowess(oos$voteshare~oos$presvote, delta=.05, )
plot(myFit, type="l")
points(oos$presvote, oos$voteshare)
points(oos$presvote, oos$voteshare, col=rgb(1,1,1, alpha=.1))
points(oos$presvote, oos$voteshare, col=rgb(1,1,1, alpha=.99))
plot(oos$presvote, oos$voteshare, col=rgb(1,1,1, alpha=.99))
points(myFit, type="l")
myFit<-lowess(oos$voteshare~oos$presvote, delta=.05, )
plot(oos$presvote, oos$voteshare, col=rgb(1,1,1, alpha=.99))
points(myFit, type="l")
plot(oos$presvote, oos$voteshare, col=rgb(1,1,1, alpha=.9))
points(myFit, type="l")
plot(oos$presvote, oos$voteshare, col=rgb(1,1,1, alpha=.1))
plot(oos$presvote, oos$voteshare)
points(myFit, type="l")
plot(oos$presvote, oos$voteshare, col=rgb(1,1,1, alpha=2))
plot(oos$presvote, oos$voteshare, col=rgb(1,1,1, alpha=.1))
plot(oos$presvote, oos$voteshare, col=rgb(1,1,1, alpha=.9))
plot(oos$presvote, oos$voteshare, col=rgb(1,0,0, alpha=.9))
plot(oos$presvote, oos$voteshare, col=rgb(1,0,0, alpha=.1))
points(myFit, type="l")
myFit<-lowess(oos$voteshare~oos$presvote, delta=.1, )
myFit<-lowess(oos$voteshare~oos$presvote, delta=.1 )
plot(oos$presvote, oos$voteshare, col=rgb(1,0,0, alpha=.1))
points(myFit, type="l")
myFit<-lowess(oos$voteshare~oos$presvote, delta=.2 )
plot(oos$presvote, oos$voteshare, col=rgb(1,0,0, alpha=.1))
points(myFit, type="l")
pnorm(.14)
pnorm(.14, lower.tail=false)
pt(.14, lower.tail=F, df=25)
pt(.14, df=25)
pt(.14, lower.tail=F, df=25)
library(data.table)
library(bit64)
library(bigtabulate)
data<-fread("~/Downloads/FinalPosts-pairwise.csv",
colClasses = c("factor", "factor"))
tempdata<-data
mytab1<-bigtable(tempdata, ccols=c(2))
dim(mytab1)
sum(mytab1==1)
which(mytab1==1)
head(mytab1)
levels(data$UserID)
levels(data$UserID)[3]
levels(data$UserID)[which(mytab1==1)]
singletons<-levels(data$UserID)[which(mytab1==1)]
which(data$UserID %in% singletons)
forDropping<-which(data$UserID %in% singletons)
length(forDropping)
table(mytab1==1)
reduced.data<-data[-forDropping,]
dim(reduced.data)
mytab1<-bigtable(reduced.data, ccols=c(1,2))
projected<-mytab1%*%t(mytab1)
dim(reduced.data)
dim(projected)
diagonal<-diag(projected)
for(i in 1:length(diagonal)){
projected[i,]<-projected[i,]/diagonal[i]
}
projected<-t(projected)
factors<-prcomp(projected)
plot(factors$rotation[,1], factors$rotation[,2])
str(factors)
plot(factors$rotation[,1], factors$rotation[,3])
plot(factors$rotation[,2], factors$rotation[,3])
plot(factors$rotation[,2], factors$rotation[,1])
plot(density(factors$rotation[,2]))
plot(density(factors$rotation[,2]))
plot(factors$rotation[,2], factors$rotation[,1])
projected[1:2, 3:4]
projected[1:2, 1:2]
labels(projected)
labels(projected)[1]
dim(labels(projected)[1])
length(labels(projected)[1])
labels(projected)[1][1]
labels(projected)
temp<-labels(projected)
temp<-labels(projected)$1
temp<-labels(projected)[[1]]
temp<-labels(mytab1)
temp<-labels(mytab1)[[2]]
temp<-labels(mytab1)[[1]]
colnames(projecte)<-rownames(projected)<-
levels(data$PostID)
levels(data$PostID)
length(levels(data$PostID))
dim(projected)
temp<-as.numeric(labels(mytab1)[[1]])
levels(data$PostID)[as.numeric(labels(mytab1)[[1]])]
colnames(projecte)<-rownames(projected)<-levels(data$PostID)[as.numeric(labels(mytab1)[[1]])]
colnames(projected)<-rownames(projected)<-levels(data$PostID)[as.numeric(labels(mytab1)[[1]])]
write.csv(projected, file="~/Dropbox/Eli/Affiliation.csv")
?prcomp
?princomp
?prcomp
factors<-princomp(projected)
2+2
diagonal<-diag(projected)
# Divide by the elements of the diganol to standardize
for(i in 1:length(diagonal)){
projected[i,]<-projected[i,]/diagonal[i]
}
dim(factors$loadings)
class(factors$loadings)
factors$loadings[1,]
factors$loadings[,1:10]
class(factors$loadings[,1:10])
factors<-princomp(projected)
factors2<-pricompe(t(projected))
factors2<-pricomp(t(projected)) # Loadings with rotating
factors2<-princomp(t(projected)) # Loadings with rotating
firstTenUnrotatedfactors<-factors$loadings[,1:10]
firstTenRotatedfactors<-factors2$loadings[,1:10]
write.csv(projected, file="~/Dropbox/Eli/FactorScoresUntransposed.csv")
write.csv(projected, file="~/Dropbox/Eli/FactorScoresTransposed.csv")
### Read in the needed libraries
library(data.table)
library(bit64)
library(bigtabulate)
## Read in the data
data<-fread("~/Downloads/FinalPosts-pairwise.csv",
colClasses = c("factor", "factor"))
tempdata<-data # ignore this
mytab1<-bigtable(tempdata, ccols=c(2)) ### Make the giant contingency table of posts/ids
table(mytab1==1) ## This shows me how many users only liked one posts
### This code removes all FB ids for people who only liked one post
singletons<-levels(data$UserID)[which(mytab1==1)]
forDropping<-which(data$UserID %in% singletons)
reduced.data<-data[-forDropping,]
## Make the somewhat less gian contingency table
mytab1<-bigtable(reduced.data, ccols=c(1,2))
## Make the projection matrix for col-likes
projected<-mytab1%*%t(mytab1)
dim(projected)
colnames(projected)<-rownames(projected)<-levels(data$PostID)[as.numeric(labels(mytab1)[[1]])]
write.csv(projected, file="~/Dropbox/Eli/Affiliation.csv")
## Now we are going to do the factor analysis for you
# get the diagonal
diagonal<-diag(projected)
# Divide by the elements of the diganol to standardize
for(i in 1:length(diagonal)){
projected[i,]<-projected[i,]/diagonal[i]
}
## Extract rthe factor scores and write them out for you
factors<-princomp(projected) # loadings without rotating
factors2<-princomp(t(projected)) # Loadings with rotating
firstTenUnrotatedfactors<-factors$loadings[,1:10]
firstTenRotatedfactors<-factors2$loadings[,1:10]
write.csv(firstTenUnrotatedfactors, file="~/Dropbox/Eli/FactorScoresUntransposed.csv")
write.csv(firstTenRotatedfactors, file="~/Dropbox/Eli/FactorScoresTransposed.csv")
library(data.table)
library(bit64)
library(bigtabulate)
data<-fread("~/Dropbox/Eli/FinalPosts-pairwise.csv",
colClasses = c("factor", "factor"))
tempdata<-data # ignore this
mytab1<-bigtable(tempdata, ccols=c(2)) ### Make the giant contingency table of posts/ids
table(mytab1==1) ## This shows me how many users only liked one posts
### This code removes all FB ids for people who only liked one post
singletons<-levels(data$UserID)[which(mytab1==1)]
forDropping<-which(data$UserID %in% singletons)
reduced.data<-data[-forDropping,]
## Make the somewhat less gian contingency table
mytab1<-bigtable(reduced.data, ccols=c(1,2))
## Make the projection matrix for col-likes
projected<-mytab1%*%t(mytab1)
dim(projected)
colnames(projected)<-rownames(projected)<-levels(data$PostID)[as.numeric(labels(mytab1)[[1]])]
write.csv(projected, file="~/Dropbox/Eli/Affiliation.csv")
## Now we are going to do the factor analysis for you
# get the diagonal
diagonal<-diag(projected)
# Divide by the elements of the diganol to standardize
for(i in 1:length(diagonal)){
projected[i,]<-projected[i,]/diagonal[i]
}
## Extract rthe factor scores and write them out for you
factors<-princomp(projected) # loadings without rotating
factors2<-princomp(t(projected)) # Loadings with rotating
firstTenUnrotatedfactors<-factors$loadings[,1:10]
firstTenRotatedfactors<-factors2$loadings[,1:10]
write.csv(firstTenUnrotatedfactors, file="~/Dropbox/Eli/FactorScoresUntransposed.csv")
write.csv(firstTenRotatedfactors, file="~/Dropbox/Eli/FactorScoresTransposed.csv")
library("bookdown")
setwd("~/Github/jmontgomery.github.io/PS363BookMarkdownFiles")
setwd("~/Documents/jmontgomery.github.io/PS363BookMarkdownFiles")
?render_book
bookdown::render_book(input="index.Rmd", output_dir="~/Github/jmontgomery.github.io/PS363Coursebook")
library(AER)
install.packages("AER")
library(AER)
library(AER)
?ivreg
?CigarettesSW
data("CigarettesSW", package = "AER")
CigarettesSW$rprice <- with(CigarettesSW, price/cpi)
CigarettesSW$rincome <- with(CigarettesSW, income/population/cpi)
CigarettesSW$tdiff <- with(CigarettesSW, (taxs - tax)/cpi)
## model
fm <- ivreg(log(packs) ~ log(rprice) + log(rincome) | log(rincome) + tdiff + I(tax/cpi),
data = CigarettesSW, subset = year == "1995")
summary(fm)
lm1 <- lm(log(packs) ~ log(rprice) + log(rincome)),
data = CigarettesSW, subset = year == "1995")
lm1 <- lm(log(packs) ~ log(rprice) + log(rincome),
data = CigarettesSW, subset = year == "1995")
summary(lm1)
fm <- ivreg(log(packs) ~ log(rprice) + log(rincome) | log(rincome) + tdiff + I(tax/cpi),
data = CigarettesSW, subset = year == "1995")
summary(fm)
lm1 <- lm(log(packs) ~ log(rprice) + log(rincome) + tdiff,
data = CigarettesSW, subset = year == "1995")
summary(lm1)
lm1 <- lm(log(packs) ~ log(rprice) + log(rincome) ,
data = CigarettesSW, subset = year == "1995")
summary(lm1)
fm <- ivreg(log(packs) ~ log(rprice) + log(rincome) | log(rincome) + tdiff + I(tax/cpi),
data = CigarettesSW, subset = year == "1995")
summary(fm)
?ivreg
?CigarettesSW
data("CigarettesSW", package = "AER")
CigarettesSW$rincome <- with(CigarettesSW, income/population/cpi)
CigarettesSW$tdiff <- with(CigarettesSW, (taxs - tax)/cpi)
fm <- ivreg(log(packs) ~ log(rprice) + log(rincome) | log(rincome) + tdiff + I(tax/cpi),
data = CigarettesSW, subset = year == "1995")
summary(fm)
?lm
lm1 <- lm(log(packs) ~ log(rprice) + log(rincome) ,
data = CigarettesSW, subset = year == "1995")
summary(lm1)
e
exp
exp(1)
x<-c(53, 57, 58, 63, 66, 67, 67, 68, 69)
y<-c(1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1)
length(x)
length(y)
x<-c(53, 57, 58, 63, 66, 67, 67, 68, 69, 70, 70, 70)
length(y)
length(x)
x<-c(53, 57, 58, 63, 66, 67, 67, 68, 69, 70, 70, 70, 70, 70)
length(x)
length(y)
mle.fit<-glm(y~x, family=binmial)
mle.fit<-glm(y~x, family=binomial)
summary(mle.fit)
mle.fit$coe
mle.fit$coefficients
b.mle<-mle.fit$coefficients[2]
mle.fit$cov.scaled
summary(mle.fit)
summary(mle.fit)$cov.scaled
var.beta.mle<-summary(mle.fit)$cov.scaled[2,2]
beta.mle<-mle.fit$coefficients[2]
var.beta.mle<-summary(mle.fit)$cov.scaled[2,2]
alpha.mle<-mle.fit$coefficients[1]
b.hat<-exp(alpha.mle+0.577216)
draw.proposal<-function(){
alpha<-log(rexp(1, 1/b.hat))
beta<-rnorm(1, beta.mle, sqrt(var.beta.mle))
return(c(alpha, beta))
}
draw.proposal()
## Evaluating proposal density
pdf.proposal<-function(theta){
alpha<-theta[1]; beta<-theta[2]
p<-1-(1/(1+exp(alpha+beta*x)))
lik<-exp(sum(dbinom(y,size=1, prob=p, log=TRUE)))
dprior<-exp(alpha)*exp(-1*exp(a)/b.hat)/b.hat
}
pdf.proposal(15, -0.27)
pdf.proposal(c(15, -0.27))
pdf.proposal<-function(theta){
alpha<-theta[1]; beta<-theta[2]
p<-1-(1/(1+exp(alpha+beta*x)))
lik<-exp(sum(dbinom(y,size=1, prob=p, log=TRUE)))
dprior<-exp(alpha)*exp(-1*exp(alpha)/b.hat)/b.hat
}
pdf.proposal(c(15, -0.27))
## Evaluating proposal density
pdf.proposal<-function(theta){
alpha<-theta[1]; beta<-theta[2]
p<-1-(1/(1+exp(alpha+beta*x)))
lik<-exp(sum(dbinom(y,size=1, prob=p, log=TRUE)))
dprior<-exp(alpha)*exp(-1*exp(alpha)/b.hat)/b.hat
return(dprior)
}
pdf.proposal(c(15, -0.27))
pdf.proposal<-function(theta){
alpha<-theta[1]; beta<-theta[2]
pr1<-exp(alpha)*exp(-1*exp(alpha)/b.hat)/b.hat
pr2<-dnorm(beta, beta.mle, sqrt(var.beta.mle))
return(dprior)
}
pdf.proposal(c(15, -0.27))
## Evaluating proposal density
pdf.proposal<-function(theta){
alpha<-theta[1]; beta<-theta[2]
pr1<-exp(alpha)*exp(-1*exp(alpha)/b.hat)/b.hat
pr2<-dnorm(beta, beta.mle, sqrt(var.beta.mle))
return(pr1*pr2)
}
pdf.proposal(c(15, -0.27))
pdf.proposal(c(15, -0.27))
pdf.posterior(c(15, -0.27))
pdf.posterior<-function(theta){
alpha<-theta[1]; beta<-theta[2]
p<-1-(1/(1+exp(alpha+beta*x)))
lik<-exp(sum(dbinom(y,size=1, prob=p, log=TRUE)))
dprior<-exp(alpha)*exp(-1*exp(alpha)/b.hat)/b.hat
return(lik*dprior)
}
pdf.posterior(c(15, -0.27))
pdf.posterior(c(1, -0.27))
alpha.mle
pdf.posterior(c(18, -0.27))
mle.fit<-glm(y~x, family=binomial(logit))
beta.mle<-mle.fit$coefficients[2]
alpha.mle<-mle.fit$coefficients[1]
var.beta.mle<-summary(mle.fit)$cov.scaled[2,2]
b.hat<-exp(alpha.mle+0.577216)
