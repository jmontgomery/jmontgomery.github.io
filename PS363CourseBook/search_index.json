[
["index.html", "Quantitative Political Methods Coursebook 1 Class Overview 1.1 Learning objective #1: What is the point of all of this group work? 1.2 Learning object #2: What is the point of all of these online activites? 1.3 Learning objective #3: How is this going to work?", " Quantitative Political Methods Coursebook Jacob Montgomery September 24, 2017 1 Class Overview 1.1 Learning objective #1: What is the point of all of this group work? This year, this course has been revised to incorporate some elements of Team Based Learning. This is an approach to creating permanent groups to work on problems in class, in lab, and out of class throughout the semester. We are not going to follow the TBL approach strictly. But we are going to have fewer lectures and more structured group activities. Learn more about Team Based Learning by watching this video. 1.2 Learning object #2: What is the point of all of these online activites? A better question would be what is the point of sitting in a room and watching a professor talk? It’s 2017! Moving an increasing amount of teaching resources online is a big trend in education at all levels. But this isn’t just trendy, it appears to be very effective. Preliminary research shows that online learning paired with in-class interactions with faculty is a much more effective than the traditional lecture format. Watch this short TED talk by Peter Norvig to learn more: 1.3 Learning objective #3: How is this going to work? The approach I am trying to adopt for this class is to help you learn in four basic steps. Initial exposure to materials through self-study. Repeated exposure to materials in short lectures. Use your new knowledge in a collaborative environment where assistance is readily available. Use your new knowledge on your own. The goal is for you to see and use information in multiple ways to improve learning outcomes. Experience (and research) shows that this approach is far superior to simple lectures in helping you learn more and retain it longer. To help you along this process, each learning component will be broken down as follows. You will read your book and review online materials. Regular online quizzes will help motivate you to stay current. I will begin most classes with a short lecture where I will cover the materials again and answer questions. You will apply your knowledge during in-class team assignments. If you have any questions or run into problems, I will be right there to give you help. To keep the team motivated, these assignments will be graded. You will apply your knowledge on your own (or with friends) in your problem sets. "],
["measurement-error.html", "2 Measurement Error 2.1 Learning Objectives 2.2 Learning Objective 1: Types of Biases asdf 2.3 What’s the takeaway?", " 2 Measurement Error 2.1 Learning Objectives Learn to define the following types of errors: sampling bias, response bias, non-response bias 2.2 Learning Objective 1: Types of Biases asdf If the goal of a survey is to be representative, then anything that causes a survey to deviate from the population is a major problem. Something that causes a survey result to deviate from the population is referred to as bias. Bias can be introduced in a number of ways. We will divide biases into three categories: sampling bias, response bias, and non-response bias. 2.2.1 Sampling bias Sampling bias occurs when there is a problem in the sampling process. Sometimes, the sample is poorly conducted. For example, in a survey of WashU undergraduates, if we put together a stratified sample, based on residential area, and then selected 50% of our subjects from Shepley Hall, then our survey would not be reflective of the general WashU undergraduate population, because 50% of WashU students do not live in Shepley hall. This is a sampling error. To give a political example, in telephone surveys, sometimes people do not pick up their phone. In this situation, most polling firms will try calling back later. If a firm does not try to call back when people are out, then they run the risk of introducing sampling error, because younger people are more likely to be out than older people, and younger people are more likely to be liberal. Other times, a sample simply isn’t random. Certain groups are systematically left out of a sample. This is called a selection bias, or selection effect. In this case, the sample will fail to reflect the population as a whole. Although this may sound like an obvious problem, this happens all the time. For example, hosts at various cable news shows sometimes ask viewers to text in what they think about a certain policy, and then display the results as if they are reflective of the general population. But just because 93% of people who texted in think that the President should be re-elected does not mean that the population as a whole agrees. Since it is not a randome sample, it doesn’t even mean that 93% of viewers agree! A common situation where sampling error occurs involves phone lines. People without phone lines are excluded from most surveys. That may not be too big of an issue in a country like the United States, where there may not be a substantial difference between people with phone lines, and people without them. But in a country like Afghanistan relying on phone calls to conduct a survey would be a very big problem since phones are only owned by (relatively) wealthy individuals and some areas have no service at all! Another example is Geico commercials, where they say that people who switch to Geico on average save money. This is a selection bias, because only people who are offered a cheaper rate will switch. If people were randomly selected and forced to switch to Geico, then they would not be saving 30% or more. 2.2.2 Response bias In some surveys, the questions may be poorly worded. The language may be ambiguous, or may be open to different interpretations. So the process of answering the questions may lead to some bias in how the questions are answered. In some surveys, something about the survey itself causes subjects to respond differently than they would in real life. For example, if someone is being surveyed on how often they volunteer, and (s)he rarely volunteers, (s)he may feel embarrassed or guilty about this fact, and tell the interviewer that (s)he volunteers frequently. This is called the social desirability effect. For example, if a survey asks someone if they voted in the last election, and they had not, that person may be more inclined to answer untruthfully, because failing to vote is frowned upon. Other times, the ordering of questions might affect responses. For example, if a survey begins by asking subjects numerous questions about terrorism, and then asks them to list the issues most important to them, more subjects might list terrorism than would be the case if the survey had not first asked numerous questions about it. This is referred to as order effect. An example of order effect occurred in a survey in 1948 that asked, Q1: Do you think the US should let Communist newspaper reporters from other countries come in here and send back to their papers the news as they see it? But half of the people were first asked, Q2: Do you think a communist country like Russia should let American newspaper reporters come in and send back to America the news as they see it? Among people who were first asked the question about American reporters (Q2), 73.1% said yes to Q1. But, among people who did not first answer Q2, only 36.5% said yes to Q1. 2.2.3 Non-response bias People don’t always respond to surveys. When people who do not respond to surveys differ in some systematic way from people who do respond, then the sample fails to be representative of the general population. For example, students who submit course evaluations tend to do so because they either disliked their professor, or loved their professor. In a class, students who do not have strong feelings one way or another are less likely to submit an evaluation, so the evaluation results are likely to indicate that the class is more polarized than it actually is. Note: Non-response bias is not the same as selection bias. Selection bias has to do with how the sample was constructed by the researcher. Non-response bias has to do with how individuals chosen to be in the sample behave. 2.3 What’s the takeaway? There are several different ways that a survey can fail to reflect the population. The sample itself could not be reflective of the population, which would be a sampling bias. Sometimes, something about the survey could prompt people to give answers that don’t reflect what they actually believe or do, which is a response bias. Other times, the people who are selected, but don’t respond differ in an important way from people who do respond, causing the results of the survey to be skewed. This is called a non-response bias. "],
["scales-of-measurement.html", "3 Scales of Measurement 3.1 Learning Objectives: 3.2 Learning Objective 1: Scales 3.3 Summary: 3.4 Learning Objective 2: Granularity 3.5 What is granularity? 3.6 What are the takeaways?", " 3 Scales of Measurement 3.1 Learning Objectives: Learn what ordinal, nominal, and interval scales are. Be able to distinguish between them. Learn what “granularity” means, learn what the two types of granularity, continuous and discrete, mean, and learn how to distinguish them 3.2 Learning Objective 1: Scales 3.2.1 First of all, what do we mean by scales? A “scale” is just the way we measure or quantify a variable. There are three different scales with which data can be measured: nominal, ordinal, or interval. 3.2.2 Nominal Scale Variables measured on a nominal scale can be separated into different categories, but they have no natural ordering. We can’t say that one data point is “more something” than another point. One example is conenent. It is possible to put countries into different categories based on the continent they’re in. The reason that continent is a nominal variable is that, while we can put countries into different categories, there is no universal, objective way of putting them along a dimension. Note: Sometimes variables measured on a nominal scale are called “categorical” data. 3.2.3 Ordinal Scale: Variables measured on an ordinal scale have a natural ordering, but no natural distances between them. We can put the data points in a conceptual line, but there isn’t a precise (or meaningful) distance between observed values. Many survey questions measure opinion on an ordinal scale. For example, in 2012 The Pew research Center asked: Is your overall opinion of Barack Obama very favorable, mostly favorable, mostly unfavorable, or very unfavorable? We can be pretty sure that people who respondend “very unfavorable” approved less of President Obama than people who responded “mostly unfavorable.” But how much less? We don’t know exactly, which is why this variable is measured on an ordinal scale. (You can see many more examples of question wordings to measure approval Preisdent Obama here.) 3.2.4 Interval Scale: Variables measured on an interval scale have both a clear ordering and the difference between numbers has a clear meaning. We can not only put data points in a line, we can also position them within precise distances from each other. An easy example is height. We can order everyone in QPM in a line, from tallest to shortest, and we can say precisely how much taller (or shorter) one person is than the others. 3.2.5 Ratio scale: Ratio scale is a subset of Interval data. In a ratio scale, the zero value signifies that there is none of that variable. Most scales in social science are ratio scales. For example, if we were to measure the proportion of the time a legislator votes with the leadership of a certain party, then a score of 0% means that that legislator cast no votes with the party leadership. An example of a scale that is not measured on a ratio scale is temperature measured in Fahrenheit or Celsius. If a room is 0 degrees Fahrenheit or Celsius, that does not mean that there is no warmth. 3.2.6 Focus on the measurement and not the concept Certain variables, when measured in different ways, can be measured with different scales. For example, think about political ideology. One way to measure the ideology of a member of Congress would be to ask them if they are “Very liberal, somewhat liberal, somewhat conservative, or very conservative.” This would have an ordinal scale. However, there are other ways to measure ideology. For example, we could measure the proportion of votes each member cast that were consistent with the Republican leadership. This would allow us to make a scale of ideology that is interval. (We might find that Nancy Pelosi’s voting record is 70.2% more liberal than John Boehner’s.) Below is the link to Congressional rankings released by Americans for Democratic Action, a left-wing political group, and rankings released by the Club for Growth, and right-wing political group. http://www.adaction.org/media/votingrecords/2010.pdf http://www.clubforgrowth.org/projects/ 3.3 Summary: Watch this video for a very nice summary of these concepts and some thoughts on how best to plot them. 3.4 Learning Objective 2: Granularity 3.5 What is granularity? Granularity refers to how answers fit on a scale. If a variable can take on any value along a scale, it is continuous. An example of a continuous variable is the proportion of the vote a candidate receives in an election. A candidate can receive 0%, 100%, or any value in between. If a variable can only take on certain (countable) values, it is discrete. All nominal data are discrete. Consider the following survey question, asked in a poll by Rusmussen Reports (full survey here): A proposal has been made to repeal the health care law and stop it from going into effect. Do you strongly favor, somewhat favor, somewhat oppose or strongly oppose a proposal to repeal the health care law? In the above question, the results would be discrete, because there are only 5 possible answers. Respondents can strongly favor, somewhat favor, some oppose, or strongly oppose the proposal. There is no possible answer between strongly favor or somewhat favor, for example. Here is a video that can help you tell the difference between discrete and continuous data. 3.6 What are the takeaways? Based on whether data have a natural order or natural distance, they can be measured on a nominal, ordinal, or interval scale. A specific type of interval scale is a ratio scale. Some variables, measured in different ways, can potentially be measured on several types of scales. Based on the space between answers, or granularity, scales can either be continuous, if variables can take on any value on the scale, or discrete, if they can only take on certain values. This video will help you keep this all straight. "],
["random-samples.html", "4 Random Samples 4.1 Learning Objectives 4.2 Learning Objective 1: Random Samples 4.3 Learning Objective 2: Types of Random Samples 4.4 What are the takeaways?", " 4 Random Samples 4.1 Learning Objectives 1)Understand what a random sample is, and why random samples are important. 2)Identify simple random samples, systematic random samples, stratified random samples, cluster samples, and multi-stage sampling, and understand why scientists use them. 4.2 Learning Objective 1: Random Samples 4.2.1 What is a random sample? A random sample is a sample where every subject in the population in question has an equal chance of being selected. If there are 6,000 undergrads at WashU, and I want to select 600 to be in my sample, then if my method of selecting subject is random, each undergrad has the same chance of being selected, 10%. 4.2.2 Why do we care if a sample is random? We want our samples to be representative. In other words, we want our sample to look like the population we’re drawing from, just smaller. If we are conducting a study on how happy they are with their dorm, and 10% of WashU undergrads live in a “traditional” dorm on the South 40, then we want 10% of our sample to be comprised of students living in a traditional dorm on the South 40. More technically, random samples are representative in expectation. That means that if we took a lot of different random samples, on average they would be representative. So while any one random sample might be off just a bit, random samples in general will be representative. 4.2.3 Real-life application: Don’t look ignorant in your newspaper column Below is the link to an article by Rodger Simon, Politico’s chief political columnist. In this article, he dismisses polls because they ask so few people what they think. This is a perfectly reasonable argument to make if one has had no background in statistics. What Roger Simon failed to grasp, and the reason why polls work, even though they ask such few people, is that, when they are done well, they are representative of the population as a whole. http://www.politico.com/news/stories/1211/70717.html 4.3 Learning Objective 2: Types of Random Samples There are several different types of random samples. In each of these samples, every subject in the population has the same probability of being selected, but the subjects are selected in different ways. 4.3.1 Simple Random Sample: This is the most basic method of sampling. A certain number of subjects are chosen randomly out of a population. An example of this is lotteries. In a lottery, each number has the same chance of being selected. Here’s a helpful video that explains simple random samples: If simple random samples are representative, why not just use them for all samples? The problem is that it is extremely difficult, if not impossible, to put together a simple random sample for large populations. One challenge is that certain groups have different response rates than other groups. In this case, if we just do a simple random sample, then groups that have a higher response rate will be over-represented. Another challenge is that there is no directory with the names and contact information of every single person, so it can be difficult to put together a sample in the first place. Scientists employ more complex sampling methods to remedy these problems. 4.3.2 Systematic Random Sample: In a systematic sample, subjects are selected through some sort of system. For example, I might collect the student id numbers of all WashU undergrads, order them from least to greatest, and then start with the 8th number, and sample every 10th person. This is still a random sample, because each person has the same chance of being selected. It’s not, however, a simple random sample, because instead of being selected entirely randomly in no order, there was a system to choosing the people. In a simple random sample, every set of people has the same chance of being in the sample. In a systematic random sample, that’s not true. For example, if my student ID number is 1 higher than my roommate’s, then the in a simple random sample it’s possible that both my roommate and I will be in the sample. In the systematic random sample I just described, however, it is impossible that my roommate and I will be in the same sample. Everyone in the sample has a student id number that’s 10 apart. Here’s a helpful video that explains systematic random samples: 4.3.3 Stratified Random Sample: In a Stratified Random Sample, we first split the population into different groups, or strata. We then select a set number of subjects from each group. The goal is to set up the samples such that the proportion of subjects in the sample from each strata match the proportion of subjects from each strata in the population. To return to the example of sampling the undergraduate population of WashU, we might divide undergrads into different strata based on their residential area. If 5% of all WashU undergrads live in Mudd Hall, and we want our sample to be 600 people, then 5% of those 600 people, or 30 people, will be from Mudd Hall. To achieve this, we would do a simple random sample of Mudd residents, choosing 30 subjects. And we would do this for each residential area. Here’s a helpful video that explains stratified random sampling: What are the advantages of a stratified random sample? Stratified Random Samples can be used to remedy problems of over-sampling, or under-sampling. If different groups have different rates of response, then we can select more subjects from those groups, so the set of people who respond are representative. For example, if freshmen respond much less to surveys than upper-classmen, then we might use a stratified random sample, and over-sample freshmen, to correct for this. 4.3.4 Cluster Samples: In a cluster sample, the population is divided into different clusters. These are organic groupings of people. A simple random sample is run to select several clusters, and then everyone in each selected cluster is surveyed. For example, in the WashU undergrad survey, we might “cluster” students by floor. Wheeler 1 would be a cluster, Lopata 3 another, and so on. We would randomly select a few floors, and interview everyone on each floor. Here’s a helpful video that explains cluster sampling: What are the advantages of a cluster sample? In some ways, cluster samples are easier to execute. If many of the people we are interviewing live close together, it might be easier to reach them. Of course, cluster sampling is not without its drawbacks. Because subjects in the same cluster tend to be similar in certain systematic ways, the cluster sampling can lead to over-sampling of certain types of people. 4.3.5 Multi-Stage Sampling: A multi-stage sample employs several types of random samples. For example, for the undergrad survey, we might first stratify WashU undergrad by whether they live on the South 40, in the Village, or off campus. We might then do a cluster sample within each strata. We could choose 4 floors from each strata, and interview each person on that floor. 4.4 What are the takeaways? Random sampling is the way scientists develop representative samples. There are several different ways of putting together random samples. These include cluster samples, stratified random samples, systematic samples, and multi-stage samples, which combine aspects of different sampling methods. "],
["installing-r-and-r-studio.html", "5 Installing R and R Studio 5.1 Installing R and R Studio on a Mac (would also work for other platforms) 5.2 Installing R and R Studio on Windows", " 5 Installing R and R Studio 5.1 Installing R and R Studio on a Mac (would also work for other platforms) 5.2 Installing R and R Studio on Windows "],
["a-brief-introduction-to-r.html", "6 A Brief Introduction to R", " 6 A Brief Introduction to R Learning objectives: 1)Learn how to assign a value to an object in R. 2)Learn your way around R (using R Studio). 3)Learn how to do simple arithmetic in R. "],
["importing-data-into-r.html", "7 Importing data into R 7.1 Learning Objectives: 7.2 Importing Data 7.3 Let’s go through that all again 7.4 Now let’s practice a bit", " 7 Importing data into R 7.1 Learning Objectives: Learn how to import data into R. Learn how to look at the data. 7.2 Importing Data There are many different kinds of data: there are comma-separated-value files (.csv), text files (.txt), STATA files (.dta), SPSS files (.spss), and some others. We will work primarily with .csv files and some .dta files in this course. In order to import data, we need to make sure we set the correct working directory. After downloading the data, make sure the R working directory is set to the location you saved the data. Then once you are working in the correct directory, we can read csv files in by using the “read.csv” function. We’ll try this with some energy data. Go to: (https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ARKOTI) and download PESenergy.csv. This is data on TV news coverage about energy policy used in this study. 7.2.1 Telling R where to look The first thing you need to do, is make sure R is looking in the right place for the file. We can use the getwd This shows you R’s current working directory getwd() The output from this command always tells you what directory R is looking in for files. To change where R is looking, we will use the setwd command. setwd(&quot;/Users/yourname/Downloads&quot;) You will have to change the argument inside the parentheses to point to the right folder on your computer. You can use the dir to list out all of the files in your working directory. dir() 7.2.2 Reading in your first data file Once you have got R looking in the right place, now you can load the data using the read.csv command. data &lt;- read.csv(&quot;PESenergy.csv&quot;) This imports PESenergy.csv into a data frame in R. Once we have the data in R, we can manipulate in a number of ways. Experiment with the code below: head(energyData) tail(energyData) summary(energyData) table(energyData$Energy) energyData[c(10:20),c(&quot;Date&quot;,&quot;Energy&quot;,&quot;rmn1173&quot;)] 7.2.3 Summarizing the variables in your dataset The function summary creates a summary table but for all of the variables. summary(energyData) The function summary creates a summary table but for all of the variables. We can also look at just one variable by specifing the column name of a variable like this: summary(energyData$Approval) 7.2.4 Adding a variable to your data Let’s creates a new variable, which is exactly 0.01 times the Approval variable. energyData$approvalProp &lt;- energyData$Approval*0.01 We can see this more clearly if we run the summary of our new variable, the output should be exactly 0.01 times the ouptut of the previous summary table: summary(energyData$approvalProp) 7.3 Let’s go through that all again 7.4 Now let’s practice a bit Follow this link to datacamp.com.. You just need to complete the “Read TXT” and “Read CSV” files. But note that there is also a lesson using the XLConnect package for reading in Excel files. Students who prefer working with data in Excel will want to review this material. "],
["visualizing-data-in-r.html", "8 Visualizing Data in R 8.1 Let’s start with a couple of good videos 8.2 Basic visulualization 8.3 Try a bit more on your own.", " 8 Visualizing Data in R Learning Objectives: Learn how to create simple plots in R. Learn how to interpret the results. 8.1 Let’s start with a couple of good videos 8.2 Basic visulualization Now go back to the PSenergy data we used in the last chapter. Follow those instructions to open the data as an object called energyData. 8.2.1 Making your first plots We use R to create many different kinds of plots: hist(energyData$Energy, xlab=&quot;Television Stories&quot;, main=&quot;Title&quot;) boxplot(energyData$Energy, ylab=&quot;Television Stories&quot;, main=&quot;Title&quot;) plot(y=energyData$Energy, x=energyData$Approval,ylab=&quot;Television Stories&quot;,xlab=&quot;Presidential Approval&quot;) plot(x=energyData$Energy, ylab=&quot;Television Stories&quot;,xlab=&quot;Month-Year&quot;,type=&quot;l&quot;) 8.2.2 Saving your plot You can save your plot by re-creating it within a “pdf device.” In essence, R will create the plot as a new file rather than on your screen. library(lattice) getwd() pdf(&quot;stories_over_time.pdf&quot;) densityplot(energyData$Energy, xlab=&quot;Television Stories&quot;) dev.off() Do not forget the dev.off or the file will never be rendered. 8.3 Try a bit more on your own. Complete chapter 1 of the datacamp course on data visualization in R. "],
["measures-of-position.html", "9 Measures of Position 9.1 Percentile 9.2 Interquartile Range 9.3 Outliers 9.4 Skew", " 9 Measures of Position Learning Objectives: Understand and the terms percentale, interquartile range, outliers and skew. 9.1 Percentile The nth percentile means that n% of observations fall below (or are equal to) n, and (100-n)% fall above n. For example, if you score in the 90th percentile on a test, that means that 90% of students who took the test earned the same or a lower score than you, and 10% scored higher than you. The percentile tells us a lot about a specific observation within a data set because it gives us its relative position. The median is the 50th percentile because exactly 50% of observations fall above the median, and 50% of observations fall below the median. 9.2 Interquartile Range This way of looking at data splits observations into four parts (four quarters). The Interquartile Range (IQR) determines the middle 50% of the data (the middle two quarters). IQR= (75th percentile-25th percentile) The 25th percentile is the lower quartile (25% of data falls below the 25th percentile) and the 75th percentile is the upper quartile (25% of data falls above the 75th percentile). A good way of finding these observations in an ordered set of data is to determine the median, then find the median from the highest data point to the median, and the lowest data point to the median. For example, if you had the data 1 2 3 4 5, you could determine by counting in 3 numbers from each side that the median is 3. You could then find the median of 1 2 3 and of 3 4 5. That will tell you that 2 is the value of the lower quartile, and 4 is the value of the upper quartile. 4-2= 2, which means that the IQR= 2. 9.3 Outliers An outlier is an observation whose value is so much greater or lesser than the other observations that it can be considered an extreme. For example, if you had the data 1 2 3 4 8000, you can easily tell that 8000 is an outlier because it is so much larger than the other values. Although there are many definitions, one common definition of an outlier is any observation that falls 1.5(IQR) above the upper quartile, or 1.5(IQR) below the lower quartile. In our above data set, we can determine that the IQR=2. 1.5(2)=3. This means that any value less than -1 or greater than 7 is an outlier, so 8000 is clearly an outlier. 9.4 Skew The skew tells us the tendency of the data. To determine skew, you can look at a histogram of the data, or compare the median and the mean of a set of data. If the longer tail of the data is to the left of the mode, the data is skewed to the left. We call this a negative skew. If that longer tail of the data is to the right, the data is skewed to the right; a positive skew. In comparing the median and the mean, if the mean is greater than the median, the data is positively skewed, and if the mean is smaller than the median, the data is negatively skewed. We can compare these two measures of center because the mean is more sensitive to outliers than the median. For an easy way to remember skew, check out this video. You’ll never forget it again. "],
["measures-of-central-tendency.html", "10 Measures of Central Tendency 10.1 Learning objective 1: Mean 10.2 Learning objective 2: Median 10.3 Learning objective 3: Mode 10.4 Another look:", " 10 Measures of Central Tendency Learning Objectives: Learn what a mean is and how to calculate one. Learn what a median is and how to calculate one. Learn what a mode is and how to calculate one. Understand how all of alternative measures differ and why. 10.1 Learning objective 1: Mean $x_1 $ indicates a single observation, or data point. \\(\\bar{x}\\) represents the mean of all the observations, which is taken by adding up all the observations and dividing them by the number of observations. We call this the arithmetic mean, or average. More formally, the equation for the mean is: \\[\\bar{x} = \\frac{\\sum_i^n x_i }{n} \\] The mean is used only for quantitative data and interval data. (For example, if you were to ask respondents on a survey to circle their preferred baseball team, the Cardinals, Cubs, Reds, or Pirates, you couldn’t then add up the teams and divide by four.) The mean is highly influenced by outliers, which means that for skewed distributions of data, the mean lies in the direction of the skew. The trimmed mean is a way of correcting for the effect of outliers on the mean. It involves cutting out a certain percentage of the data from both extremes. For example, a 10% trimmed mean involves throwing out the top 10% and bottom 10% of findings, then calculating the mean from the remaining middle 80% of the data. 10.2 Learning objective 2: Median The median is a measure of central tendency found by determining the middle value of a set of data. The value of the median is the 50th percentile because 50% of observations fall above the value of the median, and 50% of observations fall below the median. The median it is the middle of an ordered sequence. For example, if we look at the numbers 1 2 3 4 5, the middle of that ordered sequence is 3. If we have an even number of observations, say 1 2 3 4 5 6, we take the mean of the two middle numbers. So the median here is equal to 3.5. The median is used for quantitative, ordinal or interval data. For symmetric distributions (e.g., a normal bell curve), the mean and the median are identical. However, the median is not affected by outliers like the mean is, so if a distribution of data is highly skewed, the median tends to be the preferred measure of center over the mean. This image shows three graphs two skewed and one symmetrical bell curve. Note the way the mean is easily influenced by the skew of the graph, how it is pulled towards the longer tail of a distribution. Comparing the mean and the median is a good way to determine the skew of data without actually making a histogram. Source: http://experimentaltheology.blogspot.com/2012/03/central-tendency-in-skewed.html 10.3 Learning objective 3: Mode The mode is a measure of central tendency that is the value that occurs most frequently. The mode is the most common data point. For example, if you collected the data 1 2 2 3 4, the mode is the number 2. Imagine a bar graph of the data. The mode is whichever bar is the tallest. For symmetric distributions, the mean, median, and mode are all the same value. When data is bimodal, which means that there are two distinct values that tend to occur more frequently (in a histogram, this would mean there are two lumps in the graph), it can indicate polarization around two extremes. An example of a bimodal distribution is shown bewlow. This histogram shows the change in polarization in the public between 1993 and 2014 10.4 Another look: For more information about measures of center, check out this video: "],
["measures-of-dispersion.html", "11 Measures of Dispersion 11.1 Learning objective 1: Range 11.2 Learning objective 2: Deviations 11.3 Learning objective 3: Variance 11.4 Learning objective 4: Standard deviation 11.5 Learning objective 5: The Empirical Rule 11.6 Review that all again", " 11 Measures of Dispersion Learning Objectives: Understand how to calculate the range of a variable and how to calculate it. Understand and be able to calculate the deviation of specific data point. Understand how to calculate the variance of variable. Understand how to calculate the standard deviation of a variable, and its relationship to the variance. Understand the “Empirical Rule” and how it is related to the standard deviation. 11.1 Learning objective 1: Range The range is a simple measure of variability that shows how spread out the observations in a data set are. Range= (largest observation - smallest observation). Because the range is a measure of distance, the value of the range is always positive. The range really doesn’t tell us anything much that is useful. 11.2 Learning objective 2: Deviations A deviation is a measure of distance from a measure of center, \\(|x_1 - \\bar{x}|\\). The observation may be greater or lesser than the value of the center, but because deviation is a measure of distance, its value is always positive. 11.3 Learning objective 3: Variance Variance is the average of squared deviations. The value of the variance is a squared value (for instance, inches squared, percentage squared, children squared) which does not necessarily tell us very clearly about how spread out the data is from the mean. Variance is calculated by dividing the sum of squared deviations from the value of the population by \\(n-1\\). This is the equation: \\[s^2=\\frac{\\sum_i^n (x_i - \\bar{x})^2}{n-1} \\] When variance increases it means the observations are more spread out from the mean. 11.4 Learning objective 4: Standard deviation The standard deviation is the square root of the variance. \\[s=\\sqrt{s^2}=\\sqrt{\\frac{\\sum_i^n (x_i -\\bar{x})^2}{n-1}}\\] Like the variance, the standard deviation is always a positive number because it represents a distance from the mean. When x is a constant (for example, you have the observations 3 3 3 3 3), the standard deviation is 0 because there is no deviation from the mean. The standard deviation increases as variability increases around the mean. NOTE: The standard deviation is greatly affected by outliers. 11.5 Learning objective 5: The Empirical Rule The Empirical Rule is the distribution of observations is the histogram of the data is approximately bell shaped; if it is a normal, symmetrical distribution. The Rule says that about 68% of observations ball between \\(\\bar{x}-s\\) and \\(\\bar{x}+s\\); that about 95% of observations fall between \\(\\bar{x}-2s\\) and \\(\\bar{x}+2s\\); and that nearly all the data in a normal distribution will fall between \\(\\bar{x}-3s\\) and \\(\\bar{x}+3s\\). The Empirical Rule only works if the data is approximately bell shaped. It does not apply to data that is skewed. 11.6 Review that all again For more information about measures of dispersion, check out this video: "],
["frequency-and-probability-distributions.html", "12 Frequency and Probability Distributions 12.1 Learning objectives 12.2 What is a probability? 12.3 Frequency distribution", " 12 Frequency and Probability Distributions 12.1 Learning objectives Learning Objectives: Define probability. Understand the basic properties of a probability. Understand the meaning of “frequency distribution.” 12.2 What is a probability? There are many definitions of a probability, but here is one that is easy to understand and that is pretty useful for this class is the frequency interpretation of probability. In essence, we imagine doing something (flipping a coin, rolling a dice, taking a random sample). Probability (frequency interpretation) The relative frequency of occurrence for some particular outcome if a process is repeated a large number of times under similar conditions. Probabilities help us answer questions like: If I flip a coin three times, what is the probability that I will get exactly two heads? If I roll two dice, what is the probability of getting a two? If I take a random sample of 100 Wash U students, what is the probability that less than 40% of the sample will be male? If you are asked to find the probability that the variable \\(Y\\) will take on a specific value such as 3, probabilities can be written in several ways. \\[Pr(Y=3) \\mbox{ or }P(Y=3)\\mbox{ or } P(3)\\mbox{ or } Pr(3)\\] All of these are asking you to figure out the probability that the variable \\(Y\\) will take on the value of 3. In other cases you may see a probability written a more generic form, where the specific data point is replaced by a letter. Often this will be written in one of several ways depending on the math book you are looking at. \\[Pr(Y=y), Pr(Y), P(Y=y), \\mbox{ or } P(Y)\\] 12.3 Frequency distribution A probability distribution of a discrete variable, Y, assigns a probability to each possible outcome. As an example, we can write out the frequency distribution for all possible outcomes of flipping two coints. Outcome #Heads Probability TT 0 \\(\\frac{1}{4}\\) TH 1 \\(\\frac{1}{4}\\) HT 1 \\(\\frac{1}{4}\\) HH 2 \\(\\frac{1}{4}\\) In the left column, is the outcome. In the right column is the probability of observing that outcome. So, if we want to represent the probability of getting zero heads, one head, or two heads, we would end up with the frequency distribution: Y Probability 0 \\(\\frac{1}{4}\\) 1 \\(\\frac{1}{2}\\) 2 \\(\\frac{1}{4}\\) Here \\(Y\\) is the random variable of interest. We can think of this table a mapping between inputs (Y) and outputs (probabilities). In more formal notation, this table represents a function, which we can call \\(P(Y=y)\\) or just \\(P(y)\\). This function is what we call a frequency distribution. 12.3.1 Probability and sets We are going to formalize this kind of thinking another step and define something callled a set, \\(S= \\{y_1, y_2, \\ldots, y_k\\}\\), which means that S represents the set of all possible outcomes. In our example, the set of possible outcomes is \\(S=\\{0,1,2\\}\\). 12.3.2 What makes a frequency distribution special? We have defined our frequency distribution \\(P(Y)\\) and the set of possible outcomes \\(S\\). Now we can turn to thinking about what makes a frequency distribution special. First, for all possible outcomes \\(0 \\le P(y \\in S) \\le 1\\). This looks complicated, but just means that the probability of something happen can’t be negative and can’t be greater than 1. It makes no sense to say that something is 110% likely to happen and it makes no sense to say that something has a negative chance of happening. Second, (remembering from above that \\(S= \\{y_1, y_2, \\ldots, y_k\\}\\) \\[\\sum_{k=1}^K p(y_k )=1.\\] This means that the sum of all the probabilities that an event will occur equals 1. 12.3.3 External resources Here are some videos to help: This last video walks you through the construction of a probability density function. "],
["some-basic-plotting-in-r.html", "13 Some basic plotting in R", " 13 Some basic plotting in R We have made some videos to help you do some basic plots. These were made especially to help you with the problem set. How to plot a normal curve This video shows how to include multiple plots in the same figure. This shows you how to import some data and make a scatter plot. This shows you how to include multiple sets of data on the same plot. This shows you how to put two histograms on the same plot and gives you an example of how to subset data. "],
["sampling-distributions-pt-1.html", "14 Sampling Distributions, Pt. 1 14.1 Let’s review 14.2 What is a sampling distribution? 14.3 How is a sampling distribution different from the sample distribution or the population distribution? 14.4 What are the takeaways?", " 14 Sampling Distributions, Pt. 1 Learning Objectives: Understand what a sampling distribution is, and how it is different from a sample distribution, or the population distribution. Understand the difference between a standard deviation and a sandard error. 14.1 Let’s review Thus far, we’ve dealt primarily with two distributions: samples, and populations, with samples being a subset of the population. Often times, it can be difficult to know the exact parameters of a population, so we use samples to estimate them. But samples are not perfect. Even a good random sample may have some error. That is, the statistics calculated from the sample may not exactly match up with the population parameters. So, for instance, the sample mean may not exactly match the population mean. We want to know how much error there will likely be in our sample statistics. To do this, we use the sampling distribution. The sampling distribution uses probability theory to describe how sample statistics will vary. For example, imagine we are running a study on the average household income in the United States. It would be infeasible to collect data for every single household, so we decide to do a survey. Let’s say we decide that our survey will have 2,000 households. We know that the sample mean might be slightly different from the actual population mean. But how different might it be? What is the probability that your sample will be off by 1,000? By 3,000? By 10,000? To answer this question, we use the sampling distribution, which tells us the expected results of a sample if we were to (theoretically) collect a sample of that size many, many times. 14.2 What is a sampling distribution? A sampling tells us how the sample statistics would be distributed if we took a random sample from the poppulation many times. In other words, if you were to draw large samples from a population repeatedly, and graph the resulting sample means, the sampling distribution is what that graph would look like. In the samples we’ve been dealing with so far, the standard deviation is a measure of how far individuals in a population tend to be from the population mean. It is a measure of how spread out individuals are in the population. In a sampling distribution, the standard error is a measure of how far a sample mean or proportion tends to be from the true population mean or proportion. (NOTE: Read this paragraph a few times and make sure you understand the difference. DO NOT CONFUSE STANDARD DEVIATIONS WITH STANDARD ERRORS.) To distinguish between samples, sampling distributions, and populations, it may help to think of this using college terms. Below are the college equivalents to these terms. Populations: Population: Students at Wash U Population parameter (\\(\\mu\\)): Average GPA at Wash U Sample: the students in one course Sample mean (\\(\\bar{x}\\)): The mean GPA in that one course Sampling distribution: the hypothetical distribution of mean course GPAs for all classes at Wash U. Standard error: a measure of spread between a hypothetical mean course GPA and the school GPA Below is a short video describing a sampling distribution. (Don’t worry about the exact formulas yet. Just try and get the concept.) 14.3 How is a sampling distribution different from the sample distribution or the population distribution? The distribution of the sample (sometimes confusingly called the sample distribution) is the distribution of a single sample. In our example of household income, each point in the sample distribution represents one household that we surveyed. The sampling distribution shows the results of repeated samples, and each point in the sampling distribution represents one sample mean. The population distribution (if we could ever see it, which we cannot) shows every single possible data point in the population, and each data point represents one household. 14.4 What are the takeaways? A sampling distribution is the theoretical distribution of sample statistics. Each point on the sampling distribution represents a sample statistic. The standard error the sampling statistic is an indicator of how far a sample statistic is likely to be from the actual population parameter. "],
["sampling-distributions-pt-2.html", "15 Sampling Distributions, Pt. 2 15.1 Let’s review 15.2 How do we know what the shape of a sampling distribution is when the sample size is large? 15.3 Take a step back 15.4 How do we know what the mean of a sampling distribution is? 15.5 How do we know what the standard error is? 15.6 Some notes on notation 15.7 What’s the takeaway?", " 15 Sampling Distributions, Pt. 2 Learning Objectives Be able to calculate the parameters of the sampling distribution for a sample mean in two circumstances: The population variance is known The population variance must be estimated from our data Understand the notation for the center and spread of the population distribution, sampling distribution, and sample distribution 15.1 Let’s review A sampling distribution is the theoretical distribution of sample statistics. We use sampling distributions to figure out how close a statistic calculated from a sample (e.g., a sample mean) is likely to be to the population parameter we’re concerned with. Each point on a sampling distribution represents a statistic calcualated from a different sample. 15.2 How do we know what the shape of a sampling distribution is when the sample size is large? Due to a rule known as the Central Limit Theorem, when the sample sizes are sufficiently large, the resulting sampling distribution will be normally distributed, regardless the population distribution. The videos below offer a good explanation of the Central Limit Theorem and how they relate to calculating sampling distributions. Seriously . watch them. No really. Don’t jump ahead. Don’t skip videos. Don’t fast forward. Watch these all the way through.If you find something confusing, watch it twice. This is probably the most important concept we will cover in the entire class. 15.3 Take a step back For this class you need to really (REALLY!) need understand this, so pay attention. When the sample size is large, the sampling statistic will be distributed normally no matter how the population is distributed. This fact comes from the Central Limit Theorem and is the basis for many of the statistical tests we will cover. In some other circumstances (that we will discuss in great detail later) the sampling distribution of a sample statistic may be a t-distribution or a binomial distribution (and there are a few others). Probably the most confusing thing about this class for many students is understanding what a sampling distribution is and figuring out which sampling distribution to use for different situations. So do yourself a favor. If you are still a little hazy about what a sampling distribution is, go and watch those videos again, come to office hours, or find some other resource. 15.4 How do we know what the mean of a sampling distribution is? So far, we’ve figured out what the shape of a sampling distribution is when the sample size is large, but we have not yet figured out what the center of the distribution is. When we are looking at the sampling distribution of sample means, then the mean of the sampling distribution is the mean of the population, \\(\\mu_\\bar{x} = \\mu\\). Pretty easy huh? But it’s a bit confusing to say. You may be thinking now, “But we don’t know the population mean, \\(\\mu\\)!” You are right. So we are going to estimate the mean of the sampling distribution as \\(\\hat{\\mu}_\\bar{x} = \\bar{x}\\). That is, our best guess for the mean of the sampling distribution is the mean of the sample distribution. Again, this is pretty easy but can be confusing to keep straight. 15.5 How do we know what the standard error is? The tricky part of finding a sampling distribution is calculating the standard error. 15.5.1 If we know the population variance: To calculate the exact standard error for means, we use the population standard deviation. The formula to calculate the standard error for means is given by: \\[\\sigma_\\bar{x} = \\frac{\\sigma}{\\sqrt{n}}\\] 15.5.2 If we don’t know the population variance AND the sample size is large: But what happens when we don’t know the population standard deviation, which, in all likelihood is the case? In that event, we estimate the population variance (and hence the variance of the sampling distribution) using the sample standard deviation. Our estimate of the population variance is: \\[\\hat{\\sigma}^2= \\text{s}^{2}\\] Therefore, our estimate of the standard error (the square root of the variance) of the sampling distribution is:. \\[\\hat{\\sigma}_\\bar{x}=\\frac{\\hat{\\sigma}}{\\sqrt{n}}=\\text{s}\\sqrt{\\frac{1}{n}}\\] You should be able to prove this to yourself using just the the two equations above. Try it for yourself, just to make sure you understand. Ask about it in class if you cannot quite get it. 15.6 Some notes on notation There are a lot of moving parts when it comes to samples, populations, and sampling distributions, so it is important to keep all the symbols straight. There are a few broad rules when it comes to assigning symbols. When there is a hat (\\(\\hat{\\mu}, \\hat{\\sigma}, \\hat{\\pi}\\)), these sample statistics that we are using to estimate some population parameter. Alphabetic letters (\\(\\bar{x}\\), \\(s\\), \\(p\\), \\(n\\)) are sample statistics that we calculated from our sample. These will often correspond to the “hat” notation above. Greek letters without hats or subscripts, (\\(\\mu, \\sigma, \\pi\\)) are parameters of the population distribution. Greek letters with subscripts (\\(\\mu_\\bar{x}, \\sigma_\\bar{x}\\), etc.) are parameters of the sampling distribution. Below is a table of notations for samples, sampling distributions, and populations when we’re dealing with means. Distribution: Mean: Standard deviation: Estimated mean (using our sample): Estimated standard deviation (using our sample) Sample distribution \\(\\bar{x}\\) \\(s\\) N/A N/A Sampling distribution \\(\\mu_{\\bar{x}}\\) \\(\\sigma_\\bar{x}\\) (Called standard error) \\(\\hat{\\mu}_{\\bar{x}}\\) \\(\\hat{\\sigma}_{\\bar{x}}\\) Population distribution \\(\\mu\\) \\(\\sigma\\) \\(\\hat{\\mu}\\) \\(\\hat{\\sigma}\\) 15.7 What’s the takeaway? To calculate the shape of the sampling distribution, we look at the size of the sample, and statistics calculated from the sample distribution. The center of the population distribution is the center of the sampling distribution, and is estimated as the mean of the sample distribution, \\(\\bar{x}\\). The standard deviation of the sampling distribution, known as the standard error, is calculated according to the formulas above. Sometimes we know the population standard deviation (\\(\\sigma\\)), and we use that to calculate the standard error. \\[\\sigma_{\\bar{x}}=\\frac{\\sigma}{\\sqrt{n}}\\] Sometiems use the sample standard deviation to estimate the population standard deviation, when we don’t know the population variance. For the sampling distribution of sample means, the standard error is then: \\[\\sigma_{\\bar{x}}=\\text{s}\\sqrt{\\frac{1}{n}}\\] "],
["working-with-the-normal-distribution.html", "16 Working with the Normal Distribution 16.1 The Standard Normal Distribution 16.2 Z-scores 16.3 What’s the point of the z-score? 16.4 Examples", " 16 Working with the Normal Distribution Learning Objectives Understand the characteristics of a standard normal distribution, and how it relates to any normal distribution. Learn how to find the area under the normal curve for any interval. 16.1 The Standard Normal Distribution The standard normal distribution is a normal distribution with a mean of \\(\\mu=0\\) and a standard deviation of \\(\\sigma=1\\). NOTE: I am going to provide some examples of how to plot the normal distribution. These are here to provide some examples of how to plot it, but the code itself is not critical for understanding this chapter. x&lt;-seq(from=-5, to=5, by=.1) y&lt;-dnorm(x, mean=0, sd=1) plot(x,y, type=&quot;l&quot;, main=&quot;The standard normal distribution&quot;) Any normal distributions can be transformed to standard normal distributions using the following formula: \\[Z = \\frac{x-\\mu}{\\sigma}.\\] That is, if \\(x \\sim N(\\mu, \\sigma)\\) and we apply the formula above we will have a new variable \\(z\\) that corresponds to the standard normal distribution. 16.1.1 Example The probability \\(P(X&lt;x)\\) for a normal probability distribution in R is given by the pnorm() function. So assume we have a normal distribution with \\(\\mu=5\\), and \\(\\sigma=2\\) (which means that \\(\\sigma^2=4\\)). The are to the left \\(X=6\\) for the normal curve is found by coding: pnorm(6, mean=5, sd=2) ## [1] 0.6914625 We can look at this visually as the shaded area in the plot below: x&lt;-seq(from=-2, to=12, by=.1) y&lt;-dnorm(x, mean=5, sd=2) plot(x,y, type=&quot;l&quot;, main=&quot;P(X&lt;6) ; X~N(5,2)&quot;) ### The code below is for shading in an area under the curve cord.x &lt;- c(-2,seq(-3,6,0.01),6) cord.y &lt;- c(0,dnorm(seq(-3,6,0.01), mean=5, sd=2), 0) polygon(cord.x,cord.y,col=&#39;skyblue&#39;) But could get this same number by calculating a new value: \\[Z=\\frac{6-5}{2}=0.5\\] We can then look up the probability on the standard normal distribution pnorm(.5, mean=0, sd=1) ## [1] 0.6914625 Again we can see this visually by looking at: x&lt;-seq(from=-4, to=4, by=.1) y&lt;-dnorm(x, mean=0, sd=1) plot(x,y, type=&quot;l&quot;, main=&quot;P(X&lt;.5) ; X~N(0,1)&quot;) ### The code below is for shading in an area under the curve cord.x &lt;- c(-4,seq(-4,0.5,0.01),0.5) cord.y &lt;- c(0,dnorm(seq(-4,0.5,0.01), mean=0, sd=1), 0) polygon(cord.x,cord.y,col=&#39;skyblue&#39;) Since the arguments mean=0 and sd=1 are the default settings for the pnorm function, we can get the same answer by just coding: pnorm(.5) ## [1] 0.6914625 16.2 Z-scores A z-score is what we obtain after we apply the above equation. So, for istance, if we are looking at a normal distribution with \\(\\mu = 2\\) and \\(\\sigma=2\\) we can calculate the z-score of the point \\(x=4\\) as \\[Z = \\frac{x-\\mu}{\\sigma}=\\frac{4-2}{2} = 1.\\] A z-score of 1 means that the the point \\(x=4\\) is on standard deviation above the mean. Likewise, a z-score of 2.4 would mean that \\(x\\) is 2.4 standard deviations above the mean. 16.3 What’s the point of the z-score? Once a z score has been calculated, it is very easy to calculate probabilities (areas under the curve) for any normal distribution just using a table for the standard normal distribution. If a dataset follows a normal distribution, then about 68% of the observations will fall within one standard deviation of the mean, about 95% of the observations will fall within two standard deviations of the mean, and about 99.7% of the observations will fall within 3 standard deviations of the mean.(Remember the empirical rule? If not, go back and review.) An explanation of an example problem is posted below in the examples section. But first watch this video. It is a bit long, but this is something you really need to be good at for the test. So it is totally worth your time. Click here to see the table we will use in class so you can follow along with the examples. (There are some more examples below). 16.4 Examples Below are some examples showing how to use the standard normal distribution and the z table in order to solve for probabilities.You should use the Agresti table when you try these example problems. This is what you will be using on the rest of the exams and homeworks for this course-so pay attention. 16.4.1 Example 1: Finding the area to the left, when \\(x&lt;\\mu\\) For a normal distribution with \\(\\mu = 55\\) and \\(\\sigma = 3,\\) find the probability that an observation falls at or below the value 52.75. In R we could just type in pnorm(52.75, mean=55, sd=3) ## [1] 0.2266274 x&lt;-seq(from=48, to=62, by=.1) y&lt;-dnorm(x, mean=55, sd=3) plot(x,y, type=&quot;l&quot;, main=&quot;P(X&lt;52.75) ; X~N(55,3)&quot;) ### The code below is for shading in an area under the curve cord.x &lt;- c(48,seq(48,52.75,0.01),52.75) cord.y &lt;- c(0,dnorm(seq(48,52.75,0.01) , mean=55, sd=3), 0) polygon(cord.x,cord.y,col=&#39;skyblue&#39;) To find the z-score, plug in the mean and standard deviation into \\(Z = (x-\\mu)/\\sigma\\) The variables describing this normal distribution will be converted into a z score that can will used as part of a standard normal distribution \\[P(x \\le 52.75)=P(z \\le (52.75-55)/3))=P(z \\le -0.75)\\] x&lt;-seq(from=-4, to=4, by=.1) y&lt;-dnorm(x, mean=0, sd=1) plot(x,y, type=&quot;l&quot;, main=&quot;P(X&lt;-0.75) ; X~N(0,1)&quot;) ### The code below is for shading in an area under the curve cord.x &lt;- c(-4,seq(-4,-0.75,0.01),-0.75) cord.y &lt;- c(0,dnorm(seq(-4,-0.75,0.01), mean=0, sd=1), 0) polygon(cord.x,cord.y,col=&#39;skyblue&#39;) Now, we use our calculated z score in conjunction with the z table above to find our probability. However, the table does not include negative values for z because the table is for the area to the right. So what should we do? The key is to remember that the standard normal distribution is symmetric around 0. This means that \\(P(z \\le -0.75)=P(z \\ge 0.75)\\). x&lt;-seq(from=-4, to=4, by=.1) y&lt;-dnorm(x, mean=0, sd=1) plot(x,y, type=&quot;l&quot;, main=&quot;P(X &gt; 0.75) ; X~N(0,1)&quot;) ### The code below is for shading in an area under the curve cord.x &lt;- c(0.75,seq(0.75, 4,0.01),4) cord.y &lt;- c(0,dnorm(seq(0.75,4,0.01), mean=0, sd=1), 0) polygon(cord.x,cord.y,col=&#39;skyblue&#39;) Notice the change from “less than” to “greater than.” Since the distribution is symmetric, we can multiply the equation by -1 to get a value for the side of the table that we want. After doing so, our equation changes from \\(P(z \\le -0.75)\\) to \\(P(z \\ge 0.75)\\). Next we find the z score we just calculated on the corresponding z score table above. The first decimal place holds a 7; the 2nd decimal place holds a 5. When we locate the .7 (row 7 column 1) and then move over to the .05 column, we find the corresponding probability to be about 0.227. Remember that this probability refers to the area below the curve to the right of the z score we calculated (as shown by the graphic above the table). Lucky for us, this is exactly what the equation tells us to find \\((P(z\\ge0.75))\\). Our answer, therefore, for \\(P(x \\le 52.75)\\) = 0.227. 16.4.2 Example 2: Finding the area to the right when \\(x \\le \\mu\\). For a normal distribution with \\(\\mu = 55\\) and \\(\\sigma = 3\\), find the probability that an observation falls at or above the value of 52.75. In R we could just type in pnorm(52.75, mean=55, sd=3, lower.tail=FALSE) ## [1] 0.7733726 Noe that we had to use the lower.tail=FALSE argument, which means we want to measure the area under the curve to the right. This corresponds to the following plot: x&lt;-seq(from=48, to=62, by=.1) y&lt;-dnorm(x, mean=55, sd=3) plot(x,y, type=&quot;l&quot;, main=&quot;P(X&gt;52.75) ; X~N(55,3)&quot;) ### The code below is for shading in an area under the curve cord.x &lt;- c(52.75,seq(52.75,63,0.01), 63) cord.y &lt;- c(0,dnorm(seq(52.75,63,0.01) , mean=55, sd=3), 0) polygon(cord.x,cord.y,col=&#39;skyblue&#39;) To find the z-score, plug in the mean and standard deviation into \\(Z = (x-\\mu)/\\sigma\\). The variables describing this normal distribution can be converted into a z score that can be used as part of a standard normal distribution \\[P(x \\ge 52.75) = P(z\\ge(52.75-55)/3)) = P(z \\ge -0.75)\\] We are looking for an area to the right, just like the table expects, but there are no negative values on our table, so what should we do? Again, we have to alter our equation so that we have a can use our z score in conjunction with the z table. The key is to realize that \\(P(z\\ge-0.75) = 1-P(z\\le-0.75)\\). This is true because the total area under the normal curve always sums to 1. So, for any point x, the area to the right is always one minus the area to the left. If this doesn’t make sense, draw yourself a picture. So to complete this problem, all we need to do is find \\(P(z\\le -0.75)\\). But this is exactly what we found in the last problem! So we find the area to the right of positive 0.75 just like before, which is 0.227. The probability value we find is the same as the above value: 0.227. However, note that this value designates the probability to the right of the z score-and we want probability to the left of the z score. In this case, we take (1-) the probability to the right of the z score to get the probability to the left of the z score. So 1 - 0.227 gives us our answer: 0.773. 16.4.3 Example 3: Finding the point given an area For a normal distribution with \\(\\mu = 55\\) and \\(\\sigma = 3\\), I want to find the point where the area under the curve to the right is equal to 0.0985. In R, we can use the qnorm() function like this: qnorm(0.0985, mean=55, sd=3, lower.tail=FALSE) ## [1] 58.87044 Note again we used the lower.tail=FALSE argument because we are looking for the area to the right. But what if you don’t have R? Don’t panic. The first thing to realize is that this is basically just an algebra problem. You are pretty good at Algebra, right? First, set up the equation based on the formulas below. \\[P(Z&gt;\\frac{x-\\mu}{\\sigma})=0.0985 \\rightarrow P(Z&gt;\\frac{x-55}{3})=0.0985\\] So this looks bad. It looks like there is only one equation above, but there are two unknowns, \\(Z\\) and \\(x\\). The trick is to realize that we can use the table to find the correct value of \\(Z\\). For these kinds of problems, we just need to look around the Z-table until we find a value that is as close as possible to 0.0985. Somtimes, you will have to guesstimate when the exact number you are looking for isn’t there. In this case, the number is right there in the row that starts with 1.2 in the column labled .09. This means that the z-value we plug into the equation above is 1.29! \\[P(1.29&gt;\\frac{x-55}{3})=0.0985\\] In fact, now that we have found the value of \\(z\\), we don’t even need most of this formula. All we need to do is solve the equation below for \\(x\\). \\[1.29 =\\frac{x-55}{3}\\] So, after a few easy steps with algebra, we see that \\[3\\times1.29=x-55\\] \\[3\\times1.29+55=x\\] \\[x=58.87.\\] "],
["the-binomial-distribution.html", "17 The Binomial Distribution 17.1 The Binomial Distribution 17.2 Calculating probabilities using a table", " 17 The Binomial Distribution Learning Objectives Understand the characteristics of a binomial distribution and how it relates to a normal distribution. Understanding how to look up probabilities for the binomial distribution on a table. 17.1 The Binomial Distribution A binomial random variable is discrete, not continuous. This means that there are only two possible outcomes (examples: heads/tails coin toss, win/lose football game, support Trump/Not Trump). In general, there are certain requirements that must hold for the binomial distribution to be applicable. n repeated identical independent trials two outcomes (success/failure) P (success) + P(failure) must be equal to 1 The binomial function for calculating the probability distribution of the binomial for any probability p and number of trials n is as follows: \\[Pr (x = k) = {n \\choose k} p^k(1-p)^{n-k}\\] The mean and the variance of the binomial distribution are as follows: \\[\\mu= np\\] \\[\\sigma^2= np(1-p)\\] As useful as the binomial distribution is, it becomes more difficult to use as the values for n get larger. Lucky for us, when p = .5, the binomial distribution closely approximates a continuous density function which results in a smooth, symmetrical, bell-shaped curve called the standard normal distribution. In fact, the standard normal works for any value of p so long as n is large. x&lt;-seq(from=100, to=200, by=1) y&lt;-dbinom(x, size=500, p=.3) plot(x,y, main=&quot;Binom(n=500, p=0.5)&quot;) See this video for some detailed examples of how to use the binomial distribution (don’t worry, you won’t be doing these calculations by hand!) 17.2 Calculating probabilities using a table Download this table and take a moment to look through it. With this table, we can find the probability of observing any possible number of successes for many possible values of \\(n\\) and \\(p\\). The far left column shows the size of the trial \\(n\\), and the second column shows the number of successes. The other columns show the probability of observing that number of successes for different values of \\(p\\). If the probability of a success is \\(p=0.20\\) and \\(n=8\\), the probability of observing two successes (r=2) is 0.294. To see this, you go to the second page of the table linked above. Look near the top where the column labeled \\(n\\) says 8. Then look at the row where the column labeled \\(r\\) says 2. Follow this row along to the column labeled 0.20. That’s the answer. (DO NOT JUST READ THIS WITHOUT TRYING IT. LOOK AT THE TABLE.) Try this yourself. If I have 16 trials and p=.55, what is the probability of observing 10 successes? The answer is 0.168. If this still doesn’t make sense to you, post a question on Facebook or go talk to one of the TAs in office hours. "],
["working-with-the-t-distribution.html", "18 Working with the T-distribution 18.1 A bit about the t-distribution 18.2 Finding probabilities", " 18 Working with the T-distribution Learning objectives Get a general idea about the t-distribution. Learn how to find areas under the curve for the t-distribution using a table. 18.1 A bit about the t-distribution The t distribution is a theoretical probability distribution. It is symmetrical, bell-shaped, and similar to the standard normal curve. It differs from the standard normal curve, however, in that it has different parameter, called degrees of freedom, which changes its shape. This is usually labled \\(df\\) or \\(\\nu\\). Note that the smaller the df, the flatter the shape of the distribution, resulting in greater area in the tails of the distribution. The t distribution is useful for distributions with degrees of freedom less than 30. It is a more accurate way of describing distributions with relatively low degrees of freedom. As df approaches infinity, the t distribution becomes more like the normal distribution. x&lt;-seq(from=-5, to=5, by=.1) plot(x, dnorm(x), lwd=3, type=&quot;l&quot;, col=1, lty=1) lines(x, dt(x, df=10), lwd=3, ylim=c(0, .4), col=2, lty=2) lines(x, dt(x, df=5), lwd=3, ylim=c(0, .4), col=3, lty=3) lines(x, dt(x, df=1), lwd=3, ylim=c(0, .4), col=4, lty=4) legend(&quot;topleft&quot;, c(&quot;n(0,1)&quot;, &quot;t(df=10)&quot;, &quot;t(df=5)&quot;, &quot;t(df=1)&quot;), lty=c(1,2,3,4), col=c(1,2,3,4), bty=&quot;n&quot;) 18.2 Finding probabilities For the time being, I only want you learn how know how to use a t-table, which you can find here. This is really best shown using an example. For a distribution with \\(n = 10\\), find the probability of \\(t \\ge 1.38\\). In R this can be done with pt(1.38, df=9, lower.tail=FALSE) ## [1] 0.1004493 But what if we are doing this on a test? First, we need to determine the degrees of freedom (10-1 = 9). Once we have established the degrees of freedom as 9, we can use the t value to find the probability \\(P (t \\ge 0.1)\\). That is, we can now use our t-value in conjunction with our t table in order to find our probability. Once we have located how many degrees of freedom we have, we can go to our table and look in the row labeled 9 on the left. Now, we want to find the column that is closest to our value. In this case, we see that the first column has 1.363, which is pretty close to the number we have. We then look up at the column header and see \\(t_{.100}\\), so corresponding probability is approximately 0.100. Sometimes you have to guestimate between two values. Don’t worry to much about this. As long as you put a number between the two values, we will mark it as correct on the exam. (But you should just use R for the problem sets.) 18.2.1 Finding quantiles given a probability Sometimes we may ask you to reverse this process. We may tell you that the probability to the right is \\(0.025\\) and we have 16 degrees of freedom. In this case, we just go to the right column (labeled 16) and go to the column labeled \\(t_{0.025}\\). You shoudl see that the cell is lableed 2.120, which is the right answer. In R this can be done with: qt(0.025, df=16, lower.tail=FALSE) ## [1] 2.119905 "],
["estimating-population-means-using-confidence-intervals.html", "19 Estimating population means using confidence intervals 19.1 Statistical inference is how we draw conclusions from the data. 19.2 We make statistical inferences by using sample statistics to estimate population parameters. 19.3 The method we use to make statistical inferences using sample statistics is called estimation. 19.4 Constructing confidence intervals to make statistical inferences. 19.5 Remember that we already learned about the standard error …. 19.6 Remember that we already learned about the z-score …. 19.7 Example. 19.8 Conclusion.", " 19 Estimating population means using confidence intervals Learning Objectives Understand what statistical inference is Understand how we make statistical inferences using sample statistics to estimate population parameters Understand what kind of estimate a confidence interval is Understand how to do a simple confidence interval calculation 19.1 Statistical inference is how we draw conclusions from the data. Statistical inference is the process that we use to draw conclusions from a data. A set of data is a sample from our population. The sample is a subset of the population. Inference involves using statistics we calculate from the sample to make and informed guess about population. When we do statistical inference we are interested in drawing conclusions from a set of data (sample) so that we can estimate population parameters. (Population parameters are the characteristics of the population in which we are interested.) Parameters of the population include things like the mean, standard deviation, and variance. Statistical inference is the process by which we use sample statistics to make inferences about population parameters. Figure 1 and Table 1 describe the relationship between sample statistics and population parameters. English term Sample Statistic Population parameter Mean \\(\\bar{x}\\) \\(\\mu\\) Standard deviation \\(s\\) \\(\\sigma\\) Variance \\(s^2\\) \\(\\sigma^2\\) 19.2 We make statistical inferences by using sample statistics to estimate population parameters. Using sample statistics we can make estimates about population parameters. There are two types of estimates of parameters: point estimates and interval estimates. A point estimate is a single number that is the best guess for the parameter. An interval estimate is an interval of numbers around the point estimate, within which the parameter value is believed to fall. 19.3 The method we use to make statistical inferences using sample statistics is called estimation. The term estimator refers to a particular type of statistic for estimating a parameter. This is conceptual. The term estimate is a noun that refers to the value of that particular statistic. For example, the sample mean is an estimator of a population mean. If the sample mean of a particular sample 75 then the value 75 is the estimate for the population mean. Just as there are multiple possible sample statistics, there are are many possible estimators. If we are interested, for example, in the population’s mean, then we could use the mean, the median, or the mode of the sample as our estimator. How do statisticians decide which estimator is best? Good estimators usually have two qualities: they are unbiased and efficient. An unbiased estimator has its sampling distribution “centered” around the parameter; in other words, the expected value of the statistic is identical to the population parameter. An efficient estimator has a relatively small standard error, meaning that the variance of the sampling distribution is small. Unbiased and efficient estimators give us accurate and precise estimates of population parameters. The point estimators we teach you in this class have been shown to be the most efficient unbiased estimators possible. 19.4 Constructing confidence intervals to make statistical inferences. A truly informative statistical inference, however, should provide not only a point estimate but should also indicate how confident we can be that the estimate is correct. So, rather than a single value, we often prefer to use a range of values. This is what is known as an interval estimate: an interval of numbers (usually centered around some point estimate) within which the parameter value is believed to fall. Another name for interval estimates is confidence intervals, because they contain the parameter with a certain degree of confidence. Confidence intervals have the following form: Point estimate \\(\\pm\\) Margin of Error. Specifically, we can calculate confidene intervals for the sample mean …. Under certain circumstances, confidence intervals for means take the following formula: \\[\\bar{y} \\pm z (\\frac{s}{\\sqrt{n}}) \\] Where: \\(\\bar{y}\\) = sample mean \\(z\\) = z-score corresponding to confidence level \\(s\\) = sample standard deviation, and \\(n\\) = sample size. The point estimate of the population mean \\(\\mu\\) is the sample mean \\(\\bar{y}\\). For large random samples, by the Central Limit Theorem, the sampling distribution is approximately normal. So, for large samples, we can find a margin of error by multiplying a z-score from the normal distribution by the standard error. 19.5 Remember that we already learned about the standard error …. The standard error of the sample mean is given by the following equation: \\[\\sigma_\\bar{y} = \\frac{\\sigma}{\\sqrt{n}} \\] In the above equation, \\(\\sigma\\) is the population standard deviation. But like \\(\\mu\\), \\(\\sigma\\) is an unknown parameter. In practice, we estimate \\(\\sigma\\) using the sample standard deviation \\(s\\). So, in practice, confidence intervals use the estimated standard error. This is how we end up with the equation for confidence intervals: \\[\\hat{\\sigma}_\\bar{y} = \\frac{s}{\\sqrt{n}} \\] 19.6 Remember that we already learned about the z-score …. When we are calculating a confidence interval for a mean, our point estimate is our sample mean. Our standard error is calculated by dividing the sample standard deviation by the square root of the sample size. We have yet to understand where the z-score comes from. The z-score corresponds to the confidence level we are interested in. We choose a confidence level before we go about constructing our confidence interval. We try to choose confidence levels that are close to (but not exactly) 100%. The higher the confidence level, the more confident we can be that our unknown parameter falls within the interval estimate. The z-score indicates what our confidence interval is. Recall that 95% of the normal distribution falls within 1.96 standard deviations of the mean. We use the same logic to calculate the z-score for our confidence intervals. If we are interested in a 95% confidence interval, then our z-score is 1.96; if we are interested in a 99% confidence level, then our z-score is 2.575. In general, in order to find the z-score that corresponds to our confidence level, we take \\[(1-\\mbox{confidence coefficient)}/2.\\] The z-score that corresponds with the resulting probability is the z-score that we use when we construct our confidence interval. NOTE: The confidence coefficient is often denoted by the Greek letter \\(\\alpha\\) 19.7 Example. Let’s do a quick example. Let’s suppose we have a sample in which the sample mean is 10 and the standard error is \\(\\frac{s}{\\sqrt{n}}=2\\). We want to calculate a 95% confidence interval. We use the following formula: \\[\\bar{y}\\pm z(\\frac{s}{\\sqrt{n}}) = 10 \\pm z(2) \\] In this case, the z-score that corresponds to a 95% confidence interval is \\(z=1.96\\). How do I know that? Because \\[\\frac{1-0.95}{2}=.025\\]. I can find the corresponding z-statistics by coding the following. qnorm(0.025, lower.tail=FALSE) ## [1] 1.959964 I can then find the confidence interval using the above formula. \\[10 \\pm 1.96(2) = 10 \\pm 3.92\\] Note that the formula above is only representative of our confidence interval. It shows our original point estimate and margin error. Our confidence interval must be a range of numbers In this case, it is: [6.08, 13.92]. What does this mean? It means, (kind of) that according to our calculations, we can say with 95% confidence that our population mean falls between 6.08 and 13.92. Technically, however, that interpretation is not quite right. The 95% confidence level means that if we took many, many samples and calculated many, many confidence intervals according the procedures above, 95% of the time the true population parameter would fall within those many, many 95% confidence intervals. We actually don’t know the probabilyt that \\(\\mu\\) falls within the particualr interval [6.08, 13.92]. This is a bit confusing to think about, and most of the time you can think of it using the sentence in the last paragraph. But, keep in mind that the actual interpretation is a bit muddier. Here is a nice video that goes through another example and explains the relationship between the confidence interval and the normal distribution: 19.8 Conclusion. In this section we learned that we make statistical inferences to make conclusions from our data. Statistical inference requires that we estimate population parameters using sample statistics. One way we can make a statistical inference is by constructing confidence intervals. Confidence intervals give us an interval of numbers around the point estimate within which the parameter value is believed to fall. Although confidence intervals can be constructed for means or proportions, in this article, we learned that how to construct a confidence interval for a population mean (parameter) using the sample mean and sample standard deviation (sample statistics). "],
["team-assignment.html", "20 Team Assignment", " 20 Team Assignment Team number Member 1 Membeer 2 Member 3 Member 4 Member 5 1 Kelly Ryan William Scerpella Henry Nguyen Sydney Welter 2 Ashlee Chung Aliza Astrow Nicole Ferzoco Nathan Praeger 3 Ilana Engel Kalen Davison Nate Lowis Kristen Lau 4 Stephanie Wright Monye Pitt Andrew Englund Jack Broitman 5 Joey Vettiankal Hannah Schanzer Sydney Curtis Amanda Mendelsohn 6 Claire Thomas Carter Paterson Rishi Patel Pilar Gonzalez 7 Camilo Haller Mikaela Adwar Emily Garner Massil Adnani 8 Drew McPike Josh LaFianza Giulia Neaher Nicholas Castriz 9 Noah Gordon David McGarr Joe Gorman 10 Zach Vincent Aidan Strassmann Tom Hutchison Andrea Tam Zachary Virgilio 11 Shreya Tripathy Brian Adler Hannah Daniels Jack Ploshnick Alex Cheung 12 Robert Stoffa Joel Brown Kevin Si Serina Karkhanis 13 Jon Wingens Dugan Marieb Sarah Paul Carolyn Perlmutter "],
["constructing-confidence-intervals-using-proportions.html", "21 Constructing Confidence Intervals Using Proportions 21.1 What is a proportion? 21.2 Confidence Intervals With Proportions 21.3 Example", " 21 Constructing Confidence Intervals Using Proportions Learning Objectives Further understand proportions and how they differ from population means Understand how to construct confidence intervals for proportions. A little clarification on the notation for proportions. 21.1 What is a proportion? Proportions are significantly different from population means. Proportions are for dichotomous variables: variables that are characterized by only two values. So supposing we have some variable called Obama Approval Rating. This is a dichotomous variable because there are really only two values we are interested in: the percentage that “approve” and the percentage that did not respond that they “approved.” Contrast this with population means that take the form of quantitative variables. Quantitative variables are what we have predominantly dealing with in the course so far. These are characterized by values that differ for each individual respondent. That is, our data for each person might be many different numbers. Height, for example, is a continuous quantitative variable because each data point has a different value (Note how this differs from Obama Approval Rating where each data point can only take one of two values). 21.2 Confidence Intervals With Proportions Statistics problems that use proportions use most of the same equations as the problems for population means. But to calculate the standard error for proportions, we use the following formula: \\[\\hat{\\sigma}_{\\hat{\\pi}}=\\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi}) }{n}}\\] For a refresher on why this might be, go back and review the information on the sampling distribution of a proportion. Our estimator (\\(\\hat{\\pi}\\)} for the population proportion (\\(\\pi\\)) is the sample proportion \\(p\\). The standard equation for writing a confidence interval for a proportion is as follows: \\[\\pi \\pm Z \\times \\sigma_{\\pi}\\] The confidence that we can have for any proportion varies with size of the sample and the degree to which the two values for our dichotomous variable match. As \\(n\\) increases, the confidence we have in our statistic also increases. Note also, that as the value for our proportion approaches 50% (or .50), the less confidence we have in our statistic. If that seems a little weird, go back and look at the equations above and try a few different numbers for p, and see what you get. 21.3 Example Suppose that the point estimate for the proportion of Americans who approved George W. Bush as president in 2008 is 25% where n = 5000. Construct a 95% confidence interval for this proportion. First, write out the equation for the confidence interval for a proportion: \\[\\pi \\pm z \\times \\sigma_{\\pi}\\] We already have the point estimate for this proportion (p=0.25). We still have to calculate the appropriate Z value and the standard error. To get the Z value, we need to calculate for the observation that corresponds to a 95% confidence interval. In general, to find the observation that corresponds to some confidence interval we take (1-confidence coefficient)/2. In this case: (1-.95)/2 = .025. To find the Z value that corresponds, we simply look this probability up in a Z table. We find .025 in the column that reads .06 and the row that reads 1.9. Therefore, we have found our Z value to be 1.96. Next we need to calculate our standard error using the following formula: \\[\\hat{\\sigma}_{\\hat{\\pi}}=\\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi}) }{n}}\\] \\[\\hat{\\sigma}_{\\hat{\\pi}}=\\sqrt{\\frac{.25(1-.25)}{ 5000}} = .0061\\] We are now ready to construct our confidence interval by plugging in our calculated values into the equation above. \\[.25 \\pm 1.96\\times .0061\\] \\[.25 \\pm .012\\] Here is another example of how to calculate a confidence interval for a proportion you might find instructive: "],
["large-n-hypothesis-testing-for-proportions.html", "22 Large-n hypothesis testing for proportions 22.1 Qualitative data 22.2 Learning Objectives 22.3 Science is about testing claims with data 22.4 But you will need to construct a hypothesis test… 22.5 This all sounds a bit confusing, so let’s break this down… 22.6 Watch all of these videos", " 22 Large-n hypothesis testing for proportions 22.1 Qualitative data 22.2 Learning Objectives Understand conceptually why we use hypothesis tests Understand the five parts of a hypothesis test A couple of very helpful videos videos 22.3 Science is about testing claims with data Let’s say that you are arguing with your friends over dinner, and they claim that WashU students are more conservaitve than the national average. Since everyone you know seems to be an Obama fan, you don’t believe that is true. You are a determined skeptic. One way you could solve this problem would be to argue really loudly (whoever is most obnoxious wins). Another way to win might be to tell your friend about some anecdotes and examples (tell them about your liberal classmates). Neither of these represent good ways to really test competing claims. But f you think about it, this is how most political arguments seem to be resolved (at least on cable TV). But a better way is to try to resolve this conflict using data. Because you can use Google, you know that 20.7% of College Freshman nationwide identified themselves as Conservaitve. http://heri.ucla.edu/PDFs/pubs/TFS/Norms/Monographs/TheAmericanFreshman2011.pdf Imagine you take a simple random sample of 50 WashU Freshman. 7 of them identify themselves as conservaves, 14 of them identify themselves as liberal, and the rest sa they are “middle of the road.” There are twice as many liberals as conservaitves in your sample! Based on this sample evidence, can you determine whether or not your friend is right? Yes. Yes, you can. 22.4 But you will need to construct a hypothesis test… In a hypothesis test, we have an assumption that the population parameter takes a particular value, called the null hypothesis. This usually corresponds to the claim you are trying to disprove. In the above example, the null hypothesis (the view of the determined skeptic) would be that the proportion of students at WashU is 20.7% or less (\\(H_0: \\pi_0 \\le 0.207\\)). The alternative hypothesis, your friend’s theory, is that the proportion of students who are conservative is more than the national average (\\(H_a: \\mu_a&gt;0.207\\)). A hypothesis test analyzes the evidence against the null hypothesis. It tells us if we would be likely to get our sample value just because of normal sampling variability if the null hypothesis were true. We judge how far the estimate falls from the null hypothesis’ parameter value, and then look at the probability of getting a sample value this far away from the null hypothesis value or further. 22.5 This all sounds a bit confusing, so let’s break this down… Learning objective 2: Understand the five parts of a hypothesis test The five parts of a hypothesis test are: assumptions, hypotheses, test statistic, p-value, and conclusion. 1) Assumptions To perform a hypothesis test you must assume: The data is obtained through random sampling We have qualitative data (things that are coded zero or one) As a rule of thumb, we want a sample size such that \\[ n \\ge \\frac{10}{min(\\pi_0, 1-\\pi_0)}\\] min() means the minimum of the two numbers The 10 in that equation is kind of arbitrary (it serves as a rule of thumb) If the sample size is this large, you can assume that there is a normal sampling distribution for the sample proportion. This gives us the leverage we need to make some calculations. 2) State hypotheses The null hypothesis is going to be that the true population proportion, \\(\\pi\\), is equal to the null hypothesis value, \\(\\pi_0\\). In this case \\(\\pi_0\\le0.207\\). The alternative hypothesis is that \\(\\pi_a\\) is greater than \\(\\pi_a &gt; 0.207\\). 3) Calculate test statistic For large samples, we use a test statistic to determine how many standard errors (we use standard errors and not standard deviations because we are working with the sampling distribution) our observed sample proportion falls from the null hypothesis. Here, we will denote our test statistic \\(TS\\) Using the information above, we know that \\(\\hat{\\pi} = 0.14\\). \\[TS=\\frac{\\hat{\\pi} - \\pi_0}{se_0}\\] Intuitively, this means that the farther $ falls from \\(\\pi_0\\) on the sampling distribution, the bigger our test statistic. However, it is important to note that we have to calculate our test statistic under the assumption that the null hypothesis is true. So, we are going to use the following formula for proportions \\[\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}} \\] In words, that means we are going to calcualte the standard error for the sampling distribution under the assumption that \\(\\pi=\\pi_0\\). In our case this means that \\[\\sqrt{\\frac{0.207(1-0.207)}{50}}=0.057\\] That means that \\[TS=\\frac{0.14 - 0.207}{0.057}=-1.169\\] 4) Calculate the p-value A p-value is the probabiliyt of observing your sample statistic, or something more extreme, assuming that the null hypothesis is true. Remember that in a sampling distribution, each value of \\(\\hat{\\pi}\\) has a probability associated with it. Values that fall very far away from \\(\\pi_0\\) have a very low probability of occurring if the null hypothesis is true. To determine that exact probability, we use the z-table! We simply go to the z-table and look up what probability is associated with our TS value. The below picture illustrates. In the sampling distribution, sample proportions that are closest to the true population proportion have a higher probability of occurring. Sample proportions that are way out in the tails are less likely to occur. NOTE: Some hypothesis tests are two-sided and others are one-sided. Most z-tables only provide you with a one-sided probability. So if you are performing a two-sided test, you will need to multiply your p-value by two. This multiplication is basically a way to account for not only the probability of obtaining a y.bar as large or larger than the one you obtained, but also a way to account for the probability of obtaining a value as or more extreme than the one obtained in either direction. In this, case we can look up the p-value, as the area to the right of \\(z=1.169\\) and see that \\(p \\approx 0.12\\). This represents the probability of observing a sample statistic this extreme (OR MORE) if the null hypothesis is true. 5) Draw a conclusion But what does it mean if you obtain a p-value that is incredibly tiny? If you obtain a p-value of .05 this means that the probability of obtaining a sample statistic as or more extreme than the one you obtained is only about 1/20 or 5%. This suggests that the null hypothesis is incorrect and that we should reject it. Generally, you decide upon an \\(\\alpha\\) or a level of significance before you perform the hypothesis test. The \\(\\alpha\\) tells you that for a p-value under the chosen value you will reject the null hypothesis. In this case, we have failed to reject the null hypothesis. That means, that there is not sufficient evidence in this sample to support your friends claim. You cannot reject the null hypothesis (which is not the same thing as supporting it). 22.6 Watch all of these videos A video about hypothesis tests (this is a two-sided test) A video about one-sided and two-sided hypothesis tests Large sample proportion hypothesis testing (WARNING: he uses different notation) "],
["type-i-and-type-ii-errors.html", "23 Type I and Type II Errors 23.1 Learning Objective: 23.2 Type I and Type II Errors", " 23 Type I and Type II Errors 23.1 Learning Objective: Recognize the difference between Type I and Type II errors 23.2 Type I and Type II Errors A type I error occurs when the null hypothesis is falsely rejected. We minimize the risk of making a type I error by using an \\(\\alpha\\) level as the basis of comparison for our p-value. For example, an \\(\\alpha\\) level of 0.05 means that only 1 in 20 times, we will make a type I error and falsely reject the null where the null hypothesis is a true descriptor of our data. A type II error occurs when the alternative hypothesis is true, but we fail to reject the null. Here is a table explaining the two types of errors we can encounter when interpreting a hypothesis test: ~ Truth=H0 Truth=Ha Fail to Reject H0=True No Error Type II Reject H0 Type I No Error Imagine a fire detector. The null state of being is “no fire.” The alternative hypothesis is “fire.” If the fire detector goes off but there is no fire (like when you take a hot shower in the traditional dorms), then a type I error has occurred- the null hypothesis of “no fire” was falsely rejected. Let’s say that your fire alarm keeps making a type I error, so you get frustrated and remove the batteries from the fire alarm. A few days later, there actually is a fire, but the alarm doesn’t go off because you took out the batteries. This means that a type II error has occurred- the alternative hypothesis was true (there was a fire), but instead you falsely failed to reject the null. In general, we are more concerned about Type I errors, since this will lead us to reject the null hypothesis when it is actually true. So, for instance, we might conclude that our experiment worked, when in fact the treatment had no effect. This video starts with a good example of two-sided large n hypothesis test (in case you need to refresh your memory), and at about the 3:00 mark, it explains the difference between type I and type II errors. "],
["hypothesis-testing-for-small-sample-quantitative-data.html", "24 Hypothesis Testing for Small Sample Quantitative Data 24.1 Learning Objective: 24.2 Hypothesis Test", " 24 Hypothesis Testing for Small Sample Quantitative Data 24.1 Learning Objective: Learn how to conduct a hypothesis test with a small number of observations for quantitative data. 24.2 Hypothesis Test 24.2.1 Step 1: Make assumptions about your data For these tests, we assume that our data is quantitative and that the population is normally distributed. We also know that our sample size is going to be relatively small, which means that we will be referencing the t table rather than the z table. 24.2.2 Step 2: Formulate null and alternative hypotheses Just like with other types of hypothesis testing, the null is what a determined skeptic would believe about what we are going to measure, and the alternative hypothesis is our research hypothesis based on the data collected. As with other types of hypothesis testing, this test can be either one-sided or two-sided. Make sure you keep in mind how you intend to test the data. 24.2.3 Step 3: Calculate a Test Statistic This statistic summarizes how much our alternative hypothesis differs from the rest of the data if the null was true. Here is the formula, which we’ve used many times before: \\[t^\\ast= \\frac{\\bar{x}-\\mu_0}{\\frac{s}{\\sqrt{n}}}\\] Don’t forget to note the degrees of freedom! d.f.= n-1 24.2.4 Step 4: Calculate a p-value Remember, a p-value is a measure of surprise, so small p-values more strongly contradict the null because that means it would be extremely surprising to see the results of our alternative hypothesis if the null were true. To find the p-value, go to this t table (click here), or we can use R. Keep in mind the confidence interval, the degrees of freedom, and if the hypothesis is one or two-sided. 24.2.5 Step 5: Draw a conclusion Are the results statistically significant given the pre-determined \\(\\alpha\\) level (usually is the p value &lt; 0.05)? If p&lt;\\(\\alpha\\), we reject the null and conclude that the evidence supports the alternative hypothesis. If \\(p&gt;\\alpha\\), we fail to reject the null. For more help, watch this video from the Khan Academy, which will walk you through a small-sample t-test step-by-step: "],
["confidence-intervals-in-r.html", "25 Confidence intervals in R 25.1 Learning Goals 25.2 Example Graphs and R Code: 25.3 What is a Confidence Interval? 25.4 Confidence Intervals for the Mean of Populations 25.5 In R:", " 25 Confidence intervals in R 25.1 Learning Goals Plotting the t distribution, with different degrees of freedom Building confidence intervals (CIs) 25.2 Example Graphs and R Code: par(mfrow=c(1,3)) x.numbers &lt;- seq(from=-6.869, to=6.869, length.out=100) plot(x=x.numbers, y=dt(x.numbers, df=2), xlab=&quot;x&quot;, ylab=&quot;Density&quot;, main=paste(&quot;t Distribution: DOF=2&quot;), type=&quot;l&quot;,col=&quot;green&quot;,ylim=c(0,0.5)) plot(x=x.numbers, y=dt(x.numbers,df=10), xlab=&quot;x&quot;, ylab=&quot;Density&quot;, main=paste(&quot;t Distribution: DOF=10&quot;), type=&quot;l&quot;,col=&quot;red&quot;,ylim=c(0,0.5)) plot(x=x.numbers, y=dt(x.numbers,df=30), xlab=&quot;x&quot;, ylab=&quot;Density&quot;, main=paste(&quot;t Distribution: DOF=30&quot;), type=&quot;l&quot;,col=&quot;blue&quot;,ylim=c(0,0.5)) 25.3 What is a Confidence Interval? Interval estimate of a population parameter Takes into account the uncertainty Depends on the data and confidence level Confidence level: If confidence intervals are constructed across manyseparate data analyses, the proportion of such intervals that contain the true value of the parameter will match the confidence level. For exampl, if we had a 90% confidence interval, we would expect 90% of confidence intervals we construct to include the true parameter \\(\\mu\\). Example This is is 30 samples (n=150) from a normal population (\\(\\mu=0\\)) and the resulting 90% confidence interval. Out of the 30 confidence intervals, only 27 of them (90%) include \\(\\mu\\)! What happens when we increase/decrease confidence level? The confidence intervals will get wider and more of them will include \\(\\mu\\). 25.4 Confidence Intervals for the Mean of Populations Obtain \\(\\bar{y}=\\frac{\\sum{y_i}}{n}\\) Obtain \\(\\sigma_{\\bar{y}}=\\frac{\\sigma}{\\sqrt{n}}\\) or \\(\\hat{\\sigma}_{\\bar{y}}=\\frac{S}{\\sqrt{n}}\\) Choose a confidence level Obtain confidence coefficient: \\(\\frac{\\text{confidence level}}{100}\\) Find the Z-score that corresponds to \\(\\frac{(\\text{1-confidence coefficient})}{2}\\) [Normal Table or R] Confidence Interval for your chosen confidence level: (\\(\\bar{y}-z*\\sigma_{\\bar{y}},\\bar{y}+z*\\sigma_{\\bar{y}}\\)) 25.5 In R: 25.5.1 Numerical Summaries: Get the mean, standard deviation and number of observations of thevariable you are interested in 25.5.2 Normal Quantiles The qnorm() gives you the required z-score. 25.5.3 Code Obtain the Z- score and save it in an object called z.score: z.score = qnorm(1-confidence coefficient/2) For example, for a 99% confidence interval, I could use z.score = qnorm((1-.01/2)) z.score ## [1] 2.575829 Get mean, sd, and n of the variable and save in appropriately named objects: mean.variable = mean(nameofdataset$nameofvariable,na.rm=T) sd.variable = sd(nameofdataset$nameofvariable,na.rm=T) n = length(na.omit(nameofdataset$nameofvariable)) Remember that R stores missing data as NA. So the na.omit() removes missing data from the vector. And the na.rm=T tells R to ignore missing data when calculating the standard deviation or the mean. Compute lower bound: mean.variable - z.score*sd.variable/sqrt(n) Compute upper bound: mean.variable+ z.score*sd.variable/sqrt(n) "],
["poster-projects.html", "26 Poster Projects 26.1 Grading 26.2 Finding the right topic, question, theory, and data 26.3 Actually making a poster (writing and visual design) 26.4 R Code 26.5 Methods 26.6 Results 26.7 Conclusion and Limitations 26.8 Technical stuff 26.9 Tips for Doing Well 26.10 Past Poster Project Examples", " 26 Poster Projects As we get closer to starting group research projects, I wanted to give you a run-through of my expectations. I’ve put together an overview of what is expected of your group. But if you have any questions, please don’t hesitate to reach out to me. The TAs are a good resource for specific question, help with R, etc. But I grade these posters personally and consider them the most important part of the course. So I should be your first and last point of contact for “big” questions like topic, research design, etc. 26.1 Grading Your project’s grading will be based on a very specific rubric listed on the syllabus. You will be graded on eight different parts of your project: Introduction and Theory Methods Results Limitations and Conclusions Statistical Analysis (poster) Statistical analysis (R Script) Visual Design Writing Quality Each section is graded on a 5 points (for a total of 40 points). The necessary requirements to get all five points per section are extensively explained on the rubric. I suggest that you spend time looking over it in order to familiarize yourself with my expectations. 26.2 Finding the right topic, question, theory, and data 26.2.1 Topic You should have a strong and substantive motivation for why you chose the research question you did. This section should include a few sentences of background. Why did you choose this question? Why is it important? What is it that your research attempts to answer? To be perfectly clear you are allowed to study any topic you want. But that does not mean that all topics and questions are equal. Be prepared to tell me why your topic is interesting. 26.2.2 Question This section should also specify your research question. Your research question should be succinct, easily understood. Most importantly, your question should be something you care about. What topics interest you? What phenomenon are you dying to explain? Consider questions like these in order to formulate your research. If you don’t care about the question itself, then the project will be miserable to complete. 26.2.3 Theory What you expected when you started gathering data (your theory of how the world works), and why. You don’t have to write much, but you should clearly state why your expectations were what they were. 26.2.4 Out of bounds Though you are given a lot of room explore, you’re not allowed to do any of the following projects: No “Time Series” studies where we are studying one unit over time. Questions like, “what drives presidential approval?” are out of bounds because time-series requires additional statistical skills not covered in this course. No exploratory projects ex. What factors determine attitudes toward abortion? You need to tell me what factors drive attitudes towards abortion and then test that theory. No sensitive data/risky behaviors/at-risk populations ex. Surveys of dating habits, drug use, alcohol consumption, etc. Surveys of minors, the homeless, etc. Do NOT sample on the dependent variable ex. Study the causes of election fraud using a sample of fraudulent elections. We also need to know about cases where there was not a fraudulent election to answer this question. ex. Study the causes of police shootings using a database of instances where there is a police shooting. We also need to know about cases where there was not a police shooting to answer this question. Do NOT sample on the independent variable ex. Studying the effect of church attendance on political beliefs by surveying people at church. To know the effect of church attendance we also have to know about the behaviors of non church goers. 26.2.5 Hypothesis Your hypothesis needs to be falsifiable. Don’t ask broad questions such as: Why do Americans Vote? What causes peace? Geting more specific allows you to state a hypothesis that you could reasonably test with data. “What makes college kids vote?” is bad. “College students don’t vote when there are stricter registration laws in their state,” is good. If you need examples, please see the sample poster presentations below for ideas of what your hypothesis should look like. But they need to be clear. And you need to be able to tell me what kind of statistical evidence would prove you wrong. Because if you cannot explain that, then you don’t really have a scientific theory (or a null hypothesis for that matter). 26.2.6 Data Collection You have the option of using pre-collected data or collecting it on your own (which is encouraged, but not required). The university pays for all students to have access to Qualtrics, a powerful survey software that is at your disposal. Should you choose to collect your own, please see me before executing your data-collecting plan. I may be able to provide you with “incentives” (e.g., a drawing for a gift card). If you don’t plan on collecting your own, here are a few resources I recommend for data: Harvard’s Institute for Quantitative Social Sciences American National Election Study (ANES) The ICPSR data set archives The Political Science Subject Librarian can also be a good resource for finding data: Cynthia Hudson-Vitale chudson@wustl.edu \\(314.935.7465\\) And if only you knew a faculty member with wide ranging interests in quantitative analysis who is experienced in helping undergraduates get the data they need to do a good project, maybe you could go ask her/him for some help. 26.3 Actually making a poster (writing and visual design) Remember: This is a poster NOT a paper. Your project should use clear and concise writing and full sentences WITHOUT cramming paragraph on paragraph worth of information on your poster. In fact, I would be happy with no paragraphs. You should be able to succinctly explain each step of your research without having to write a page on it. The fewer words the better. The number one way that students lose points on the poster is that I view them as “wordy.” Most groups will choose to create their poster using Microsoft PowerPoint but there are, of course, other ways. The layout should flow easily and be aesthetically pleasing. Please don’t make a poster that is unreasonably colored or has weird fonts. This is unnecessary and will distract from the substantive material on the poster. I should not have any issues attempting to read your findings. Finally, you will be responsible for actually printing a copy of the poster and bringing it to the last lab period. Don’t put it off to the last minute. 26.4 R Code The code you use for your analysis should be submitted to me in the form of an R Script. The only thing I should need to change in the script you submit to me is the working directory. Otherwise, I should be able to run your entire analysis without any error readings and it will replicate the results on your poster exactly. Thus, I will also need all of the data. Any errors, or discrepancies will result in a subtraction from your score on this section of the rubric. Additionally, your code should have informative comments throughout (using the # in your script) to explain what you’re doing. Code without comments will be penalized. In general I do not need to see all of the code you used to collect/organize/clean your data. But I do want the code that takes your final dataset and conducts the statistical analyses and generates the figures. 26.5 Methods First, your methods section should include an explanation for how you collected your data and the methods with which you used to analyze it. Did you collect the data yourself? If so, how did you do it? If you used a survey, what questions did you ask? If you did not collect it yourself, please explain where you got your data from (a State Board of Elections, Harvard’s Dataverse, Census, etc.) and give a brief account of its methedology. This goes for data you used for covariates as well, should you choose to include some. Second, explain how you tested your hypothesis. It should include all important aspects of how your study was conducted in a detailed and replicable fashion. What method you used (Difference in Differences, Instrumental Variables, a Fixed Effects model, etc.), if you used a dummy variable, or any other aspect that is vital to understanding your study design. Overall, your methods section should include all important aspects of how the study was conducted that convincingly motivates and defends the key choices in your design process. 26.6 Results Your results section should include a combination of figures and tables that illustrate your findings in an intuitive and easy-to-understand format. It should be clear exactly what you found. Any figures you use should be created in R. Excel figures make me angry. Tables may be created using Microsoft Office. Because a majority of this section will be dedicated to your graphics, don’t worry about including a long, written analysis for your results section. Let your graphics and tables tell the story of your analysis with short comments that explain what they are showing. Look to past poster examples for an idea of what I’m looking for. 26.7 Conclusion and Limitations 26.7.1 Conclusion Use this section to discuss potential explanations for your findings. What did you find? Was it statistically significant? What does this tell you about the question your project attempts to answer? What inferences can we draw from what you found? What are some ideas for future research? Your conclusion should include whether or not you were able to reject your null hypothesis. Please keep in mind, it is not required that you find statistically significant results that will allow you to reject your null hypothesis. More than likely, not a lot of groups will be able to. This is ok! You can still create a well-designed project and make substantive recommendations for future research. Remember, you are new at this. If you are really testing a new/interesting idea, the odds are pretty good that you are going to be wrong and the data won’t back up your hypothesis. But that’s how this is all supposed to work. I greatly prefer teams who go out on a limb in persuit of exciting knowledge and fail to teams that play it safe. 26.7.2 Limitations Every single one of your research projects will have limitations. This section should be a detailed discussion of those that you faced through every step of the process. What problems did you face that would affect your results? Were there possible confounding variables you were unable to control for? What were the limits of your study design? Or in the data collection? These are all things you should consider when formulating this section for your poster. 26.8 Technical stuff 26.8.1 Submitting Your Projects: All projects must be submitted to me by email before class on Wednesday, December 4th. You do not need your printed poster for class that day, but you will need it ready by the beginning of your lab session as you will present to me, your ATI and your peers in your section. Be sure to include the title of your poster in the subject line of the email and your group number and the names of all members of the team in the main text. Your emailed file should be zip file that includes your data, a PDF of your poster, and an R script of your analysis. 26.8.2 Printing Yes, you have to print out your poster. There are multiple places on campus and off that have the capability of printing out a poster like this. It is up to you to fund and print your poster before presenting to your lab. We will not do this for you. That being said, we will provide some possible printing options and resources closer to the project due date. 26.9 Tips for Doing Well Start Early This is true for all group projects, but starting early will ensure that you have enough time to work out any kinks or issues you have with your projects before its due. Groups that start their project after Thanksgiving Break don’t do as well as groups that get started closer to the beginning of November. Please remember that this project is worth 25% of your grade and should not be taken lightly. My expectations for these projects go up every year, therefore my threshold for an A level poster goes up as well. Office Hours: Come Early, Come Often While you are only required to come speak with me once about your projects, I encourage you to check in with me more than this. I grade the projects and therefore only I know what I’m looking for. The teams that meet with me early and often are the ones that always do well. Pay Close Attention to the Rubric Please take a close look at what the markers are for getting high marks in each grading section. I will follow these requirements for 5’s very closely. 26.10 Past Poster Project Examples Below I am providing posters that did very well in past years. That being said, the posters were not perfect. So while all of these were great projects, I’ll point out a few strengths and weaknesses so you can do even better. PDF version here. Strengths: Strong motivations for their research and hypothesis Collected their own data! Two different datasets! Exhaustive limitations and conclusion sections that includes substantive interpretation of their results Weaknesses: This project is way too wordy. The design should include fewer descriptions and rely more on their graphs and tables to speak for themselves While the results themselves are well done, the organization and structure makes it hard to follow. Should include prospects for future research PDF version here. Strengths: Well thought out and interesting research design with substantive motivation Collected their own data! Great description of their data collection methods that encompasses all relative information pertaining to their methodology. Shows a thorough understanding of their results section Still too wordy. Weaknesses: Methods section does not discuss how they tested their data. Results section, while it has a lot of description, the figures themselves are hard to interpret. Please make sure that your graphs are easily understood. The limitation and conclusion section should be expanded on. PDF version here. Strengths: Substantive reasons for research and clear hypothesis Methods section describes both data-collection techniques and hypothesis testing techniques Limitations and conclusions sections are substantive and perceptive Results section allows graphics to speak for themselves and it is easy to understand their findings without paragraphs of information needed Weaknesses: Should include more specific grounds for future research Some small spelling and grammatical errors Somewhat wordy in parts. PDF version here. Strengths: Well considered research question and hypothesis testing Methods section is specifies all important aspects of how the study was conducted. Appproaches a difficult inferential problem from two different approaches, one of which required gathering their own data. Perceptive and detailed discussion of their limitations and conclusion including possible future research. Attractive design that makes their research easy to interpret Consise wording that conveys information directly. Weaknesses: Makes me worry that I may be out of a job. "],
["causation-and-average-treatment-effects.html", "27 Causation and Average Treatment Effects 27.1 Learning Objectives: 27.2 Introduction 27.3 So, what exactly is causation? 27.4 How do we determine what the counterfactual is? 27.5 But what if we can’t do an experiment? 27.6 Conclusion", " 27 Causation and Average Treatment Effects 27.1 Learning Objectives: Understand how social scientists define causation Understand the fundamental problem of causal inference Understand why experiments are social scientists’ ideal method of determining causation Understand the term “average treatment effect” Understand the difficulties social scientists face when using correlation in the absence of an experiment to establish causality. 27.2 Introduction So far, we’ve talked about univariate inference. This is useful, say, if we wanted to conduct a poll to determine what proportion of the country intends to vote Republican in the next election, or to determine the average household income in a certain country. But there is one crucial part of social science that this class has not really addressed: how to determine cause and effect. Causation is a crucial part of the social sciences (and of science in general). Political scientists aren’t just concerned with describing a certain factor in the world (income, political view); they’re concerned with understanding how those factors relate to each other. Does income inequality cause political polarization? Does being democratic make a country less likely to go to war with its neighbors? Does being appointed by a Republican make a judge more likely to vote consistent with conservative policies? 27.3 So, what exactly is causation? Imagine you’re driving a car down a four-lane road. All of the sudden, out of nowhere, a car drives out of its lane, into yours, right in front of your car. You can tell that the driver is texting, and isn’t looking at the road. You slam on the breaks, and your hood collides with his fender. Fortunately, you’re shaken, but fine, and so is the other driver. Chances are, you will be asking yourselves some questions. Why did that happen? What would have happened if I had been paying more attention to the cars around you? What if that driver hadn’t been texting? You would be asking yourself what caused that accident. What does it mean for something to have caused that accident? What things can we point to? Well, we’d probably say that the other driver caused the accident. Or maybe, if we wanted to be precise, we’d say that the fact that the driver was texting while driving caused the accident. But what does it mean when we say that the fact that the driver was texting caused the accident? And what makes the fact that the driver was texting while driving any different than the fact that the sky was blue before the accident? After all, both are true statements. But only one could reasonably be considered a “cause” of the accident. Philosophers and scientists have pondered the question of cause and effect for centuries, and have come up with various different definitions of causation. But the definition that is generally used by political scientists is this: a X causes Y if Y would not have occurred but for X. One of the most famous examples of this in popular culture occurs in the movie “It’s A Wonderful Life.” In the movie, the protagonist, George Bailey, in a moment of despair, wishes that he had never existed. Following this, his guardian angel shows him what his community would look like if he had never existed. As George Bailey walks around this new version of the world, he directly sees what he has caused, because the world he sees is a world in which everything is the same except for one thing: he was never born. Because of this, every difference between the actual world and this hypothetical world can be attributed to his birth and life. In science, the term for what would be if it were not for the existence of some causal factor-is called the counterfactual. In this movie we see two worlds. The first, is what actually happened. George saved his brother from drowning when they were kids. He got married and had kids. He runs a small Building &amp; Loan company that staves off the worst of the great depression in his town. The world his angel shows him is the counterfactual. It is the the world as it would have existed without George Baily. His little brother drowned. His girl never go married and his kids were never born. The Building &amp; Loan shut down and his hometown became a slum. One world is what happened with George Baily (\\(Y=State~of~world|X=GB~Born\\)). The other is the world that would have happened without George Baily (\\(Y=State~of~world | X=GB~not~born\\)) but where everything else was exactly the same. Anything that was different between these two worlds was, under our definition, caused by George Baily. Turning to a more political example, if we wanted to know whether the stimulus package of 2008 created jobs, then the counterfactual would be what would have happened if the stimulus package had not been passed. So to estimate the effect of the stimulus, we have to know what would have happened without it. If that counterfactual is that unemployment would have increased to 20% in the absence of stimulus, then we could conclude that the stimulus package was effective in creating jobs. If the counterfactual was that unemployment would have gone down to 2% in the absence of stimulus, then we could conclude that the stimulus package failed. So your estimate of the effect of the stimulus depends entirely on what you assume would have happened in the absence of stimulus. 27.4 How do we determine what the counterfactual is? As you may have figured out, it’s actually impossible for us to know what the counterfactual is, because, logically, X cannot simultaneously be true and not true. Either the incumbent wins an election, or she doesn’t. Either a household experiences tax increases, or it doesn’t. Either the stimulus is put into effect, or it isn’t. Either the other driver was texting or he/she wasn’t. We do not have the luxury of being able to see the results of both scenarios play out simultaneously. This is sometimes called the fundamental problem of causal inference. We can observe only the world in front of us. We cannot observe hypothetical counterfactuals But do not lose hope. There are several ways scientists have developed to estimate causal effects that can partially overcome this problem. The method that most directly determines cause is an experiment. 27.4.1 How does an experiment work? Very basically, in an experiment, we isolate the one variable whose effect we want to measure. We create two groups, which are (ideally) identical in every single way, except one group is exposed to that variable (called the treatment group), while the other is not (called the control group). In order to get the two groups to be as similar as possible, we use randomization. That is, we randomly assign people to be either in the control or treatment groups. For example, if we wanted to study the impact of a certain medication on heart pressure, then we would randomly assign the participants to either receive the medication or receive a placebo. If the treatment group improved, and the control group didn’t, we would be more confident that the cause of this improvement was the medication. 27.4.2 So, how do we measure the effect of the treatment in an experiment? Imagine you’re running an experiment on political views. Let’s say you want to know whether exposure to negative campaign ads makes someone less likely to believe that politicians care about them. You randomly assign half of your subjects to watch a negative campaign ad, and half of your subjects watch the same clip, altered slightly to change the tone from negative to neutral. You then ask the subjects to fill out a survey, evaluating their attitudes towards politics. If your experiment works the way any other experiment does, there’s probably a lot of statistical noise. Even if your experiment worked, it’s unlikely that every single person in the treatment group scored lower than every single person in the control group. There’s probably a decent amount of variance in both groups. So how do we measure the effect of the experimental treatment? It’s actually somewhat straightforward: we measure the difference between the average score for the treatment group, and the average score for the control group. This will give us something we call the average treatment effect (ATE). To state it more formally, the formula for the ATE is: \\[\\frac{1}{N}\\sum_{i=1}^N (Y_i(1) - Y_i(0)) \\] Where N is the number of subjects, \\(Y_i(1)\\) is the outcome if the observation is in the treatment group, and \\(Y_i(0)\\) is the outcome variable in the control group. As an example, imagine that individuals 1,2,3 are assigned to the treatment group, and individuals 4 and 5 are assigned to the control group. \\[ATE=\\frac{Y_1+Y_2+Y_3-Y_4-Y_5}{5}\\] (NOTE: We haven’t yet covered regression, but when we do, something to keep in mind: the beta coefficient in a regression is that ATE for that variable.) 27.5 But what if we can’t do an experiment? Especially in the social sciences, we cannot experimentally manipulate everything. We cannot randomly have certain countries go to war with each other, and others not, to see what the impact is. We cannot force certain counties to elect Democrats, and others to elect Republicans. In those cases, we cannot use experimental methods to evaluate causal claims. So instead, we use some statistical methods to try to untangle causation. When we can’t use experimental control, then we collect data, and try to determine cause using statistical control. When we are trying to figure out whether X caused Y, there are 3 things we want to do: 1) Establish temporal order For X to have caused Y, X must have come before Y. This is relatively straightforward. BUT, just because X came before Y, doesn’t mean that X caused Y. 2) Show an association This is what we will focus on the rest of the semester, using regression and a number of other statistical techniques. Basically, you have to show that there is some sort of relationship between two variables. BUT, just because X is associated with Y, doesn’t mean X caused Y. Correlation is not causation. For example, ice cream sales are closely related to drowning deaths (i.e., when ice cream sales rise, so do drowning deaths). Of course, that doesn’t mean that ice cream sales cause drowning deaths; both just happen to occur in the summer. 3) Eliminate alternative explanations We want to show that no other factor explains the outcome that we are trying to explain. Once we have done these three things, then we can say that X caused Y. But doing this is not easy. In particular, eliminating alternative explanations is very difficult because of something we call confounding. This brings up an important concept known as endogeneity. That basically just refers to when one variable in a model is correlated with another, like how the time of year predicted ice cream sales, but there actually is no causal relationship. 27.5.1 Confounding relationships So, what other relationships can exist other than a direct causal relationship? There could be a spurious relationship between the two variables (let’s call them X and Y), where some third variable (let’s call it Z) is actually the causal factor. \\(Z \\rightarrow X\\) and \\(Z \\rightarrow Y\\) Spurious relationships are especially problematic, because they will result in an association between two variables that is not causal. There could be a chain relationship, where: X \\(\\rightarrow\\) Z and \\(Z \\rightarrow Y\\) There could be multiple causal relationships, where: \\(X \\rightarrow Y\\) and \\(Z \\rightarrow Y\\) Finally, there could be reverse causality, where, \\[Y \\rightarrow X\\] 27.5.2 Example: Women in tech firms When it is laid out like this, it can all seem very simple. Duh, right? Wrong. In fact, thinking carefully about causation can be very hard, and many people are very, very bad at it. Does more education lead to higher turnout, or are socioeconomically advantaged peoople more likely to grow up in a house that encourages both education and voting? Does paying Kaplan $10,000 for LSAT training cause people to have higher scores, or are high achievers just more likely to be willing to pay $10,000? Endogenity is everywhere, and we rarely stop to consider it. And it matters. As an example, consider the 10-page memo distributed internally at Google in the Summer of 2017. The document is a bit dense, but the argument essentialy breaks down like this. Psychology has said that women are different than men in terms of of traits like empathizing vs. systemtizing extraversion/agreeableness neuroticism status seeking desire for work-life balance Since these differences can explain differential interest and success in computer science (and related fields), policies designed to achieve gender balance are “unfair, divisive, and bad for business.” At the time, this memo caused quite a stir and got a lot of media attention. After Google fired the author, some commentators were appalled at Google’s reaction. After all, a lot of high-quality scientific results backed up the claims that men differ from women. Why does Google hate science? The answer is that Google understands social science. The author was making a causal claim, and in doing so he had failed to take into account the most important factor in causal analysis in the social sciences – confounders. The claim is that gender differences in personality/ambition/whatever (\\(X\\)) cause women to be less interested/successful in tech (\\(Y\\)). \\(X \\rightarrow Y\\). Could it be a supurrious relationship? So let’s imagine a world in which there are gender norms (\\(Z\\)). That is, maybe women and men aren’t treated the same. Now, we could imagine that: \\(Z \\rightarrow X\\) and \\(Z \\rightarrow Y\\) This means that gender norms could, for instance, lead women to desire grater work-life balance and that gender norms could work against professional success in the tech industry. If this was the case, it would appear as if things like gender differences in, for example, “desire for work-life balance” are correlated with success/interest in tech. But, in fact, this relationship would not be causal. The same argument goes for personality differences and everything else. No matter how much we try to ignore it, correlation is not causation in the social world because so many things are involved in determining outcomes. To see this more clearly, let’s also note that women have longer hair and shorter fingers than men on average (there are, of course, men with long hair and women with short hair). Following the logic used in this memo we could argue: Physiologically, women may have a harder time engaging in computer science because: Short fingers (makes it hard to type) Long hair (gets in the way of screen) Since these differences can explain differential interest and success in computer science (and related fields), policies designed to achieve gender balance are “unfair, divisive, and bad for business.” And empirically this would be true. If I ran a study comparing the succss of people at Google based on hair length, I would find a negative correlation. But this argument is on its face idiotic because there are so many other things that determine the success of programmers that might happen to also be correlated with finger size and hair length. Those other things are called confounders. And failing to consider them can lead you to a very wrong conclusion. 27.6 Conclusion X causes Y if Y would not have occurred if not for X. Ideally, to establish causation, we use experiments, in which we manipulate X, and see if Y occurs. We use random assignment to ensure that the only difference between the treatment and control groups is the treatment. If we cannot use experiments, then we have to rely on other methods to establish correlation. But because correlation does not imply causation, we need to establish temporal order, establish association, and rule out other explanations. This is much more difficult, and requires more advanced statistical techniques. "],
["finding-average-treatment-effects-in-r.html", "28 Finding average treatment effects in R 28.1 Factors in Data 28.2 Visualizing data as a table 28.3 Creating factor variables 28.4 From factor to string 28.5 Using tapply()", " 28 Finding average treatment effects in R This chapter briefly covers how to find the average treatment effect in R. Average treatment effects Working with factors Comparing data using tables Comparing means using the tapply() function 28.1 Factors in Data We are going to analyze data from an experiment where resumes were sent in for hypothetical job applicants. Importantly, the gender and apparent race of the applicant were randomly assigned. The outcome variable of interest is whether the applicant received a call. First we will read in the data resume &lt;- read.csv(&quot;https://raw.githubusercontent.com/kosukeimai/qss/master/CAUSALITY/resume.csv&quot;) colnames(resume) ## [1] &quot;firstname&quot; &quot;sex&quot; &quot;race&quot; &quot;call&quot; Let’s look at the variabls `sex.’ class(resume$sex) ## [1] &quot;factor&quot; str(resume$sex) ## Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 1 1 1 1 2 1 1 1 2 ... We see that this variable is a “factor.” Further, usting the str() funcion, we can see that the factor has two levels (female and male) that are numerically stored as 1 and 2 respectively. class(resume$call) ## [1] &quot;integer&quot; The call variables, however is an variable containing integers. 28.2 Visualizing data as a table We can then use view this data in a convenient form: table(resume$race, resume$call) ## ## 0 1 ## black 2278 157 ## white 2200 235 28.3 Creating factor variables What if we want a new factor variable? Create a new column in dataframe for callbacks as a factor: resume$callback &lt;- factor(resume$call, levels = c(0,1),labels = c(&quot;No_Call&quot;, &quot;Call&quot;)) Reviewing the table with this factor: table(resume$race, resume$callback) ## ## No_Call Call ## black 2278 157 ## white 2200 235 If we are interested in propotions we can use the prop.table() function around our defined table: prop.table(table(resume$race, resume$callback)) ## ## No_Call Call ## black 0.46776181 0.03223819 ## white 0.45174538 0.04825462 28.4 From factor to string Factors can be problematic if we want variables as text and not really unordered categories categories: class(resume$firstname) ## [1] &quot;factor&quot; This is easy to deal with by using the as.character() function to change it into a text/character variable. class(as.character(resume$firstname)) ## [1] &quot;character&quot; 28.5 Using tapply() Suppose we want to know the “Average Treatment Effect” of being white or black. We can use tapply to create a table of these treatment effects by race. tapply(X = resume$call, INDEX = resume$race, FUN = mean) ## black white ## 0.06447639 0.09650924 The tapply() function will apply function specified by the FUN argument to the variable in the X argument. However, it will do so in separate groups as specified in the INDEX slot. In this instance, it found the mean for resume$call first among black respondents (resume$race==1) and then among white respondents (resume$race==2). Thus, we see that We see that 9.65% of resumes from white applicants received a call while only 0.64% of resumes from black respondents did. In this experiment, the difference between these numbers would be conisdered the average treatment effect. "],
["comparing-means.html", "29 Comparing Means 29.1 Comparing Means for Quantitative Variables 29.2 What Information Goes Into Comparing Means for Quantitative Variables? 29.3 What Formulas are Needed for Comparing Means for Quantitative Variables? 29.4 Putting it all into practice! 29.5 What did you learn?", " 29 Comparing Means 29.1 Comparing Means for Quantitative Variables You will learn how to compare means between two different samples. This should make you all very excited; because it gives you the ability to answer some pretty cool types of questions. For example: Does studying actually get me a better grade on a test? If I eat more sugar, will that make me gain weight? Do people become more partisan as they age, or more moderate, or neither?! The reason that you will be able to answer these questions is we will show you how to compare the means of two groups, to see if they can plausibly be equal. It is important to note that this section will work only for means, not proportions. 29.2 What Information Goes Into Comparing Means for Quantitative Variables? In order to make this comparison, you need 4 pieces of information: The mean for sample 1 (\\(\\bar{y}_1\\)) The mean for sample 2 (\\(\\bar{y}_2\\)) The standard deviation for sample 1 \\(S_1\\) The standard deviation for sample 2 \\(S_2\\) 29.3 What Formulas are Needed for Comparing Means for Quantitative Variables? Hypothesis Testing About Differences: ((Estimate-Null)/Standard Error) Estimate=\\((\\bar{y}_1 - \\bar{y}_2)\\) Null = generally zero (that is, you are guessing that there is no difference between the two samples) If the sample size is large then there will be a corresponding Z-Score. The p-value will be calculated just as we have done with hypothesis testing previously. And Standard Error will be calculated as follows: \\[ SE= \\sqrt{\\frac{S_1^2}{n_1} +\\frac{S_2^2}{n_2}} \\] If the sample size is small, then all calculations will be the same except for two differences. First, the calculation for Standard Error will be as follows : \\[SE=\\hat{\\sigma}\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}\\] where \\(\\sigma^2\\) is the pooled variance, which is calculated as \\[\\hat{\\sigma}=\\sqrt{\\frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1+n_2-2}}\\] Second, the “test statistic” must be taken from the T-Score table, with the Degrees of Freedom= \\((n_1+n_2-2)\\) 29.4 Putting it all into practice! Step 1: Assumptions Either Large “N” or Small “N” (If N is less than 30, use Small “N”) If n is small, we must must be able to assume a normal distribution Null and Alternative Hypotheses For both small and large “N” for differences of the mean, the “Null” would be that the means are the same \\((\\mu_1-\\mu_2=0)\\), and the alternative would be that the means are not equal\\((\\mu_1-\\mu_2\\ne0)\\) Calculating a Test Statistic Formula: (Estimate-Null)/Standard Error First, you calculate the difference of the means (estimate). This merely involves subtracting the sample mean of one sample from the sample mean of the other sample Then you must calculate the Standard Error For Large Sample Size Formula for SE: \\(\\sqrt{\\frac{S_1^2}{n_1} +\\frac{S_2^2}{n_2}}\\) Use Z-score For Small Sample Size Formula for SE: \\(\\hat{\\sigma}\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}\\), where \\(\\hat{\\sigma}\\) is the pooled standard deviation above. Use T-Score The value for the Null is usually “0” 4a. Calculating P-Value For large samples Take the absolute value of the test statistic Find the corresponding probability of the test statistic or more extreme If the Alternative Hypothesis is Sample 1 does not equal Sample 2 (as opposed to, say, Sample 1 is less than Sample 2) then you must multiply the probability by 2 For Small Sample Size Take the absolute value of the test statistic Find Degrees of Freedom: Formula: DF=N1+N2-2 Find the corresponding probability of the test statistic or more extreme, given the degrees of freedom If the Alternative Hypothesis is Sample 1 does not equal Sample 2 (as opposed to, say, Sample 1 is less than Sample 2) then you must multiply the probability by 2 Draw a Conclusion Choose an Alpha, generally 0.01, 0.05, or 0.10 in Political Science If the P-value is larger than the alpha level, you will fail to reject the null hypothesis If the P-value is smaller than the alpha level you will reject the null hypothesis In these videos, Kahn discusses the sampling distribution for the difference of means from two samples and shows how the calculate a confidence interval for that quantitity. He then goes on to show how to conduct a hypothesis test (which, as we know, is a very related task). But watch all of the videos, as I think they are worthwhile for understanding the key points discussed above. Here, he uses a large “N” and therefore a Z-score. If “N” were small, the only difference would be that you would need to calculate P-value using a T-score (with the correct standard error equation), and consider the Degrees of Freedom. 29.5 What did you learn? Goal 1: Learning what goes into comparing two samples of quantitative variables Goal 2: Understanding the formulas for performing a hypothesis test comparing two samples of quantitative variables with both small and large sample sizes Goal 3: Understanding how to perform a hypothesis test comparing two samples of quantitative variables with both small and large sample sizes "],
["t-tests-in-r.html", "30 T-tests in R 30.1 Gerber, Green, &amp; Larimer GOTV Data 30.2 Did these mailers affect turnout? 30.3 Now we can finally run our t-test 30.4 Now a video", " 30 T-tests in R In order to do hypotheses tests for difference in means, we are going to need to Learn how to recode factors Learn how to do hypothesis testing for the difference of means in R 30.1 Gerber, Green, &amp; Larimer GOTV Data We are going to discuss the social pressure experiment I showed in the lecture on causality. In 2006, one of three mailers was sent out as part of a study: Civic Duty (It’s your duty to vote) Hawthorne (You’re being studied) Neighbors (Your neighbors will know if you voted) First let’s read in the data and look at it: social &lt;- read.csv(&quot;https://raw.githubusercontent.com/kosukeimai/qss/master/CAUSALITY/social.csv&quot;) colnames(social) ## [1] &quot;sex&quot; &quot;yearofbirth&quot; &quot;primary2004&quot; &quot;messages&quot; &quot;primary2006&quot; ## [6] &quot;hhsize&quot; 30.2 Did these mailers affect turnout? Here is an example of the mailer they sent out? We want to know if these messages had a causal effect on turnout. Let’s take a look at the messages that were sent. table(social$messages) ## ## Civic Duty Control Hawthorne Neighbors ## 38218 191243 38204 38201 Just to make things easier, let’s recode this variable so we only a have a control group and an overall treatment group (including all three treatment messages). ### You will need to install this package ## install.packages(&quot;car&quot;) library(car) social$messages2 &lt;- recode(social$messages, &quot; &#39;Hawthorne&#39;=&#39;Treatment&#39;; &#39;Civic Duty&#39;=&#39;Treatment&#39;; &#39;Neighbors&#39;=&#39;Treatment&#39; &quot;) table(social$messages2) ## ## Control Treatment ## 191243 114623 30.3 Now we can finally run our t-test Does turnout differ between treatment and control? Using the t.test() function we can find out. We are using the data=social dataset Within that dataset, we want primary2006 to be the outcome Within that datset, we want messages2 to be the treatment We are using a two-sided test var.equal indicates whether we want to assume that the population variance in both samples is identical. In class we always assume they are (to make the math easier, but here we are more agnostic) t.test(primary2006 ~ messages2, alternative = &quot;two.sided&quot;, var.equal = FALSE, data = social) ## ## Welch Two Sample t-test ## ## data: primary2006 by messages2 ## t = -23.869, df = 234580, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.04506412 -0.03822505 ## sample estimates: ## mean in group Control mean in group Treatment ## 0.2966383 0.3382829 30.3.1 Understanding the output The output tells us several things. We get a t-statistics of -23.869 We have 234,580 degrees of freedom. The p-value is equal to \\(2.2\\times 10^{-16}\\), which is a very small number. It shows the 95% confidence interval for the difference in means. It provides the mean in the treatment and control groups. 30.3.2 important options for the t.test() function Adjust for our hypothesis, given \\(H_0\\) is \\(\\mu_a = \\mu_b\\): if \\(H_a:\\) \\(\\mu_a \\neq \\mu_b\\), then alternative = “two.sided” if \\(H_a:\\) \\(\\mu_a &gt; \\mu_b\\), then alternative = “greater” if \\(H_a:\\) \\(\\mu_a &lt; \\mu_b\\), then alternative = “less” Adjust for variance assumptions: If we assume unequal variance, then var.eq = F If we assume equal variance, then var.eq = T 30.3.3 Alternative Usage of the t.test() Function We can also give t.test two vectors. First, create objects for turnout for two specific mailers: neighbors &lt;- social[social$messages == &quot;Neighbors&quot;, &quot;primary2006&quot;] civic_duty &lt;- social[social$messages == &quot;Civic Duty&quot;,&quot;primary2006&quot;] Note that at this point we are looking at two of the treatment groups and comparing them (which is not what we did above). Now we can run our t-test using t.test(x = neighbors, y = civic_duty) ## ## Welch Two Sample t-test ## ## data: neighbors and civic_duty ## t = 18.463, df = 76271, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.05667888 0.07014226 ## sample estimates: ## mean of x mean of y ## 0.3779482 0.3145377 30.4 Now a video Now watch this video "],
["chi-squared-test-of-independence.html", "31 Chi Squared Test of Independence 31.1 What is a Chi-Squared Test of Independence? 31.2 Performing a Chi-Squared Test of Independence 31.3 Seeing How it is Done!", " 31 Chi Squared Test of Independence 31.1 What is a Chi-Squared Test of Independence? When you are given a contingency table (which deals with two variables), you may want to know whether the results indicate that the two variables are related. That is, you might want to see if the two variables are independent of one another. If the variables are independent, then a change in one should not be related with change in the other. If they are dependent, one variable will change in concert with the other. In the Chi-Squared Test, the goal is to compare the values in a contingency table that would be expected if the variables are independent with the values we actually observe. If the observed numbers differs “too much” from what we would expect if they are independent, we can reject the null hypothesis of independence. 31.2 Performing a Chi-Squared Test of Independence To start, you will need a contingency table. You must have the responses tallied in each row and column with the aggregate totals for each row and column as well. Here is an example of table that looks at how age is related to preferences for hamburgers or hotdogs. ~ Hamburger Hotdog Total Under 30 25 13 38 Over 30 22 13 35 Total 47 26 73 Then you will need to specify your explanatory and outcome variables Explanatory variable: The variable that is statistically independent. For instance, in a medical experiment this might be some new drug. Outcome variable: The variables that might be statistically dependent. In experiment, for example, this would be the desease you are trying to cure. Our null hypothesis is that the distribution of the outcome variable will not change as a function of the value of the explanatory variable. Then you must calculate the \\(\\chi^2\\) statistic: First you must find \\(f_{observed} = f_o\\) = observed frequency = the raw count (NOT THE %). This is the number of cases we observe in each cell of the table. Then you must find \\(f_{expected} = f_e\\)= what we would expect for independent samples. This value is equal to: \\[\\frac{\\text{Row total }\\times \\text{Column total}}{\\text{Grand total}}\\] If the null hypothesis true, then we would expect \\(f_{observed} = f_{expected}\\) The formula for the \\(\\chi^2\\) measures the degree to which our obsrerved data differs from what we would expect under the null. Specifically, \\[\\chi^2 = \\sum\\frac{(f_0-f_e)^2}{f_e}\\] Note that we summing over all cells in the table. A few comments on \\(\\chi^2\\): If the samples are truly independent, \\(\\chi^2 \\rightarrow 0\\). If they are actually dependent, \\(\\chi^2 \\rightarrow \\infty\\) This statistic is distributed according to the \\(\\chi^2\\) distribution. And we will use that to calculate p-values. YOU WILL ALWAYS HAVE A POSITIVE NUMBER. Once you get the \\(\\chi^2\\) statistic you must find the Degrees of Freedom to calcualte the p-value. DF= (rows - 1)(columns - 1) In R, the code is: pchisq(\\(\\chi^2\\), df = (rows-1)(columns-1), lower.tail=FALSE) Use this table for exams. Conclusion If your P-value is greater than your \\(\\alpha\\), you will fail to reject the null hypothesis. If it is less than your \\(\\alpha\\), you will reject the null hypothesis. 31.3 Seeing How it is Done! First let’s pose a question: Is age independent of preference between hamburgers and hotdogs Now, let’s look at a chart which gives responses for an actual example. Here is the contingency table from above. ~ Hamburger Hotdog Total Under 30 25 13 38 Over 30 22 13 35 Total 47 26 73 Now let’s get Null and Alternative Hypotheses: Null: Age has no impact on preference between Hotdogs and Hamburgers Alternative: Age does have an impact on preference between Hotdogs and Hamburgers From there, it is time to calculate the Chi Squared statistic First, you must calculate the expected (\\(f_e\\)) values for each of the cells using the following formula above. You calculate it for all of the “response” cells. The \\(f_e\\) for each cell are provided in each cell in parenthesis. ~ Hamburger Hotdog Total Under 30 25(24.5) 13(13.5) 38 Over 30 22(22.5) 13(12.5) 35 Total 47 26 73 Second, you must calculate chi squared statistic with the following formula: \\[\\chi^2 = \\sum\\frac{(f_0-f_e)^2}{f_e}\\] If you do the calculation, you should find 0.061 Now you need the degrees of freedom. DF= (Row-1)(Column-1)=(2-1)(2-1)=1 Note: You might be tempted to say that the # of rows=4 and the # of columns=4. However, we are not looking for the total number of rows and columns in the table, just the number of rows and tables for the “response” cells! Now you calculate the p-value: Given the test statistic of 0.061 and df=1, P-value should come out to around 0.8. This is a very high p-value, and therefore you will fail to reject the null hypothesis, and therefore age does not seem to be related to hamburger/hotdog preference. Below is a video, which will take you through the same type of calculations that were performed above "],
["the-chi-square-test-in-r.html", "32 The Chi Square Test in R", " 32 The Chi Square Test in R "],
["contingency-tables-and-bivariate-data.html", "33 Contingency tables and bivariate data 33.1 Discrete by Discrete Data Display 33.2 Contingency Tables 33.3 Creating Contingency Tables in R 33.4 Discrete by Continuous Displays 33.5 Continuous By Continuous Displays 33.6 The value of Scatterplots", " 33 Contingency tables and bivariate data It is very common that a question relies on a discussion of the relationship between two variables. What type of variables? Age and Vote Intentions Party Ideology and Party Desire to Form Coalitions Income groups and Spending on Alcohol 33.1 Discrete by Discrete Data Display Assume the two variables of interest are both countably discrete. To examine the relationship between these variables, we will use . A contingency table is used to plot membership by groups. Democrat Republican Male 36 56 Female 43 22 Here we see that women are more likely to be Democrats than men. 33.2 Contingency Tables The same contingency table with row percentages. Does it make more sense to percentagize by sex or vote in the previous table? General rule: percentagize by the independent/ explanatory variable. Below we do it by gender (i.e. by row). Democrat Republican Male 39.1% 60.9% Female 66.1% 33.9% Note that the percentages add up to 100 on each row, not over each column. 33.3 Creating Contingency Tables in R Recall: Contingency tables take factors. If it is not a factor, you will need to convert it (see the chapter on average treatment effects.) Code like this will give you a contingency table: # Frequency freq.tab &lt;- with(data, table(factor1, factor2)) Code like this will give you a contingency table with percentages: #Percentage with(data, round(prop.table(freq.tab)*100, 2)) Code like this will conduct the \\(\\chi^2\\) test for independence: # Chi-Square chisq.test(freq.tab) 33.4 Discrete by Continuous Displays When we have one discrete variable and one continuous Rely on some of the methods that we have used previously! Options: boxplots for each group; histograms for each group; plot the distribution of observations for each group. You can make plots like this using the boxplot() function: boxplot(y~x,data = dataset, xlab = &quot;Informative X label&quot;, ylab = &quot;Informative Y label&quot;, main = &quot;Informative Title&quot;,col = c(col1, col2, ....)) 33.5 Continuous By Continuous Displays If we want to examine the relationship between two continuous variables graphically, we will utilize a scatterplot. Income by Age Income by Ideology (21-point scale) Note: when ordered categories are plenty enough, consider them as continuous plot(x, y, data = dataset, main = &quot;Informative Title&quot;, xlab = &quot;Informative X label&quot;, ylab = &quot;Informative Y label&quot;, pch = 19) 33.6 The value of Scatterplots Scatterplots allow us to assess the direction and strength of a relationship between 2 variables. 3 types of relationships: Positive: As x increases, y also increases. Negative: As x increases, y decreases. No Relationship: There is no relationship between the variables. 33.6.1 In R, we can only make scatterplots with numeric variables! Check whether the variable is numeric using code like this: str(datasetname$variableofinterest) is.numeric(datasetname$variableofinterest) If your variable is not numeric in structure but convertible to be numeric, such as numbers as factor levels or numbers as characters within quotation marks, we can convert it to a numeric variable (only if it makes sense): datasetname$newvariable &lt;- as.numeric(datasetname$variableofinterest) "],
["bivariate-regression.html", "34 Bivariate Regression 34.1 Learning Objectives 34.2 Summary 34.3 What is a linear regression? 34.4 When do we use linear regression?", " 34 Bivariate Regression 34.1 Learning Objectives Understand what linear regression is Understand what linear regression is used for Know how to calculate a linear regression 34.2 Summary Linear regression is a type of model used to describe the relationship between two variables. The linear regression just means we are drawing a straight line through the data. The line we choose is the one which minimizes the sum of the squared distances between the data point and the expected value along that line. A linear regression can tell us whether there’s an association between two variables and how strong the relationship is. 34.3 What is a linear regression? So far, the tools we have covered can help us tell whether there is a difference between two groups (e.g., a treatment and control group). Now we’re going to cover a tool that helps us determine the relationship between two variables in one population. For example, let’s say we want to look at the relationship between age and political ideology. A linear regression is a line that describes the “best fit” for the observed data. We will talk about how we determine what that best fit is in the next section, but right now, let’s talk about the components of a line. A line can be written as where \\(\\beta\\) is the slope of the line, and \\(\\alpha\\) is the y-intercept (the value of y when x is zero). If the line is horizontal, then the slope would be zero. The slope is given as the change in y divided by the change in x, or “rise over run.” In the line above, the y intercept is .8, and the slope is \\(\\frac{(4-2)}{(8-3)} = \\frac{2}{5}\\). Therefore, the equation of the line above can be given as \\(y=.8+.4 \\times x\\) . 34.4 When do we use linear regression? Imagine you want to determine the relationship between the average income in a county, and the percentage of voters in that county who vote. In this case, there are two interval variables: an average income and a percent. Imagine that your hypothesis is that having a higher income causes voter turnout. What you need is to have some way to measure the relationship between these two variables. That’s what regression provides. Linear regression is the most common way that researchers determine the relationship between two variables. Remember the parts of a line. In the equation for a line, there are two variables, x and y. According to your hypothesis, voter turnout is the dependent variable, and income is the independent variable. In our equation for a line, e let y represent the dependent variable, and let x represent the independent variable. This means that we have a line that tells us that \\[Turnout = \\alpha + Income \\times \\beta.\\] Your theory would lead you to expect that \\(\\beta\\ne0\\) (that there is a relationship between turnout and income). 34.4.1 How to choose the “best” line The problem then becomes, how do we determine the “correct” values of \\(\\alpha\\) and \\(\\beta\\)? To answer that question, we have to take a step back, and note that, unless one variable perfectly predicts another (which is extremely rare in social science), your data points are unlikely to all fall exactly on one line. This means that there is some error inherent in linear models. When we take into account error, \\(\\epsilon\\), we get the following formula: \\[y = \\alpha + x \\beta + \\epsilon \\] In regression models we assume that the errors are normally distributed. (That is very, very important. Don’t forget it.) When we say that errors are normally distributed, what we mean is that if you were to plot the distance from the actual y-value to the expected y-value based on the line, the resulting plot would be a normal distribution with a mean at 0, and a standard deviation of \\(\\sigma^2\\). 34.4.1.1 Fitted values This means that , our best guess for \\(y\\) based on the regression and some observed value of \\(x\\), is given by the equation \\[ \\hat{y_i} = \\alpha+x_i \\beta\\] 34.4.1.2 Estimating \\(\\alpha\\) and \\(\\beta\\) So how do we know what numbers to plug in for \\(\\alpha\\) and \\(\\beta\\)? We plug in the numbers that result in the lowest sum of squared error. To calculate this, we use the Sum of Squared Error (SSE). The equation is: \\[SSE = \\sum_{i=1}^n(y_i - \\hat{y_i})^2 = \\sum (y_i - \\hat{\\alpha} - x_i\\hat{\\beta})^2\\] We use this equation to determine the value of \\(\\alpha\\) and \\(\\beta\\) for which the SSE is lowest. The solution is the two following equations \\[\\hat{\\beta} = \\frac{\\sum_{i=1}^n\\Big((X_i - \\bar{X})(Y_i - \\bar{Y})\\Big)}{\\sum_{i=1}^n(X_i - \\bar{X})^2}\\] \\[ \\hat{\\alpha} = \\bar{Y} - \\hat{\\beta}\\bar{X}\\] 34.4.2 Additional resources The video below does a very thorough job of breaking down how linear regression. Here’s another video on how linear regressions work. It’s a little shorter. "],
["marginal-and-conditional-standard-deviation.html", "35 Marginal and Conditional Standard Deviation 35.1 Learning Objective: 35.2 Marginal standard deviation 35.3 Conditional standard deviation", " 35 Marginal and Conditional Standard Deviation 35.1 Learning Objective: Understand the difference between marginal and conditional standard deviation. At this point, you already know the basics of bivariate regression. In a bivariate regression, you use a set of independent x values to predict your y values using a simple linear model \\(y = a + \\beta x\\). For instance, let’s say that you have a dataset that contains information on the number of years of education a person has completed and their income. In your data set, the mean income is 40,000 dollars and the standard deviation for the entire data set, that is, the average distance away from the mean, is 18,000 dollars. 35.2 Marginal standard deviation This average distance a data point is from the mean for the entire data set is described by the marginal standard deviation. It is just the definition of the standard deviation we have been using since the concept was introduced. It is calculated by: \\[ \\sqrt{\\frac{\\sum(y- \\bar{y})^2}{n -1}}\\] 35.3 Conditional standard deviation You also have a regression function which tells you that \\(E(y) = -5000 + 3000x\\). This would mean that people with a high school education (x = 12 years of education), should have mean income \\(\\hat{y} = E(y) = -5000 + 3000 (12) = 31,000\\). We do not expect every person with 12 years of education to have an income of $31,000. Again, that is just the mean for that value of x, or the expected value. Instead, we expect the y values to vary a bit around the expected value. The conditional standard deviation is the standard deviation conditional on a certain value of x. So if x = 12, the conditional standard deviation tells me on average how far I can expect y to fall around that specific \\(\\hat{y}\\). The conditional standard deviation is calculated by: \\[ S = \\sqrt{\\frac{SSE}{n-2}} \\] The conditional standard deviation is almost always smaller than the marginal standard deviation. Why? Because the marginal standard deviation describes the average distance away from the mean for all x, while the conditional standard deviation tells you the average distance an expected y will be from the mean for only one x. "],
["inference-for-regression.html", "36 Inference for Regression 36.1 Learning objectives: 36.2 Inference with regression 36.3 Table Interpretation", " 36 Inference for Regression Has a professor ever given you a study to read as homework with a confusing table with numbers, asterisks, and letters? Today, we are going to teach you to understand that table so you can impress your friends, relatives, and (most importantly) your professors with your mastery of regression analysis. 36.1 Learning objectives: Be able to make inferences from regression using hypothesis tests, t-statistics, p-values, and confidence intervals. Be able to interpret a regression table. 36.2 Inference with regression 36.2.1 Hypothesis tests We can use hypothesis tests to determine whether or not a relationship exists between two variables in our prediction equation. Our null hypothesis is that no relationship exists between the two variables. This occurs when the slope describing the relationship between x and y, \\(\\beta\\), is zero, or a flat line. This is described by \\(H_0: \\beta = 0\\), while the alternative hypothesis Ha can either be \\(H_a: \\beta \\neq 0\\) or \\(\\beta &lt; 0\\) or \\(\\beta &gt; 0\\), depending on what we are trying to assess. To calculate our test statistic, we subtract our calculated slope value, \\(\\hat\\beta\\), from our null hypothesis value and divide by the standard error. That is: \\[ t = \\frac{\\hat\\beta - 0}{\\hat{se}} \\] The formula for the standard error of the slope is: \\[ \\hat{se} = \\frac{\\hat{\\sigma}}{\\sqrt{ \\sum(x_i - \\bar{x})^2)}} \\] where \\[\\hat\\sigma = \\sqrt{\\frac{SSE}{n-2}}\\] The test statistics, \\(t\\) is distributed according to the t distribution. We calculate the degrees of freedom by subtracting from n the number of parameters we are estimating. Since here we only have a bivariate regression, we would have degrees of freedom \\(= n -2\\) (one for \\(\\alpha\\) and one for \\(\\beta\\)). The p value describes the probability of obtaining the observed slope were the true slope describing the relationship between the two variables were zero and the null hypothesis were true. It is our measure of surprise. A small p value suggests the regression line has a non-zero slope. That is, if the slope were actually zero, than the strength of the relationship between x and y would be very surprising. If the p value is less than our predetermined \\(\\alpha\\) (DONT GET CONFUSED … this is not the same \\(\\alpha\\) we sometimes use for a constant in a regression) we can reject the null hypothesis that there is no relationship between the two variables. 36.2.2 Confidence intervals We do not just want to know, however, that the slope is non-zero and that a relationship exists between the variables. We want to know the extent of that relationship, which we can determine using confidence intervals. We calculate the confidence interval for the slope using the formula: \\[\\hat\\beta \\pm t*(\\hat{se})\\] with \\(df = n - 2\\) and the \\(se\\) calculated exactly as it is in the above hypothesis test. Let’s do an example. In a data set of size n= 100 where x = size of house in square feet and y = selling price, our observed slope is 126.6, meaning that for each square foot increase in the size of the house, the selling price increases by 126.6 dollars (on average). Our \\(se\\) equals 8.47. In a 95% confidence interval for the true slope value \\(\\beta\\), we calculate with \\(df = 100 - 2\\) and \\(t = 1.984\\): \\[ 126.6 \\pm 1.984 (8.47) = 126.6 \\pm 16.8 = [110, 143]\\] If we were to take many, many samples of size \\(n = 100\\), we would be confident that 95% of the time that the true mean increase in selling price for each 1 square foot increase in size would fall between 110 dollars and 143 dollars. 36.3 Table Interpretation In your readings for your other Poli Sci classes have you ever come across a chart like this? Well, now you have the tools to interpret what it means. For the moment, ignore the fact that there are multiple variables. We will come back to that later in the class. Imaging that this is a bivariate regression with only one explanatory variable and a constant. For example, dependent variable in this chart is a house incumbent’s legislative electoral vote share from 1980 - 1996 (the left panel). The first explanatory variable listed here is roll-call ideological extremity. The first coefficient listed is the slope. It shows us that the relationship between incumbent vote share decreases by .085 for each increase in roll-call ideological extremity. The number in parentheses is the standard error for that \\(\\beta\\). It tells us the variability in sample slope values that would result from repeatedly selecting random samples of incumbent vote share and calculating prediction equations. The stars next to the slope coefficient for roll call extremity shows us that the relationship between incumbent vote share and roll call ideology is significant at the \\(\\alpha = .01\\) or the \\(\\alpha = .001\\) level (see the note at the bottom of the table). "],
["basic-linear-regression-in-r.html", "37 Basic Linear Regression in R", " 37 Basic Linear Regression in R "],
["how-to-plot-residuals-in-r.html", "38 How to Plot Residuals in R", " 38 How to Plot Residuals in R Around 3:30 is where residuals starts "],
["regression-example.html", "39 Regression example", " 39 Regression example Let’s work work with a dataset that comes from Brandice Canes-Wrone, Brady David, and John Cogan. 2002. “Out of Step, Out of Office: Electoral Accountability and House Members Voting.” American Political Science Review 96 (1): 127 - 140. The dataset consists of election returns for incumbent members of congress and includes many variables thought to influence elections and election behavior. Download the data here The codebook for this example is here. Once you have the data, you can load it up and take a peek. elections.data&lt;-read.csv(&quot;https://tinyurl.com/yb4mztc7&quot;) table(elections.data$year) ## ## 1956 1958 1960 1964 1966 1968 1970 1972 1974 1976 1978 1980 1982 1984 1986 ## 336 298 328 349 344 349 332 324 325 335 325 347 318 354 331 ## 1988 1990 1992 1994 1996 ## 343 325 313 337 374 The dataset contains variables related to elections for the U.S. House, including spending patterns of incumbents and challengers. Data are at the electoral district level. Each observation represents a specific contest. Each line of the data includes information about the district, the incumbent, and the challenger (if there is one). Note that some variables are in logarithmic scale. We want to test the following hypothesis: As the challenger’s campaign spending increases, the incumbent’s campaign spending increases. \\[H_0: \\beta=0\\] \\[H_0: \\beta \\neq 0\\] Next it is always a good idea to take a look at the data. Make a scatterplot for the two variables of interest WITHOUT the least-squares line. As always, we want to have Have clear axes labels. plot(elections.data$chalspend, elections.data$incspend, xlab=&quot;Challenger spending (log)&quot;, ylab=&quot;Incumbent spending (log)&quot;, pch=c(19), # points are solid circles main=&quot;Incumbent vs. Challenger spending&quot;, # Label at the top col=rgb(0,0,0,.1)) ## make the points mostly transparent Conduct a regression analysis to test the hypothesis RegModel.1 &lt;- lm(incspend~chalspend, data=elections.data) summary(RegModel.1) ## ## Call: ## lm(formula = incspend ~ chalspend, data = elections.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.1221 -0.3860 0.0507 0.4140 2.1419 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.292518 0.075658 136.04 &lt;2e-16 *** ## chalspend 0.227611 0.006848 33.24 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6528 on 3249 degrees of freedom ## (3436 observations deleted due to missingness) ## Multiple R-squared: 0.2537, Adjusted R-squared: 0.2535 ## F-statistic: 1105 on 1 and 3249 DF, p-value: &lt; 2.2e-16 So what does this mean? The prediction equation for the model is: \\[incspend=10.29+0.22*chalspend\\] This means that for every one unit increase in challenger spending (measured in log(dollars)), we expect a 0.22 increase in incumbent spending. We can also see that the \\(t-value\\) for tis coefficient is 33.25, and the p-value is very small. This means we can reject the null hypothesis of no relationship. Let’s Go back to the scatterplot and dd the least-squares line. plot(elections.data$chalspend, elections.data$incspend, xlab=&quot;Challenger spending (log)&quot;, ylab=&quot;Incumbent spending (log)&quot;, pch=c(19), # points are solid circles main=&quot;Incumbent vs. Challenger spending&quot;, # Label at the top col=rgb(0,0,0,.1)) ## make the points mostly transparent abline(lm(incspend~chalspend, data=elections.data)) "],
["multivariate-regression.html", "40 Multivariate Regression 40.1 A bit more on multivariate regressions 40.2 3D Representation of the Data", " 40 Multivariate Regression First, watch this video: 40.1 A bit more on multivariate regressions What if we’re interested in modeling the relationship between two independent variables and a dependent variable? Linear regression can do that through use of matrices and the regression equations. Fortunately, we have R to solve all the matrix algebra for us. Multivariate Regression can be used to examine the joint relationship between any number of independent variables and a dependent variable. It is important to remember that all of the variables involved have to be continuous or dichotomous. However, also know that linear regression is quite robust to assumption violations. Thus, we can still get workable results if we violate this. Generally, we can use this prediction equation: \\[\\hat{y_i} = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\ldots + \\beta_k x_{ik}\\] Linear model in R: lm(formula = y ~ x.1 + x.2 + ... + x.k, data = my.data) We can then apply it to the dataset “mtcars”, which comes in R. It includes information about various car models in 1974 (boring, right, but stay with me). You can see the variables by typing help(&quot;mtcars&quot;) We want to test the very simple theory that the miles per gallon a car gets is a function of its weight and its size (displacement). \\[MPG = \\beta_0 + \\beta_1 weight + \\beta_2 displacement\\] Linear model in R: data(&quot;mtcars&quot;) fit &lt;- lm(mpg ~ wt + disp, data = mtcars) summary(fit) ## ## Call: ## lm(formula = mpg ~ wt + disp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4087 -2.3243 -0.7683 1.7721 6.3484 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.96055 2.16454 16.151 4.91e-16 *** ## wt -3.35082 1.16413 -2.878 0.00743 ** ## disp -0.01773 0.00919 -1.929 0.06362 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.917 on 29 degrees of freedom ## Multiple R-squared: 0.7809, Adjusted R-squared: 0.7658 ## F-statistic: 51.69 on 2 and 29 DF, p-value: 2.744e-10 Clearly there is a relationship between mileage and weight and, to a lesser degree, the size of the car. But keep in mind that we are “controlling for” the other variables. So, a one unit increase in weight decreases MPG by 3.35 even after accounting for the fact that heavier cars tend to be bigger. However, the size (dispersion) variable is not significant in this regression. Does that mean that bigger cars aren’t lower in MPG? Of course not. data(&quot;mtcars&quot;) fit &lt;- lm(mpg ~ disp, data = mtcars) summary(fit) ## ## Call: ## lm(formula = mpg ~ disp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.8922 -2.2022 -0.9631 1.6272 7.2305 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.599855 1.229720 24.070 &lt; 2e-16 *** ## disp -0.041215 0.004712 -8.747 9.38e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.251 on 30 degrees of freedom ## Multiple R-squared: 0.7183, Adjusted R-squared: 0.709 ## F-statistic: 76.51 on 1 and 30 DF, p-value: 9.38e-10 Here we see that there is a clear relationship between displacement and MPG. The relationship only “goes away” when we look at the effect of a one unit change in displacement controlling for weight. From this we can understand that: A heavier car that is not any bigger will have worse MPG. A bigger car that does not get heavier has a more uncertain relationship with MPG. 40.2 3D Representation of the Data Let’s take a look at the data. pairs(mtcars[,c(&quot;mpg&quot;, &quot;wt&quot;, &quot;disp&quot;)]) Here we see that all three of these variables seem to be related to eachother. Let’s tryand see this in three dimensions (you may need to install the scatterplot3d package.) library(scatterplot3d) attach(mtcars) ## The following objects are masked from mtcars (pos = 4): ## ## am, carb, cyl, disp, drat, gear, hp, mpg, qsec, vs, wt s3d &lt;-scatterplot3d(wt,disp,mpg, pch=16, highlight.3d=TRUE,type=&quot;h&quot;, main=&quot;3D Scatterplot&quot;) fit &lt;- lm(mpg ~ wt+disp) This plot shows the data points for in a three-dimensional space. But can we now draw the regression “plane” through the points? s3d &lt;-scatterplot3d(wt,disp,mpg, pch=16, highlight.3d=TRUE, type=&quot;h&quot;, main=&quot;3D Scatterplot&quot;) fit &lt;- lm(mpg ~ wt+disp) s3d$plane3d(fit, col = &#39;black&#39;) Remember, the regression equation is just a representation of he plane. And the “tilt” of the plane represents “independent” relationship between each covariate and the outcome “controlling for” the other variables. "],
["multivariate-regression-example-teaching-about-evolution.html", "41 Multivariate regression example: Teaching about evolution", " 41 Multivariate regression example: Teaching about evolution Teaching evolution can be politically controversial, but is scientifically uncontroversial and necessary for understanding biology. How can we determine if politics is skewing the teaching of evolution in schools? Are there individual biases, institutional biases, or a combination (perhaps an interaction) of both? Effects of qualifications and experience of individual teachers as well as state-level effects on how many hours biology teachers devote to teaching evolution in class. We want to ask ourselves: Does teaching vary by state? Does teaching vary by the experience of the teacher? The data are from the National Survey of High School Biology Teachers and consist of 854 observations of high school biology teachers who were surveyed in the spring of 2007. The outcome of interest is the number of hours a teacher devotes to human and general evolution in his or her high school biology class (hrsallev). To see the explanatory variables, see the codebook. Import the evolution.dta data set into R. Go ahead and open the codebook as well.First we need to recode some of the data to tell R where there are missing values. The female variable codes 1 if the teacher is female, 0 if the teacher is male, and 9 if the sex of the teacher is unknown. Recode all entries in the female column listed as 9 as NA, and write the code you used to do this below. library(foreign) evolution&lt;-read.dta(&quot;http://jmontgomery.github.io/PS363Extra/evolution.dta&quot; , convert.factors=FALSE) table(evolution$female) ### The 9&#39;s should be coded as missing. ## ## 0 1 9 ## 405 436 13 evolution$female[evolution$female == 9] &lt;- NA Begin by creating a multivariate regression model using hrsallev as your outcome variable. For explanatory variables, use female, phase1, senior_c, notest_p, biocred3, degr3, evolcourse, certified, idscitrans, confident. mod.hours&lt;-lm(hrs_allev~phase1+senior_c+notest_p+female +biocred3+degr3+evol_course+certified+idsci_trans +confident,data=evolution) summary(mod.hours) ## ## Call: ## lm(formula = hrs_allev ~ phase1 + senior_c + notest_p + female + ## biocred3 + degr3 + evol_course + certified + idsci_trans + ## confident, data = evolution) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.637 -6.069 -1.294 4.610 32.011 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.2182 1.1929 8.566 &lt; 2e-16 *** ## phase1 0.5213 0.2755 1.892 0.0588 . ## senior_c -0.5011 0.3116 -1.608 0.1082 ## notest_p 0.7584 0.7067 1.073 0.2835 ## female -1.3345 0.6025 -2.215 0.0270 * ## biocred3 0.5071 0.5077 0.999 0.3182 ## degr3 -0.3432 0.3911 -0.878 0.3804 ## evol_course 2.5657 0.6307 4.068 5.19e-05 *** ## certified -0.5562 0.7204 -0.772 0.4403 ## idsci_trans 1.9382 1.1269 1.720 0.0858 . ## confident 2.6498 0.4508 5.879 5.99e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.414 on 830 degrees of freedom ## (13 observations deleted due to missingness) ## Multiple R-squared: 0.117, Adjusted R-squared: 0.1064 ## F-statistic: 11 on 10 and 830 DF, p-value: &lt; 2.2e-16 Interpret the intercept of the model. The intercept is 10.2, meaning that theoretically if all the independent variables took on a value of 0 (the teacher had no degree, wasn’t certified, was male, had no credibility, etc), there would still be 10.2 hours taught. This is the baseline from which all the other variables come. According to the model, what is the effect of taking the college-level evolution course. How does this compare to the effect of holding an additional degree? Evol_course’s coefficient is 2.57, meaning that those who took a college-level evolution course taught 2.57 more hours of evolution in classes than those who did not take a college-level evolution course. The effect of holding an additional degree is -0.34, meaning those who hold an additional degree usually teach 0.34 fewer hours of evolution than those who do not hold an additional degree. "],
["fixed-effects-and-interactions.html", "42 Fixed effects and interactions 42.1 Fixed effects 42.2 Interactions", " 42 Fixed effects and interactions 42.1 Fixed effects In this section, you will learn about how fixed effects models work. So far we have just talked about regression in the abstract. Today, we are going to introduce you to a different type of model that will be helpful as you begin to think about what kind of statistical model you want to use for your research project. Fixed Effects is extremely useful, as it allows us to study a specific treatment, while accounting for time-invariant differences across the same units of study over time, such as states or cities. It allows us to test a certain effect or theory across that unit. This differs from other models we’ve seen in the sense that instead of looking at the differences between units such as cities or states (sometimes called a cross-sectional study), we are instead looking at the differences in repeated observations within them over time (sometimes called a panel study). 42.1.1 How about an example? We’re interested in this research question: Are elections fought over non-economic issues more when globalization increases? Let’s say our panel data includes repeated observations of Canada, France, the United States and the United Kingdom. Now, our knowledge up onto this point would tell us to look at how these countries differ among each other. How does France differ from Canada or the United States in terms of this question? The problem with this is that there exist many differences between the countries that we can’t account for in our model. 42.1.1.1 What can we do in order to eliminate this bias? How about instead of looking at the difference among each of those countries, we look at the treatment within them over time. More specifically, with repeated observations of the United States, have elections been fought over non-economic issues more when globalization increases by looking at US elections in 2000, 2002, and 2004? What about Canada, France and the United Kingdom? Have repeated observations of elections in each of those countries showed an increase in these kinds of non-economic focused races? By researching this question using a fixed effects model, we can, to a certain extent, control for this. We are able to account for time-invariant differences across units and allow us to limit the bias we would have been faced with had we used a different model. In order to accomplish this in a regression, we use dummy variables. Like you recently saw in class, dummy variables are simply categorical variables recoded in terms of a quantitative value. In a fixed effects model, we use k-1 dummy variables for k units of study. In this case, we would use countries as the dummy variable. It doesn’t matter what you use as the baseline but make sure you know which one it is as it is necessary when interpreting your results! 42.1.1.2 Visual example Without a fixed effects model, our regression would look something like this: There’s not really much to interpret here. There appears to be no relationship between these two variables. Now, with a Fixed Effects Regression, we get something that looks like this: Now this is something we can interpret! As we can see, for each country, as globalization score increases, so does the emphasis on non-economic issues. Without fixed effects, as we saw above, the regression line was almost horizontal and gave us no usable information. But by looking at how these countries change over time as the explanatory variable changes, we are able to get a more detailed understanding of what such effects actually look like. 42.1.2 Summary When you are studying time-varying treatments using panel data, using a Fixed Effects Model is one way to go. Having a different constant for each unit allows you to account for time-invariant confounders. But there are a few things to worry about here: If you have the same unit observed many times, you may not have truly independent observations. This can lead us to have incorrect (too small) standard errors. In a more advanced course you would be taught how to handle these issues. You still have to worry about time-varying confounders. 42.2 Interactions In this section, you will learn the basics of what an interaction is, what an interaction term looks like in a multiple regression as well as how to interpret it. By now, you have (hopefully) learned what a multiple regression is. The basics of such regressions being that you are testing multiple potential explanatory variables in order to ascertain the effect of one variable (\\(x_1\\)) controlling for some other variable (\\(x_2\\)). But what if one explanatory factor in some way affects the other’s ability to explain the outcome? That is, what if the relationship between one variable (\\(x_1\\)) and the outcome (\\(y\\)) varies depending on the value of a third veriable (\\(x_2\\)). This is what we call an interaction. 42.2.1 Defining an interaction An interaction exists between \\(x_1\\) and \\(x_2\\) in their true effects on Y when the true effect of one predictor on \\(y\\) changes as the value of the other predictor changes. Mathematically, this looks like: \\[y=\\beta_0 + \\beta_1 x_1 + \\beta_2x_2 + \\beta_3(x_1 \\times x_2) \\] At first look, this may seem strange, right? With regressions, we’re interested in understanding how an explanatory variable affect some specific outcome variable. Why would we be interested in how explanatory variables relate to each other? By looking at the interaction between explanatory variables, we are able to tell whether or not a change in one of them alters the ability of the other to explain your outcome. Mathematically, this is not too tricky. We just make a new variable \\((x_1 \\times x_2)\\) and calculate the regression coefficients as normal. But allowing for interactions between explanatory variables will change the way in which we interpret the outcome. 42.2.2 How about an example? Let’s say we’re interested to know whether or not education strengthens the relationship between party ID and health care reform opinion. In this example, we have three variables: Support for the Obama health care plan (1=Strongly oppose, 2=Moderately favor, 3=Moderately oppose, 4=Strongly oppose). Party ID (1=GOP, 2=Independent, 3=Demorat) College education (0=Did not graduate college, 1=Did graduate college) Here is a depiction of the data from a poll in 2009. By now, you should be used to seeing an equation that looks like this (without an interaction term): \\[\\mbox{Health Care Reform Support}=\\beta_0 +\\beta_1(\\mbox{Party ID}) + \\beta_2(\\mbox{College})\\] Now we can add in the interaction term. \\[\\begin{array}{r}\\mbox{Health Care Reform Support} ~=\\\\ ~ \\end{array}\\begin{array}{l}\\beta_0 +\\beta_1(\\mbox{PartyID}) + \\beta_2(\\mbox{College}) + \\\\ \\underset{interaction}{\\underbrace{\\beta_3(Party \\times College)}}\\end{array}\\] This term on the end is an interaction term where we simply multiple the college variable by the party variable. That’s all it is! It’s a regression with just another covariate added. It’s interpreting the results where things get a bit tricky. 42.2.3 But what does it all mean? By adding this interaction term, we are able to see whether or not, changes in one of the explanatory variables in turn creates changes in the other in their effects on support of health care reform. Variable Coefficient Constant 1.402 (0.055) Party 0.789 (0.039) College -0.111 (0.085) Party \\(\\times\\) College 0.326 (0.061) N = 981 \\(R^2\\) = 0.4979 How do I read this table? That can be tricky. To some extent, you can read this table like any other: simply divide the coefficient by the standard error and see what is significant. But actually, it can be a bit more comples. The prediction equation is: HCR support = (1.40) + (0.79 \\(\\times\\) Party) – (.11 \\(\\times\\) College) + (.33 \\(\\times\\) Party \\(\\times\\) College) 42.2.3.1 What does it mean though? Well, the easiest way to see this is to choose specific values for one of the two terms. First, what is the relationship between Party ID and support for the health care reform among non-college educated respondents? You can find the answer just by putting in the value \\(0\\) in the equation above wherever it says “College.” HCR support = (1.40) + (0.79 \\(\\times\\) Party) – (.11 \\(\\times\\) 0) + (.33 \\(\\times\\) Party \\(\\times\\) 0) HCR = 1.40 + 0.79 \\(\\times\\) Party Second, what is the relationship between Party ID and support for the health care reform among college educated respondents? You can find the answer just by putting in the value \\(1\\) in the equation above wherever it says “College.” HCR support = (1.40) + (0.79 \\(\\times\\) Party) – (.11 \\(\\times\\) 1) + (.33 \\(\\times\\) Party \\(\\times\\) 1) HCR support = (1.40 - 0.11) + ((0.79+0.33) \\(\\times\\) Party) HCR support = 1.29 + 1.12 \\(\\times\\) Party Here are the two lines that we end up with: 42.2.3.2 So … what? So what this means is that the relationship between PartyID and support for health care reform is stronger (steeper) amont college-educated respondents. We know this because: The slope is steeper for college-educated respondents (the interaction term is positive) The interaction term is signifcantly different from zero. 42.2.4 A video on interactions in R "],
["difference-in-differences-part-i.html", "43 Difference-in-Differences, Part I 43.1 What does difference-in-differences seek to accomplish? 43.2 Difference-in-differences (DiD) as a “natural experiment” 43.3 Theoretical assumptions underlying the DiD design", " 43 Difference-in-Differences, Part I Learning Objectives: Learn what the difference-in-differences research design seeks to accomplish Conceptualize diff-in-diff as a “natural experiment” Understand what theoretical assumptions justify using a diff-in-diff design 43.1 What does difference-in-differences seek to accomplish? Difference-in-differences is an empirical strategy used to make claims of causal inference. Using difference-in-differences (hereafter “diff-in-diff”), a researcher considers some intervening factor (the introduction of a policy, a stock market shock, a terrorist attack, etc.) and seeks to prove that factor to be the cause of some outcome (bank closures, casualties, vote share, etc.). As a running example for this page, we’ll consider the task of estimating the effect of closing early-voting voting precincts in 17 counties in North Caroilna had on turnout. The technique can be broken into the following steps: Identify the population (A) to which the intervention was applied Identify a comparable population (B) to which the intervention was NOT applied. In our example, we would divide all 100 counties in North Carolina into group A, counties where early voting precincts were reduced, and group B, counties where the number of early voting precincts was kept the same. Identify the characteristic of interest of both populations before and after the intervention (A-pre, A-post, B-pre, B-post). This means we need to know the turnout rate for all counties in group A before the reform (e.g., in 2012) and after the reform (e.g., in 2016). Likewise, we need to calculate the turnout rate in for counties in group B before the reform (2012) and after the reform (2016). Find D1: the difference between A-pre and B-pre. This would be difference in the average turnout rate between group A and group B in 2012. Find D2: the difference between A-post and B-post. This would be the difference in the average turnout rate between group A and gropu B in 2016. Calculate D2 minus D1 to find the difference-in-differences (DD). This would be a measure of how the differences between turnout rates in group A and B changed as a result of the closed precincts. The final DD value you obtain is the estimated causal effect of the intervention (conditional on some key assumptions to be addressed later). Here’s a visual illustration of how this research strategy works: Think of the blue circles as population A, and the red triangles as population B. (For simplicity, consider the Y-value to be the population mean for the characteristic of interest.) The gap between the two blue line segments, labeled “Effect of treatment”, is the DD value defined above. It’s the difference between the change we observed in the control group before and after treatment, and the change we observed in the treatment group before and after treatment. The difference between these changes is the DD estimator. 43.2 Difference-in-differences (DiD) as a “natural experiment” The diff-in-diff strategy seeks to replicate the process of a researcher-controlled randomized experiment, but using naturally-generated data. In a randomized experiment, the researcher divides her population into treatment and control groups which are (theoretically) identical on all relevant covariates, and then applies a treatment only to the treatment group. Any difference in post-treatment characteristics can then be causally attributed to the treatment. Real-world phenomena don’t divide populations into treatment and control groups as neatly and evenly as do randomized experiments. However, in some circumstances, they can come close. Diff-in-diff does not depend on treatment and control groups being equal across covariates; rather, the two groups need to be different in the same way and to the same degree over time. In using a diff-in-diff strategy, we must assume that the two groups would have been consistently different before and after the intervention (“treatment”) but for the intervention, so any difference in the differences between the two groups before and after treatment can be causally attributed to the treatment. 43.3 Theoretical assumptions underlying the DiD design Making plausible claims of causal inference under the diff-in-diff design depends on an empirically untestable assumption: that in the absence of the treatment, the difference between the two populations would have been the same before and after the treatment. As this is a claim about a counterfactual, it can never be empirically tested. The strength of a difference-in-differences research design rests on how well the claim is justified theoretically. We’ll take a closer look at these assumptions in a running example on the next page. "],
["difference-in-differences-part-ii.html", "44 Difference-in-Differences, Part II 44.1 Learning Objectives 44.2 One real-world example 44.3 Executing difference-in-differences with aggregate-level data 44.4 Executing difference-in-differences with individual-level data", " 44 Difference-in-Differences, Part II 44.1 Learning Objectives Consider a real-world phenomena to which diff-in-diff can be applied Execute diff-in-diff with aggregate-level data Execute diff-in-diff with individual-level data 44.2 One real-world example A series of train bombings in Madrid on March 11, 2004 left 191 people dead and 1,500 injured. In the Spanish congressional elections which took place three days later, the Socialist party defeated the Conservative party. Did the terrorist attack increase the Socialist party’s vote share? How can we go about making such a causal claim? As it turns out, real-world processes created conditions which were conducive to a difference-in-differences design, as identified by Jose Montalvo (http://84.89.132.1/~montalvo/wp/restat2011.pdf). A portion of voters for the March 14th elections lived outside of Spain, and were able to cast their absentee ballots in advance of the election – and in many cases, prior to the March 11th bombings. Spanish residents, on the other hand, all voted on election day, after the bombings. Spanish non-resident voters (voters living abroad) are most certainly not identical to Spanish resident voters across all covariates that might affect which party they support. One could speculate that they’re richer (high-paid executives working for multinational corporations); younger (students attending college abroad); or generally more liberal in their worldview. But the difference-in-difference strategy does not rely on the two groups being identical – only on their being consistently different. The counterfactual claim of similarity-in-differences is as follows: in the absence of the intervening terrorist attack, the difference between resident and non-resident support for the Conservative party would have been the same in the 2004 election as it was for the previous election. While this assumption cannot be tested, Montalvo provides a pretty compelling case for why we should accept it. The graph below shows resident and non-resident vote share over time. In the years leading up to 2004, resident and non-resident vote shares differed in a remarkably consistent way. But in 2004, when all of the resident voters received the treatment (i.e. the information of the bombings), and only a portion of the non-resident voters did, the parallel movement of the trend lines breaks. 44.3 Executing difference-in-differences with aggregate-level data The figure above shows aggregate-level data – the total vote share for the two populations at different points in time. To execute a difference-in-differences estimation of the effect of the bombings on vote share, we follow the steps outlined on the previous page: The resident voters are our “treatment” group (Tr) The non-resident voters are our “control” group (Cr). The 2000 election will be the pre-treatment period, and the 2004 election will be the post-treatment period. Approximations of Tr-pre, Cr-pre, Tr-post, and Cr-post are as follows: Conservative Party Vote % Treatment(Residents) Control (Non-residents) Pre-treatment (2000) 60 49 Post-treatment (2004) 51 51 D1 = Tr-pre minus Cr-pre = 60-49 = 11 D2 = Tr-post minus Cr-post = 51-51 = 0 D2 minus D1 = 0-11 = -11 = DD, the difference-in-differences The effect of the treatment (information of the bombings) is estimated to be -11, or a decrease of 11% in the Conservative party’s vote share. Note: you can calculate DD as described above, DD = [ (Tr-post minus Cr-post) minus (Tr-pre minus Cr-pre) ], or alternatively as DD = [ (Tr-post minus Tr-pre) minus (Cr-post minus Cr-pre) ]. The two values are algebraically equivalent. 44.4 Executing difference-in-differences with individual-level data What if we had data from individual observations as opposed to the populations in aggregate (e.g., 500 respondents from telephone surveys on election day in 2000 and 2004)? Respndent ID# Voted Conservative (Y) Treatment group (X1) Post-treatment (X2) Treatment AND Post (X1 x X2) 1001 0 0 0 0 1002 0 1 1 1 1003 1 1 1 1 1004 0 1 0 0 1005 1 0 1 0 1006 1 0 0 0 1007 0 0 1 0 1008 1 0 1 0 The first column is just an identifier for each respondent. The outcome (Y) variable, is a dummy indicating whether or not each respondent voted for the Conservative party. The treatment group variable (X1) is a dummy indicating whether the respondent was in the treatment (resident) group. The post-treatment variable is a dummy variable that is equal to zero whenthe observation is from the 2000 (pre-) election and equal to one when the observation is from the 2004 (post-treatment) election. The last variable, Treatment AND Post, is an interaction term, calculated by multiplying the treatment group dummy times the post-treatment dummy. The diff-in-diff estimation can then be accomplished with regression: \\[Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3(X_1 \\times X_2) + \\epsilon,\\] where \\(\\beta_1\\) captures any pre-treatment differences between the treatment and control groups, \\(\\beta_2\\) captures any differences that existed between the control group and the treatment group before the study began, and \\(\\beta_3\\) captures the effect of treatment on the treated groups. Executing difference-in-differences, having individual-level data offers a couple distinct advantages over aggregate data. First, it gives us a better understanding of the pre-treatment differences between treatment and control groups. It also provides standard errors on all of the coefficients, allowing us to determine the statistical significance of any effect we find within a standard regression context. Finally, it enables us to include other control variables into the regression equation (such as age or income, for the above example), so we can better isolate the causal effect of the treatment. "],
["regression-discontinuity-designs-part-1.html", "45 Regression discontinuity designs, Part 1 45.1 Learning objectives 45.2 Learning objective 1: What is regression discontinuity? 45.3 Learning Objective 2: Thinking about regression discontinuity as an experiment 45.4 Learning objective 3: Regression discontinuity in the real world", " 45 Regression discontinuity designs, Part 1 45.1 Learning objectives Learn what regression discontinuity is and why we would use it. Regression discontinuity as an experiment. Regression discontinuity in non-experimental settings. Part II of this online content will provide more specific instructions on how to estimate a Regression Discontinuity design. 45.2 Learning objective 1: What is regression discontinuity? Regression discontinuity is a method where we look for evidence of a causal effect by comparing outcomes for observations which are just above or just below some threshold. You can think about it like an experiment where the treatment or intervention is administered to all subjects above (or below) a certain threshold. Like all causal analysis methods, the goal of regression discontinuity testing is to determine whether certain treatments work more effectively than other treatments. However, in regression discontinuity testing the treatment is NOT randomly assigned. This allows us to perform regression discontinuity analysis without a formal experiment in order to test the effects of some sort of historical intervention. 45.3 Learning Objective 2: Thinking about regression discontinuity as an experiment If we wanted to conduct an experiment using regression discontinuity, we would start by determining which people were going to receive treatment. Say, for example, we wanted to test the effect of tutoring help on students, and our hypothesis was that tutoring will increase a student’s grades. Using regression discontinuity allows us to prioritize students with low grades so that the students who need the treatment more will receive it. Say we got all the participants’ grades, and decided to administer treatment (tutoring, in this case) to any student whose score is below 71% (the class mean). Thus, we have not administered our treatment at random. Instead, we have administered the treatment, which we hypothesize will be beneficial, to those who need it most. The plot here shows the distribution of students’ scores before the intervention (based on a pre-test) where the red line indicates the threshold for receiving the treatment. Now imagine that after two months of tutoring for the lower-half of the class, we gather data on grades to get the distribution of the students on a post-test. Now we are going to run a regression where the dependent variable is student’s pre-test score and the main explanatory variable is student’s pre-test grade. If the treatment works, and tutoring does increase grades, we should see a discontinuity at the threshold (e.g., at the pre-test score of 71). If the treatment didn’t work, there should be no unusual break in the relationship between these two variables at the threshold. Let’s visualize this a bit. Here is a graph showing the relationship between the pre-test score and post-test scores for students if we imagine there was no treatment effect. Note that there is no unusual break in the linear relationship between thse two variables at the 71-point threshold. Now, here is a plot where we imagine that the treatment did work. Note that there is a break – or discontinuity – right at the threshold/cutoff that determined whether students were in the treatment or control condition. It is important to note that in order to conduct regression discontinuity, we must assume that without treatment, the pre-test averages would follow a predictable pattern in relation to the post-test averages. In this case, we assume that if we did not treat at all, pre-test averages would be the same as post-test averages (which we can see by the fact that the best-fit line has a slope of 1). By assuming this, we can use the resulting graph to conduct regression discontinuity. If the graph we get once we take all the data looks like the first one above, we can conclude that the treatment had no significant effect, because the pre-test and post-test averages are the same as if there had been no treatment. However, if there is a discontinuity, we can conclude that the treatment worked. 45.4 Learning objective 3: Regression discontinuity in the real world Regression discontinuity also allows us to see the effects of past events or interventions without performing an experiment. Say, for example, we want to test being eligible to vote in one year has an effect on your tendency to vote in the future. (Some political scientists hypothesize that people develop voting habits, so that voting in the past makes you more likely to vote in the future.) Prof. Marc Meredith tested this using data from California. He looked at the turnout rates in the 2006 election for people around the voting eligibiltiy threshold for the 2004 election. Here are the results he found: As you can see, there was a modest (but real) causal effect of being eligible to vote in previous elections on people’s future tendency to vote that can be seen right at the threshold. "],
["regression-discontinuity-designs-part-ii.html", "46 Regression discontinuity designs, Part II 46.1 Learning Objective 46.2 Learning Objective 1: Basic regression discontinuity designs 46.3 Learning Objective 2: Handling non-linearities", " 46 Regression discontinuity designs, Part II 46.1 Learning Objective Learn how to set up a regression for a basic discontinuity design. Learn how to set up a regression when you anticipate non-linearities 46.2 Learning Objective 1: Basic regression discontinuity designs Now that now we know what regression discontinuity is and how to model linear regressions, we can combine the two in order to quantify the effects of regression discontinuity tests. The basic equation that we follow in order to do this is: \\[y= \\beta_0 + \\beta_1 X + \\beta_2*I(X&gt;D) + \\epsilon,\\] where X is the main variable of interest (the variable that creates the discontinuity), D is the discontinuity threshold, \\(\\epsilon\\) is the residuals or errors. In this equation, \\(I(X&gt;D)\\) is a function that equals 1 if \\(X&gt;D\\), and it equals 0 if \\(X\\). This means that the prediction equation for our regression if the \\(X\\) variable is below the discontinuity is: \\[y = \\beta_0 + \\beta_1 X\\]. However, if the \\(X\\) variable is above the threshold, the prediction equation is: \\[y=(\\beta_0+\\beta_2) + \\beta_1 X\\] Thus, the \\(\\beta_2\\) term test whether there is a break (or discontinuity) in the relationship between the explanatory and dependent variable right at our threshold. 46.3 Learning Objective 2: Handling non-linearities It is important to note that this equation assumes that the equation is perfectly linear, when in many cases, real statistical relationships are not. We can account for this in several ways. If we suspect that the slope of the equation is different on either side of the threshold, we can account for that by adding a slope argument that changes the slope if X is above the threshold: \\[y= \\beta_0 + \\beta_1 X + \\beta_2*I(X&gt;D) + \\beta_3 \\times X \\times I(X&gt;D) + \\epsilon,\\] which allows for the linear relationship between \\(X\\) and \\(Y\\) to be different on either side of the discontinuity. Additionally, if we suspect that the relationship isn’t linear at all and is instead curvy, we can add additional arguments for \\(X^2\\) and \\(X^3\\): \\[y= \\beta_0 + \\beta_1 X + \\beta_2*I(X&gt;D) + \\beta_3 X^2 + \\beta_4X^3 + \\epsilon,\\] In all of these cases, we can essentially model two separate regression equations: one below the threshold, and one above the threshold. Then, if there is any difference between the two regressions, we can quantify the effect that the treatment has by looking at the statistical significance of \\(\\beta_2\\). For more information about regression discontinuity, check out this video for another good explanation of some of the basics (don’t worry about the part about fuzzy regression discontinuity): "]
]
