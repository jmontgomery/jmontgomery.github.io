# dplyr and tidy

## Into the tidyverse

- `dplyr` is a handy package for changing data
- `tidyr` is a handy package for reshaping data
- In combination they offer a powerful way to quickly extract insight from your data

### Our example

Data comes from fivethirtyeight.com
```{r, eval=TRUE, message=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)
library(readr)
primaryPolls<-read_csv('https://jmontgomery.github.io/PDS/Datasets/president_primary_polls_feb2020.csv')
primaryPolls$start_date<-as.Date(primaryPolls$start_date, "%m/%d/%y")

class(primaryPolls)
```

- A tibble is basically a data frame 
- They are both friendlier and more structured than traditional data frames


```{r, eval=TRUE, message=FALSE, warning=FALSE}
primaryPolls
```

You can also print the first 10 rows of all the columns

```{r, eval=TRUE, message=FALSE, warning=FALSE}
print(primaryPolls, width=Inf)
```

- You can access elements basically the same way
- They never allow partial matching (which is evil anyways)
    - Partial matching allows you to type the first few characters of a column name and it will figure out which one you meant
- You can use `as.tibble` or `as.data.frame` to jump back and forth


## Back to `dplyr`

- Subset data by rows: `filter`
- Reorder by rows: `arrange`
- Subset data by column: `select`
- Create new variables as a function of other variables: `mutate`
- Collapse values down (or extract statistic): `summarise`
- We can use `group_by` to make changes in the scope

### `filter`

- `filter` allows you to easily subset data using conditions as we have done before using accessors

```{r, eval=TRUE, message=FALSE, warning=FALSE}
filter(primaryPolls, candidate_name == c("Amy Klobuchar"))

```
- Or you can include multiple conditions

```{r, eval=TRUE, message=FALSE, warning=FALSE}
filter(primaryPolls, 
       candidate_name == c("Amy Klobuchar"), 
       state=="New Hampshire")

```

- Everything you have already learned about boolean operators applies here.

### `arrange`

- `arrange`  does the same thing, but organizes data by rows insead of subsetting by rows.

```{r, eval=TRUE, message=FALSE, warning=FALSE}
arrange(primaryPolls, state, pollster_id)
```

- Or in descending order

```{r, eval=TRUE, message=FALSE, warning=FALSE}
arrange(primaryPolls, state, desc(pollster_id))
```

### `select`

- Select is just a much easier way to subset by column


```{r, eval=TRUE, message=FALSE, warning=FALSE}
select(primaryPolls, state, candidate_name, start_date)
```

- It comes with a nice syntax for doing this without just listing
- `var1:var20` will select all columns between these two
- `-var1:var20` will select everything *except* that range
- `starts_with("cand")` will select columns that start with "cand"
- Similar functionality for `ends_with`, `contains`, and matches


A fun trick to move your favorite variables to the front, but keep it all:

```{r, eval=TRUE, message=FALSE, warning=FALSE}
select(primaryPolls, state, candidate_name, start_date, everything())
```


- Note here that `rename` is the mythical easy way to rename a column, although syntax seems backwards
- You have to specify the new column name on the left side of the equal sign and the old column name on the right

```{r, eval=TRUE, message=FALSE, warning=FALSE}
basicPolls<-select(primaryPolls, state, candidate_name, start_date, pct)
rename(basicPolls, candidate = candidate_name)
```

### `mutate`

- `mutate` allows us to create a new variable that is a function of the others
- If you want to add the new column(s) to your existing tibble, you must use the assignment operator.
    - Otherwise it will just print it out and throw the results away

```{r, eval=TRUE, message=FALSE, warning=FALSE}
mutate(basicPolls, proportion=pct/100)

basicPolls <- mutate(basicPolls, proportion=pct/100)
```

- transmute creates and returns a new tibble with only the mutated variable(s)

```{r, eval=TRUE, message=FALSE, warning=FALSE}
transmute(basicPolls, proportion=pct/100)

transmute(primaryPolls, numberRespondents=round((pct/100)*sample_size))

transmute(primaryPolls, 
          proportion=pct/100,
          numberRespondents=round(proportion*sample_size),
          )
```

- Note that you can use a ton of the basic functions we have already covered like `sum`, `mean`, `sqrt`, etc.
- A useful one is `n()`, which just counts the number of observations
- This includes logical comparisons

```{r, eval=TRUE, message=FALSE, warning=FALSE}
mutate(basicPolls, top_tier=(pct>10)*1)
```

### `summarise`

- We can easily extract summary statistics


```{r, eval=TRUE, message=FALSE, warning=FALSE}
summarise(basicPolls, average_candidate=mean(pct), count=n())
```

- This is more powerful when also using the `group_by` function

```{r, eval=TRUE, message=FALSE, warning=FALSE}
basicPolls_grouped<-group_by(basicPolls, candidate_name)
summarise(basicPolls_grouped, average_candidate=mean(pct), count=n())

basicPolls_grouped<-group_by(basicPolls, candidate_name, state)
summarise(basicPolls_grouped, average_candidate=mean(pct), count=n())
```

### Piping

- The tidyverse includes a nice syntax for combining multiple commands so we don't have to create new objects all of the time.
- The `%?%` syntax allows us to pass on the results of one line to another
- The result from the left side of the `%>%` is passed as the first parameter of the next function

```{r, eval=TRUE, message=FALSE, warning=FALSE}
basicPolls %>%
  group_by(candidate_name, state) %>%
  summarise(average_candidate=mean(pct), count=n())
```

- The above code is equivalent to the following:

```{r, eval=TRUE, message=FALSE, warning=FALSE}
summarise(group_by(basicPolls, candidate_name, state), average_candidate=mean(pct), count=n())
```

- Which is equivalent to this:

```{r, eval=TRUE, message=FALSE, warning=FALSE}
grouped_by <- group_by(basicPolls, candidate_name, state)
summarise(grouped_by, average_candidate=mean(pct), count=n())
```

- See assigned readings for more useful summary variables and `ungroup`

```{r, eval=TRUE, message=FALSE, warning=FALSE}
basicPolls %>%
  group_by(candidate_name, state) %>%
  summarise(average_candidate=mean(pct), count=n()) %>%
  filter(count>10)
```


```{r, eval=TRUE, message=FALSE, warning=FALSE}
primaryPolls %>%
  group_by(candidate_name, state) %>%
  summarise(average_candidate=mean(pct), count=n()) %>%
  filter(count>10) %>%
  mutate(average_prop=average_candidate/100) %>%
  select(average_prop, candidate_name, state, count)
```

### Pivots

- In many cases the data is not quite organized the way we want.
- Right now we have each poll as a separate row.  But what if we want each candidate to be a row so we can analyze their trends in polls over time?
- Or what if we get the trend line and want to instead reorganize to look at each poll separately?
- The key commands here are `pivot_wider` and `pivot_longer`
- See Chapter 12 for some additional (but less universally useful functions)



- In our running example we have one entry for each poll.  How could we combine these to list the result from each unique poll togehter?
- The key here is that the *values* of interest are in the `pct` column and the *groupings* of interest are in the candidates column

```{r, eval=TRUE, message=FALSE, warning=FALSE}
nevadaPrimaries<-primaryPolls %>%
 filter(candidate_name %in% c("Amy Klobuchar", "Bernard Sanders", "Elizabeth Warren", "Joseph R. Biden Jr.", "Michael Bloomberg", "Pete Buttigieg")) %>%
  filter(state=="Nevada") %>%
  select(candidate_name, pct, start_date, sample_size)
print(nevadaPrimaries, n=Inf)
```
- setting `n=Inf` in the print will ensure that all rows are shown (the equivalent for columns is `width=Inf`)
- `nevadaPrimaries` contains one row for every combination of a Nevada poll (specified by `start_date` and `sample_size`) and a candidate


```{r, eval=TRUE, message=FALSE, warning=FALSE}
wideNevada<-pivot_wider(nevadaPrimaries, names_from = candidate_name, values_from = pct)
print(wideNevada, width=Inf)
```

- `wideNevada` contains one row for every Nevada poll and a column for the pct each candidate got

```{r, eval=TRUE, message=FALSE, warning=FALSE}
dim(nevadaPrimaries)
dim(wideNevada)
```


- Or we could organize it into a time series ....

```{r, eval=TRUE, message=FALSE, warning=FALSE}
timeNevada<-pivot_wider(nevadaPrimaries, id_cols=candidate_name, 
                        names_from = c(start_date), values_from = pct)
print(timeNevada, width=Inf)
```

- Of course sometimes we want to do the reverse using `pivot_longer`
- In this case, we no longer have a single indicator variable for the values of `start_date`


```{r, eval=TRUE, message=FALSE, warning=FALSE}
timeNevada %>%
  select(candidate_name, `2020-01-08`, `2020-01-06`) %>%
  pivot_longer(c(`2020-01-08`, `2020-01-06`), names_to = "start_date_test", values_to = "pct_test")
```

- This will partially undo the `pivot_wider` command above
- Now, each row is a combination of one of the six candidates and one of the two start dates
- Note that to get this, we used `names_to` and `values_to` instead of `names_from` and `values_from`

## Relational data

- Most complex analyses involve more than one table of data.
- Certainly most active databases are not just rectangles.
- Modern databases are relational, where we know how rows in each rectangle are related to each other.
- With some slight mind bending, we can learn how to work cleanly with such data.

A classic example of relational data is a social media website like Twitter. They would have multiple tables/datasets - one for each user and their 
user information, one for each tweet and its author, one for each like, the user who did the like, and the tweet that was liked, etc. 

From this data, we can use relational queries to count the average number of likes Donald Trump gets on his tweets that are tweeted between 
1 and 2 AM.

The primary relational querying language is SQL (Structured Query Language) but most of the same techniques can be applied in R using tidy.

- Download and unzip the following: https://github.com/jmontgomery/jmontgomery.github.io/blob/master/PDS/Datasets/Tweets.csv.zip
- Read these in using the correct file address


```{r, eval=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
mayors<-read_csv(file="https://raw.githubusercontent.com/jmontgomery/jmontgomery.github.io/master/PDS/Datasets/Mayors.csv")
tweets<-read_csv("C:/Users/maria/OneDrive/WUSTL/2019-20/PDS/Tweets.csv")
print(object.size(mayors), units="auto")
print(object.size(tweets), units="auto")
```

There are various ways we might want to work across levels

- *Mutating joins*, where you make a new variable constructed form matched observations in the other.  E.g., how many tweets for each mayor
- *Filtering joins*, where you filter cases based on whether they match across.  E.g., filter to mayors who have tweets.
- *Combining* the datasets in various ways to construct new databases with rows and columns that meet desired specifications.  E.g., Tweets of all moyors from Indiana.


```{r, eval=TRUE, message=FALSE, warning=FALSE}
mayors
tweets
```

This is a simple relational databset that connects tweets to mayors.

- The variable 'TwitterHandle' in the mayors dataset should match up to 'ScreenName' in the tweets data.
- But it will help you get the basics here.
- These variables will serve to link across datasets.
- Let's rename so they are the same

```{r, eval=TRUE, message=FALSE, warning=FALSE}
tweets <- rename(tweets, TwitterHandle=ScreenName)
tweets
```

- But each dataset will also need a `key` -- a unique identier
- Let's actually look to see if these are unique identifiers?

```{r, eval=TRUE, message=FALSE, warning=FALSE}
mayors %>% 
  count(TwitterHandle) %>%
  filter(n>1)
tweets %>% 
  count(TweetID) %>%
  filter(n>1)
```

Hmmm ... you might want to figure out these duplicates before running any analyses.

### Mutating join


- A mutating join adds variables to the right on your datset

```{r, eval=TRUE, message=FALSE, warning=FALSE}
tweets %>% 
  left_join(select(mayors, TwitterHandle, LastName), 
            by="TwitterHandle")
```

### Joins topography

- "Inner joins" produce results only where the linking variable appears on both tables
- "Outer joins" produce results with all data, and fills in missing values as necessary
  + Left join keeps only observations from the first specified data if their is no match.
  + Right join keeps only observations from teh second data
  + Full join keeps everything
- Let's draw up the venn diagrams for:
  + Inner joins
  + Left joins
  + Right joins
  + Full joins

### Many and one-to-many

- The join we did above was a one-to-many join, where the last name was added to all of the tweets for that mayor.
- But I did this on purpose and it can go awry when the keys are not as unique as you think they are.
- Joins when there are duplicates lead to creations of new rows for all possible combinations.
    
### Filtering joins

- `semi_join(x, y)` keeps all observations in x that have a match in y.
- `anti_join(x, y)` drops all observations in x that have a match in y.


- The latter is very useful for figuring out why your joins are not working as you expect.  
- For important data merges, you should always run these to be sure that these are returning the values and numbers you expect.


### Set operations

Tidy also includes functions where it expects datasets with the same columns.

- `intersect(x, y)` will return only observations in both x and y.
- `union(x, y)` will return unique observations in both x and y.
- `setdiff(x, y)` will return observations in x, but not in y.

This is useful for deduplication tasks and also for identifying problems from specific merge commands.

### Exercise

Add a third set of data --- mentions

```{r, eval=TRUE, message=FALSE, warning=FALSE}
mentions <-read_csv(file="https://raw.githubusercontent.com/jmontgomery/jmontgomery.github.io/master/PDS/Datasets/TwitterMentions.csv")
```

1. For each mayor, calculate the number of times they were mentioned

```{r}
mentions
mayors
mentions <- rename(mentions, TwitterHandle = "ScreenName")
mentionsCount <- mentions %>% group_by(MayorHandle) %>% tally()
answer1 <- mayors %>% left_join(mentionsCount, by=c("TwitterHandle" = "MayorHandle")) %>% select(TwitterHandle, FullName, n) %>% arrange(desc(n))
answer1
```

2. Add to the mayors datset the number of times each mayor tweeted.

```{r}
tweetCounts <- tweets %>% group_by(TwitterHandle) %>% tally()
answer2 <- mayors %>% left_join(tweetCounts, by="TwitterHandle") %>% arrange(desc(n))
answer2
```


3. Create a combined dataset of all tweets from the tweets and mentions data.  Subset down to overlapping columns (and rename where needed) to make this easy.

```{r}
allData <- full_join(tweets, mentions)
allData
```

## `stringr`

`stringr` is a tidy package that is used for processing strings and text data

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
tweets<-read_csv("C:/Users/maria/OneDrive/WUSTL/2019-20/PDS/Tweets.csv")

aTweet<-tweets[1,]$Text
aTweet

str_length(aTweet)
words<-str_split(aTweet, pattern = " ")
words

str_c(words)
str_c(unlist(words))

str_c(unlist(words), "Added")

str_c(unlist(words), "Added", sep=" ")
str_c("Before", unlist(words), "Added", sep=" ")
```

Or you may want to assemble it all into all back into a coherent whole

```{r, eval=TRUE, message=FALSE, warning=FALSE}
str_c(unlist(words), collapse=" ")
```

### Things you haven't thought about strings

- Strings in memory are different than how they appear on the screen
- You can use the function `writeLines` to see how a function would atually render to a reader (as opposed to how it is stored in the computer.)
- The big confusion is that special characters have to be "escaped" or else they get in the way (From R4DS)

```{r, eval=TRUE, message=FALSE, warning=FALSE}
x <- c("\"", "\\")
x
writeLines(x)
```

- Others to watch out for are `"\n"` and `"\t"` for `next line` and `tab`
- `na` is a problem, so you might use `str_replace_na`

- `stringr` replicates a lot of the functions we have already used for characters with (sometimes) slightly different syntax

```{r, eval=TRUE, message=FALSE, warning=FALSE}
colors <- c("red", "yellow", "blue", "green", "magenta", "cyan")
str_sub(colors, 1, 3)
str_sub(colors, -3, -1) #count back from the end
str_sub(colors, 1, 10) # Robust to going out of index

str_sub(colors, 1, 3)<-str_to_upper(str_sub(colors, 1, 3))
colors

colors<-str_to_lower(colors)
colors
```

Example:

1. Filter down to all of the tweets from Mayor Lyda Krewson in our data
2. Find the mean number of words in her tweets. Find the total.
3. Take all of the words she has used and reduce them down to only their first five letters.  Repeat (2) and compare.


Question 2

```{r}
hertweets <- filter(tweets, ScreenName == "lydakrewson")["Text"]
numWords <- hertweets %>% 
    rowwise() %>% 
    transmute(numWords= length(unlist(str_split(Text, " "))))
numWords
mean(numWords$numWords)

length(unique(unlist(str_split(hertweets$Text, " "))))
```

Question 3

```{r}
length(unique(str_sub(unlist(str_split(hertweets$Text, " ")), 1, 5)))
```

## Pipes and maps

- Download: https://www.voterstudygroup.org/publication/2019-voter-survey-full-data-set
- Codebook: https://www.voterstudygroup.org/publication/2019-voter-survey-full-data-set


```{r, eval=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
VSG<-read_csv("C:/Users/maria/OneDrive/WUSTL/2019-20/PDS/VOTER_Survey_Jan217_Release1-csv.csv", col_types=cols(weight_18_24_2018="d"))
```

Question: Is there a lane?

We are going to:

- Subset the data to just favorability raitings for Elizabeth Warren and Bernie Sanders
- Recode these variables into "approve" and "not approve"
- Find the proportion of respondents who either approve or disapprove of *both*


One strategy is just to make new datsets for each step

```{r, eval=FALSE, message=FALSE, warning=FALSE}
VSG.two <- select(VSG, fav_sanders_2019, fav_warren_2019)
VSG.three<-mutate(VSG.two, fav_sanders_binary = (fav_sanders_2019<=2))
VSG.four<-mutate(VSG.three, fav_sanders_binary= na_if(fav_sanders_2019, fav_sanders_2019>4))
```
This is bad because we're wasting a lot of memory in our environment that we'll never use again.


Another strategy is just to overwrite the same dataset

```{r, eval=FALSE, message=FALSE, warning=FALSE}
VSG <- select(VSG, fav_sanders_2019, fav_warren_2019)
VSG < -mutate(VSG, fav_sanders_binary = (fav_sanders_2019<=2)*1)
VSG <- mutate(VSG, fav_sanders_binary= na_if(fav_sanders_2019, fav_sanders_2019>4))
```
This still wastes a lot of memory because of the way memory is managed in R. It also would be terrible to debug if something goes wrong.

An even more difficult way would be write a function where the output of each stage are passed onto an outer layer.

Or we can use the pipes

```{r, eval=FALSE, message=FALSE, warning=FALSE}
VSG %>% 
  select(fav_sanders_2019, fav_warren_2019) %>%
  mutate(fav_sanders_binary= na_if(fav_sanders_2019, fav_sanders_2019>4)) %>%
  mutate(fav_sanders_binary = (fav_sanders_2019<=2)*1) 
```

### Limitations of Pipes

- Piping works by creating a new function where object manipulations are passed from one to another
- This means that piping won't work in cases where:
    + The function uses the current environment such as assign, load, or get
    + Functions that use lazy evaluation such as tryCatch, try, or supressMessages
- Pipes are not good in all situations
- If you do more than 4 or 5 pipes, go ahead and make in intermediate object.  Otherwise debugging is a pain.
- If you are not actually working on just one object but instead combinig multiple objects, this is just not worth it.
- Where ther are interdepencies (e.g., complex recoding based on multiple variables)



- The `magrittr` package comes with some other tools for using in specific situations
- The `%T>%` pipe works by calling the function to the right, but returning the function to the left

```{r, eval=TRUE, message=FALSE, warning=FALSE}
library(magrittr)
VSG<-read_csv("C:/Users/maria/OneDrive/WUSTL/2019-20/PDS/VOTER_Survey_Jan217_Release1-csv.csv", col_types=cols(weight_18_24_2018="d"))
VSG %>% 
  select(fav_sanders_2019, fav_warren_2019) %>%
  mutate(fav_sanders_binary= na_if(fav_sanders_2019, fav_sanders_2019>4)) %>%
  mutate(fav_sanders_binary = (fav_sanders_2019<=2)*1) %T>%
  plot()
```


- We may also want to use specific elements from a dataframe using the `%$%` operator

```{r, eval=TRUE, message=FALSE, warning=FALSE}
VSG<- VSG %>% 
  select(fav_sanders_2019, fav_warren_2019) %>%
  mutate(fav_sanders_binary= na_if(fav_sanders_2019, fav_sanders_2019>4)) %>%
  mutate(fav_sanders_binary = (fav_sanders_2019<=2)*1) %>%
  mutate(fav_warren_binary= na_if(fav_warren_2019, fav_warren_2019>4)) %>%
  mutate(fav_warren_binary = (fav_warren_2019<=2)*1)

library(magrittr)
VSG<- VSG %$% 
  plot(table(fav_sanders_binary, fav_warren_binary))
```

## `map`

```{r, eval=TRUE, message=FALSE, warning=FALSE}
VSG<-read_csv("C:/Users/maria/OneDrive/WUSTL/2019-20/PDS/VOTER_Survey_Jan217_Release1-csv.csv", col_types=cols(weight_18_24_2018="d"))
VSG.fav<-VSG %>% 
  select(starts_with("fav")) 
```

We want to loop over these data and recode all of the columns according to our rules

- Anything above a 4 gets moved to NA
- Then we turn them into a binary based on whether they are 2 and less or not

```{r, eval=TRUE, message=FALSE, warning=FALSE}
output<-as.data.frame(matrix(NA, nrow(VSG.fav), ncol(VSG.fav)))
for(i in seq_along(VSG.fav)){
  output[,i]<-(VSG.fav[,i]<=2)*1
}
```

- We could do this without making a new object

```{r, eval=TRUE, message=FALSE, warning=FALSE}
for(i in seq_along(VSG.fav)){
  VSG.fav[,i]= na_if(VSG.fav[[i]],VSG.fav[[i]]>4)
  VSG.fav[,i]<-(VSG.fav[,i]<=2)*1
}
head(VSG.fav)
```

- We can also loop where we aren't sure how long the final output is
- This can be done by adding each element to the end of a vector as we did in some previous lectures
- This is computationally expensive
- A better solution is to add all of them to a list and then combine them back at the end.
- See Chapter 23.3.3 in the book for more
- Or Chapter 23.3.4 for while loops (which we have already covered)

- Looping is such a pervasive task, that there are functions to make it easier
- The `map` family will do this, and you can carefully control the output
  + `map` makes a list
  + `map_lgl` makes a logical vector
  + `map_int` makes an integer
  + `map_dbl` makes a double vector
  + `map_chr` makes a character

- These functions takes in a vector as an input and applies that function
- Let's say that we want to find the mean level of favorability in our dataset
- This is *very* similar to `lapply` in base R
  + For a comparison, check [this](https://jennybc.github.io/purrr-tutorial/bk01_base-functions.html#lapply()_vs_purrr::map()) link out


```{r, eval=TRUE, message=FALSE, warning=FALSE}
map(VSG.fav, mean, na.rm=TRUE)

map_dbl(VSG.fav, mean, na.rm=TRUE)

map_chr(VSG.fav, mean, na.rm=TRUE)
```

## walk, purrr, and more

- See Chapter 21.8 for a discussion of `walk`
- Main idea here is when you want to use a function for its side operations (e.g., plot) rather than to manipulate the data
- See also Chapter 21.9.1 for some additional functionality that is occasionally useful:
    + `keep`
    + `discard`
    + `some`
    + `every`
- See Chapter 21.9.2 for even more advanced approaches to extracting data
