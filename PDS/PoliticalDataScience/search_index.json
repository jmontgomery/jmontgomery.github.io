[
["index.html", "Political Data Science 1 Don’t be a clown 1.1 As easy as cake 1.2 Class structures 1.3 Interactive R Environment (not currently working)", " Political Data Science Jacob Montgomery and Mariah Yelenick 1 Don’t be a clown I see miracles all around me Stop and look around, it’s all astounding Water, fire, air and dirt Fucking magnets, how do they work? And I don’t wanna talk to a scientist Y’all motherfuckers lying, and getting me pissed — Insane Clown Posse, Miracles Link if you don’t know what I’m talking about Pollsters are wizards, shamans, diviners. They toss numbers around the way astragalomancers once tossed bones to foretell events to come. (The name comes from the Greek astragalos, meaning “knucklebone.” But you knew that.) I have never been called by a political pollster and don’t know anybody who has, but I know some pollsters, who assure me they don’t make the numbers up, and I believe them. — Roger Simon, Chief Political Columnist, Politico Link Depending on who you listen to, artificial intelligence and machine learning1 are either the cause or the solution to all of the world’s problems. On the one hand you have algorithms spreading misinformtion, large scale corporate surveillance, and predicted mass unemployment in the service sector. On the other hand you have ever-improving health diagnostics, data-driven journalism, and all the world’s information at the touch of your fingers (or voice). The term “data science” means different things to different people, but for most people the term doesn’t mean much at all. In the two quotes above, we see both the Insane Clown Posse and Politico’s (for God’s sake) chief political columnist share a loving embrace with ignorance about fundamental forces in our life. We swim in data and statistics. News, politics, and policy is increasingly being driven by data and algorithms. But all too few people know much about how these algorithms work. Like magnets or random sampling, data science is just a mysterious force that we have to cope with. The central goal of this class is to make sure you don’t have to be a clown. In today’s world, your future career will surely You will find that data science, machine learning, big data, and artificial intelligence is just another tool (if a powerful tool). It is neither good nor evil by itself. It is humans and human judgement (or lack of judgement) that lead to good our bad outcomes. But whether you intend to work for the forces of good or evil (I tend towards chaotic neutral) 1.1 As easy as cake One way 1.1.1 Coding 1.1.2 Data acquision Is that polling data you are looking at a random sample of Americans, or just everyone who attended that sick Delta Sig party? Were the people in your data randomly assigned to watch Fox news, or did they choose to? No matter how fancy your machine learning algorithm, the results don’t mean anything very much if you don’t 1.1.3 Algorithms 1.1.4 Domain knowledge Like baking, the final thing you need to do good data science is what most people call “domain knowledge” and what I call “knowing what the hell you are talking about.” It turns out that having a really cool statistical model, huge amounts of data, and even a super computer sproting 24 GPUs will not save you from reaching the wrong answer if you don’t think (and aren’t trained to think). Link 1.1.5 Putting it together A final method. You can get along fine in may cases following the instructions on the back of the chocolate chip cookie bag. But to really become a “baker” you need to be able to create your own recipes for each occasion. And to do that you actually need to “know what the hell you are talking about.” So 1.2 Class structures 1.2.1 Learning objective #1: Team based learning This course was designed to incorporate elements of Team Based Learning. This is an approach to creating permanent groups to work on problems in class and out of class throughout the semester. We are not going to follow the TBL approach strictly. But we are going to have fewer lectures and more structured group activities. Learn more about Team Based Learning by watching this video. 1.2.2 Learning objective #2: Show up ready Moving an increasing amount of teaching resources online is a big trend in education at all levels. But this isn’t just trendy, it appears to be very effective. Preliminary research shows that online learning paired with in-class interactions with faculty is a much more effective than the traditional lecture format. Watch this short TED talk by Peter Norvig to learn more: So in advance of each lecture 1.2.3 Learning objective #3: How is this going to work? The approach I am trying to adopt for this class is to help you learn in four basic steps. Initial exposure to materials through self-study. Repeated exposure to materials in short lectures. Use your new knowledge in a collaborative environment where assistance is readily available. Use your new knowledge on your own. The goal is for you to see and use information in multiple ways to improve learning outcomes. Experience (and research) shows that this approach is far superior to simple lectures in helping you learn more and retain it longer. To help you along this process, each learning component will be broken down as follows. You will read your book and review online materials. I will begin most classes with a short lecture where I will cover the materials again and answer questions. You will apply your knowledge during in-class team assignments. If you have any questions or run into problems, I will be right there to give you help. To keep the team motivated, these assignments will be graded. You will apply your knowledge on your own (or with friends) in your problem sets and final project. 1.3 Interactive R Environment (not currently working) You might want to bookmark this page to test code quickly without opening RStudio. The username is the first name of the professor of this course and the password is the last name of the professor of this course "],
["section-getting-started-in-r.html", "2 Getting Started in R 2.1 Learning Objectives 2.2 Assigning values to objects 2.3 R as a calculator 2.4 The global environment and how to clean it 2.5 Getting help 2.6 Installing packages", " 2 Getting Started in R 2.1 Learning Objectives Learn how to assign a value to an object in R. Learn your way around R (using R Studio). Learn how to do simple arithmetic in R. 2.2 Assigning values to objects You can assign values to objects in two ways as shown below. The assignment arrow is functionally equivalent to the equal sign. When assigning values to objects, R will always take the value on the right side of the assignment operator (&lt;- or =) and store it in the object on the left side of the assignment operator. This means that the two lines of commented code in the following snippet do different things. Side note: to comment a single line of code in R, use the hashtag or pound sign at the beginning of the line. If you’re working in an R script, the commented portion will turn green. x &lt;- 4 x ## [1] 4 y = 6 y ## [1] 6 #y = x #x = y It is best practice when coding to avoid “magic numbers” - i.e. all numbers should be stored in named variables so that if we want to change the value, we only have to do so once. This also removes any ambiguity for someone else reading your code who might wonder what the number represents. total.votes.ak &lt;- 238307 voting.age.population.ak &lt;- 496387 turnout.ak &lt;-total.votes.ak/voting.age.population.ak turnout.ak ## [1] 0.4800831 2.3 R as a calculator 5+4 # Addition ## [1] 9 6-3 # Subtraction ## [1] 3 34 / 6 # Division ## [1] 5.666667 5 * 3 # Multiplication ## [1] 15 5^4 # Exponents ## [1] 625 625^(1/4) # More exponents ## [1] 5 11%%2 # modular arithmetic (11 mod 2) is the remainder operator ## [1] 1 31 %/% 7 # The integer part of a fraction ## [1] 4 R comes with a number of constants prestored that you can use 6.25 # numbers pi # And a few others NA # Missing value NULL # Nothing. 0/0 # NaN means &quot;Not a number&quot; 1/0 # Inf means infinity R follows the order of operations (Please Excuse My Dear Aunt Sally). Side note - if you have multiple exponentiation, they execute right to left. 2*(3-4)+2 ## [1] 0 2*(3-4)+2*(4 + 3)^(1/3) ## [1] 1.825862 2.4 The global environment and how to clean it Named objects are stored in the “global environment”, which means that they can be accessed at any time by any function you might run. They are “global” variables which makes them different from “local” variables (variables that can only be accessed from within a certain scope like within a function). The commands ls() and rm() are used to show or remove variables from the global environment respectively. a &lt;- 1 b &lt;- 2 ls() ## [1] &quot;a&quot; &quot;a.df&quot; ## [3] &quot;a1&quot; &quot;allData&quot; ## [5] &quot;answer1&quot; &quot;answer2&quot; ## [7] &quot;aTweet&quot; &quot;b&quot; ## [9] &quot;basicPolls&quot; &quot;basicPolls_grouped&quot; ## [11] &quot;binaryPred&quot; &quot;byOut&quot; ## [13] &quot;cat1&quot; &quot;cat2&quot; ## [15] &quot;collapse&quot; &quot;colors&quot; ## [17] &quot;ComplexModelFull&quot; &quot;ComplexModelPredictions&quot; ## [19] &quot;ComplexModelTrain&quot; &quot;congress&quot; ## [21] &quot;credentials&quot; &quot;dd&quot; ## [23] &quot;electData&quot; &quot;equation&quot; ## [25] &quot;Fatalities&quot; &quot;grouped_by&quot; ## [27] &quot;hertweets&quot; &quot;i&quot; ## [29] &quot;j&quot; &quot;l&quot; ## [31] &quot;l.fivenum&quot; &quot;m&quot; ## [33] &quot;mat1&quot; &quot;mat1.df&quot; ## [35] &quot;mat2&quot; &quot;mat2.df&quot; ## [37] &quot;mat3&quot; &quot;matdf&quot; ## [39] &quot;matdf1&quot; &quot;matdf2&quot; ## [41] &quot;matrix1&quot; &quot;mayors&quot; ## [43] &quot;mayors_new&quot; &quot;mayors_newDF&quot; ## [45] &quot;mayorsDF&quot; &quot;mentions&quot; ## [47] &quot;mentionsCount&quot; &quot;mod1_forest&quot; ## [49] &quot;mod1_knn&quot; &quot;Model1&quot; ## [51] &quot;Model1preds&quot; &quot;model2&quot; ## [53] &quot;Model2&quot; &quot;Model3&quot; ## [55] &quot;Model4&quot; &quot;myLogistic&quot; ## [57] &quot;n&quot; &quot;nevadaPrimaries&quot; ## [59] &quot;newhamp&quot; &quot;numWords&quot; ## [61] &quot;output&quot; &quot;primaryPolls&quot; ## [63] &quot;random_states&quot; &quot;resume&quot; ## [65] &quot;senate_test&quot; &quot;senate_train&quot; ## [67] &quot;senateData&quot; &quot;SimpleModelFull&quot; ## [69] &quot;SimpleModelPredictions&quot; &quot;SimpleModelTrain&quot; ## [71] &quot;split_senateData&quot; &quot;states10&quot; ## [73] &quot;states100&quot; &quot;sum.plus.2&quot; ## [75] &quot;testValues&quot; &quot;timeNevada&quot; ## [77] &quot;total.votes.ak&quot; &quot;tree_mod1&quot; ## [79] &quot;tree_mod2&quot; &quot;treePreds1&quot; ## [81] &quot;treePreds2&quot; &quot;turnout&quot; ## [83] &quot;turnout.ak&quot; &quot;turnoutX&quot; ## [85] &quot;tweetCounts&quot; &quot;tweets&quot; ## [87] &quot;tweetsDF&quot; &quot;voting.age.population.ak&quot; ## [89] &quot;VSG&quot; &quot;VSG.fav&quot; ## [91] &quot;wideNevada&quot; &quot;words&quot; ## [93] &quot;x&quot; &quot;x1&quot; ## [95] &quot;y&quot; &quot;ys&quot; rm(a) #this removes a from the global environment ls() ## [1] &quot;a.df&quot; &quot;a1&quot; ## [3] &quot;allData&quot; &quot;answer1&quot; ## [5] &quot;answer2&quot; &quot;aTweet&quot; ## [7] &quot;b&quot; &quot;basicPolls&quot; ## [9] &quot;basicPolls_grouped&quot; &quot;binaryPred&quot; ## [11] &quot;byOut&quot; &quot;cat1&quot; ## [13] &quot;cat2&quot; &quot;collapse&quot; ## [15] &quot;colors&quot; &quot;ComplexModelFull&quot; ## [17] &quot;ComplexModelPredictions&quot; &quot;ComplexModelTrain&quot; ## [19] &quot;congress&quot; &quot;credentials&quot; ## [21] &quot;dd&quot; &quot;electData&quot; ## [23] &quot;equation&quot; &quot;Fatalities&quot; ## [25] &quot;grouped_by&quot; &quot;hertweets&quot; ## [27] &quot;i&quot; &quot;j&quot; ## [29] &quot;l&quot; &quot;l.fivenum&quot; ## [31] &quot;m&quot; &quot;mat1&quot; ## [33] &quot;mat1.df&quot; &quot;mat2&quot; ## [35] &quot;mat2.df&quot; &quot;mat3&quot; ## [37] &quot;matdf&quot; &quot;matdf1&quot; ## [39] &quot;matdf2&quot; &quot;matrix1&quot; ## [41] &quot;mayors&quot; &quot;mayors_new&quot; ## [43] &quot;mayors_newDF&quot; &quot;mayorsDF&quot; ## [45] &quot;mentions&quot; &quot;mentionsCount&quot; ## [47] &quot;mod1_forest&quot; &quot;mod1_knn&quot; ## [49] &quot;Model1&quot; &quot;Model1preds&quot; ## [51] &quot;model2&quot; &quot;Model2&quot; ## [53] &quot;Model3&quot; &quot;Model4&quot; ## [55] &quot;myLogistic&quot; &quot;n&quot; ## [57] &quot;nevadaPrimaries&quot; &quot;newhamp&quot; ## [59] &quot;numWords&quot; &quot;output&quot; ## [61] &quot;primaryPolls&quot; &quot;random_states&quot; ## [63] &quot;resume&quot; &quot;senate_test&quot; ## [65] &quot;senate_train&quot; &quot;senateData&quot; ## [67] &quot;SimpleModelFull&quot; &quot;SimpleModelPredictions&quot; ## [69] &quot;SimpleModelTrain&quot; &quot;split_senateData&quot; ## [71] &quot;states10&quot; &quot;states100&quot; ## [73] &quot;sum.plus.2&quot; &quot;testValues&quot; ## [75] &quot;timeNevada&quot; &quot;total.votes.ak&quot; ## [77] &quot;tree_mod1&quot; &quot;tree_mod2&quot; ## [79] &quot;treePreds1&quot; &quot;treePreds2&quot; ## [81] &quot;turnout&quot; &quot;turnout.ak&quot; ## [83] &quot;turnoutX&quot; &quot;tweetCounts&quot; ## [85] &quot;tweets&quot; &quot;tweetsDF&quot; ## [87] &quot;voting.age.population.ak&quot; &quot;VSG&quot; ## [89] &quot;VSG.fav&quot; &quot;wideNevada&quot; ## [91] &quot;words&quot; &quot;x&quot; ## [93] &quot;x1&quot; &quot;y&quot; ## [95] &quot;ys&quot; c &lt;- 3 rm(list = ls()) #this removes all global variables Some things are present in the working environment, but not shown in the global environment .x&lt;-&quot;Hide me&quot; print(.x) ## [1] &quot;Hide me&quot; ls() ## character(0) Anything that starts with a “.” will be accessible in your code, but hidden 2.5 Getting help Learning about functions and how to specify them correctly is half the battle help(sqrt) # help w/ functions ?sqrt # same thing help.start() # lots of help help.search(&quot;sqrt&quot;) # what am I looking for? Fuzzy matching example(sqrt) RSiteSearch(&quot;missing&quot;) TIPS: 1) Remember that these help menus are usually written by the same people who wrote the functions you are using. They are uniformly not helpful unless you already know a good bit about computer programming and (in some cases) a lot about the function itself. 2) There is a basic structure that all help files must meet, and it is very important that you try and get the hang of this. 2.6 Installing packages The beauty of R is that there are packages, although things can be a bit unorganized. install.packages(&quot;BAS&quot;) # This will prompt a user interface to choose the &quot;mirror&quot; or repository library(BAS) # this will actually load the library for use. You must call this every time. search() # you can see what packages are attached to the workspace (and also what other objects) help(package=&quot;BAS&quot;) # Will (usually) give you a list of functions for the package example(BAS) # some package writers give you little examples to get you started All packages documentation are on CRAN Many packages come with example datasets built in data() data(rock) ?rock # there are help files for these ls() # there&#39;s rock -- in the global environment data(road, package=&quot;MASS&quot;) # you can load these datasets without loading the package "],
["section-data-types-in-r.html", "3 Data Types in R 3.1 Functions 3.2 Vectors 3.3 Functions and vectors 3.4 Logicals/Booleans 3.5 Characters/Strings 3.6 Matrices 3.7 Matrix algebra (optional) 3.8 Lists 3.9 Arrays 3.10 Dataframes 3.11 General info", " 3 Data Types in R 3.1 Functions Functions in R are denoted by the parentheses when you use them. For example, log is a function. The first line of the following code snippet just tells us the signature of the function - what inputs it takes and what the default values are if an input is optional. In this case, x is a required input (sometimes called parameter), and base is optional. If the user of the functiond doesn’t specify a base, it will use base \\(e\\). If you don’t specify the names of the parameters when using the function, they must be in the same order the function signature specifies. You can put them in any order, but to be safe, you should specify the names of each parameter. log ## function (x, base = exp(1)) .Primitive(&quot;log&quot;) log(2) # ln(2) ## [1] 0.6931472 log(2, base=10) # log(2) base 10 ## [1] 0.30103 log(base = 10, x = 2) ## [1] 0.30103 exp(log(1)) # e^ln(1) = 1 ## [1] 1 3.2 Vectors The c() function is used to collect/concatenate things together into a vector c(0,7,8) ## [1] 0 7 8 x &lt;- c(0,7,8) # assign this to a named object An easy way to make a sequence vector is using the : operator. This will start at the first value, increment by 1 each time, and end either before or at the second value. Here are some examples that demonstrate this behavior. numbers5to20 &lt;- 5:20 numbers5to20 ## [1] 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 1.5:10 ## [1] 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5 You can also concatenate any number of vectors to make a new vector. To do this, you need to use a comma between all inputs to the c() function. c(numbers5to20, x, numbers5to20) ## [1] 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 0 7 8 5 6 7 8 9 10 ## [26] 11 12 13 14 15 16 17 18 19 20 3.3 Functions and vectors Many functions have been “vectorized” meaning that they work on each element in the vector numbers5to20*2 ## [1] 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 sqrt(numbers5to20) ## [1] 2.236068 2.449490 2.645751 2.828427 3.000000 3.162278 3.316625 3.464102 ## [9] 3.605551 3.741657 3.872983 4.000000 4.123106 4.242641 4.358899 4.472136 log(numbers5to20) ## [1] 1.609438 1.791759 1.945910 2.079442 2.197225 2.302585 2.397895 2.484907 ## [9] 2.564949 2.639057 2.708050 2.772589 2.833213 2.890372 2.944439 2.995732 abs(numbers5to20) ## [1] 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 numbers5to20^2 ## [1] 25 36 49 64 81 100 121 144 169 196 225 256 289 324 361 400 data &lt;- c(NA, 4, 7, NA, 19) is.na(data) #tells us true or false whether each value is NA ## [1] TRUE FALSE FALSE TRUE FALSE !is.na(data) #true if the value is *not* NA ## [1] FALSE TRUE TRUE FALSE TRUE When you “interact” two vectors, they will work “elementwise”. numbers5to20*numbers5to20 ## [1] 25 36 49 64 81 100 121 144 169 196 225 256 289 324 361 400 numbers5to20+numbers5to20 ## [1] 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 numbers5to20+rev(numbers5to20) #rev() reverses a vector ## [1] 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 25 Other functions operate on an entire vector at once. sum(numbers5to20) ## [1] 200 prod(numbers5to20) #multiplies all elements together ## [1] 1.013709e+17 mean(numbers5to20) ## [1] 12.5 var(numbers5to20) ## [1] 22.66667 max(numbers5to20, na.rm=T) # maximum -- ignore missing data ## [1] 20 min(numbers5to20, na.rm=TRUE) #minimum -- notice that T=TRUE ## [1] 5 summary(numbers5to20) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 5.00 8.75 12.50 12.50 16.25 20.00 Accessing elements of a vector or matrix is usually done with the [] operators We can access the any element of numbers5to20. If you’ve coded in other languages that have zero-based indexing, R is a little different. The first element of a vector in R has index 1. This is shown below. numbers5to20[1] ## [1] 5 numbers5to20[40] ## [1] NA We can also extract several elements at a time using vectors. numbers5to20[c(3,6,7)] ## [1] 7 10 11 numbers5to20[3:7] ## [1] 7 8 9 10 11 We can repeat numbers as well numbers5to20[c(3,3,2,2)] ## [1] 7 7 6 6 This code prints everything but the 4th through 7th elements by using the negative sign numbers5to20[-(4:7)] ## [1] 5 6 7 12 13 14 15 16 17 18 19 20 x x*3 #scalar multiplication y &lt;- x-5 #simple addition and multiplicaiton are done &quot;by element&quot; y x^3 # ditto with exponents y^x # but if each are three elements long, it will execute by element Vector recycling When vectors have different lengths, the shorter one is extended by repeating the vector. This means two things: 1. The vector lengths must be multiples of each other 2. This is a very easy way to make a bad, bad mistake. You can easily find the length of a vector length(numbers5to20) ## [1] 16 Other functions will create vectors as outputs rep(1, 5) #Repeat the value 1, 5 times ## [1] 1 1 1 1 1 seq(1, 21, by=2) #Make the sequence 1 to 21 moving by increments of 2 ## [1] 1 3 5 7 9 11 13 15 17 19 21 rep(seq(2,20, by=2), 2) #Repeat the pattern 2, 4, ... 20, twice ## [1] 2 4 6 8 10 12 14 16 18 20 2 4 6 8 10 12 14 16 18 20 rep(c(1,4), c(3,2)) #Repeat 1 three times and 4 twice ## [1] 1 1 1 4 4 rep(c(1,4), each=3) #Repeat each value 3 times ## [1] 1 1 1 4 4 4 3.4 Logicals/Booleans Logicals are often used when subsetting or recoding data Generally necessary to understand this for data wrangling x&lt;-c(0, 7, 8) chooser&lt;-c(T, F, T) x[chooser] # print all elements of x where chooser is TRUE ## [1] 0 8 Arithmetic operations on logicals create numerics where: TRUE is treated as a ‘1’, and FALSE is treated as a ‘0’ sum(chooser) # Number of true values ## [1] 2 3.4.1 Boolean logic x == 7 #equals ## [1] FALSE TRUE FALSE x != 7 #does not equal ## [1] TRUE FALSE TRUE x &gt; 7 #greater than ## [1] FALSE FALSE TRUE x &gt;= 7 #greater than or equal to ## [1] FALSE TRUE TRUE x &lt; 7 #less than ## [1] TRUE FALSE FALSE x &lt;= 7 #less than or equal to ## [1] TRUE TRUE FALSE x &lt; 7 | x == 7 ## the or operator ## [1] TRUE TRUE FALSE x &lt;= 7 &amp; x == 7 ## the and operator ## [1] FALSE TRUE FALSE “or” - the pipe symbol - returns true if either/any of the conditions are true “and” - the ampersand - returns true if both/all of the conditions are true Most of the time, we use boolean logic to subset datasets. Here’s some data to get us started vap&lt;-voting.age.population&lt;-c(3481823, 496387, 4582842, 2120139,26955438, 3617942,2673154,652189,472143,14085749,6915512, 995937,1073799,9600372,4732010,2265860,2068253, 3213141,3188765,1033632,4242214,4997677,7620982, 3908159,2139918,4426278,731365,1321923,1870315,1012033, 6598368,1452962,14838076,6752018,494923,8697456,2697855, 2850525,9612380,824854,3303593,594599,4636679, 17038979,1797941,487900,5841335,4876661,1421717, 4257230,392344) total.votes&lt;-tv&lt;-c(NA, 238307, 1553032, 780409,8899059,1586105, 1162391,258053, 122356,4884544, 2143845,348988, 458927,3586292, 1719351,1071509, 864083,1370062, 954896,NA, 1809237, 2243835,3852008, 2217552,NA, 2178278, 411061,610499, 586274,418550, 2315643,568597, 4703830,2036451, 220479,4184072, NA,1399650, NA,392882, 1117311,341105, 1868363,NA, 582561, 263025,2398589, 2085074,473014, 2183155, 196217) tv[!is.na(tv)] # get all valid datapoints from the tv dataset ## [1] 238307 1553032 780409 8899059 1586105 1162391 258053 122356 4884544 ## [10] 2143845 348988 458927 3586292 1719351 1071509 864083 1370062 954896 ## [19] 1809237 2243835 3852008 2217552 2178278 411061 610499 586274 418550 ## [28] 2315643 568597 4703830 2036451 220479 4184072 1399650 392882 1117311 ## [37] 341105 1868363 582561 263025 2398589 2085074 473014 2183155 196217 small.states &lt;- voting.age.population[voting.age.population &lt; median(voting.age.population)] small.states ## [1] 496387 2120139 2673154 652189 472143 995937 1073799 2265860 2068253 ## [10] 1033632 2139918 731365 1321923 1870315 1012033 1452962 494923 2697855 ## [19] 2850525 824854 594599 1797941 487900 1421717 392344 state.size &lt;-(voting.age.population &gt; median(voting.age.population)) * 1 state.size # this code makes a new vector equal to 1 if it&#39;s a large state (larger than median), 0 otherwise ## [1] 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 0 ## [39] 1 0 1 0 1 1 0 0 1 1 0 1 0 which(state.size == 1) # returns the vector of indices of the elements that have state.size equal to 1 ## [1] 1 3 5 6 10 11 14 15 18 21 22 23 24 26 31 33 34 36 39 41 43 44 47 48 50 state.size == 1 # returns a logical vector where TRUE means it&#39;s a larger state ## [1] TRUE FALSE TRUE FALSE TRUE TRUE FALSE FALSE FALSE TRUE TRUE FALSE ## [13] FALSE TRUE TRUE FALSE FALSE TRUE FALSE FALSE TRUE TRUE TRUE TRUE ## [25] FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE TRUE TRUE FALSE TRUE ## [37] FALSE FALSE TRUE FALSE TRUE FALSE TRUE TRUE FALSE FALSE TRUE TRUE ## [49] FALSE TRUE FALSE any(x&gt;2) #returns true if the condition is true for any element ## [1] TRUE all(x&gt;2) #returns true if the condition is true for all elements ## [1] FALSE 3.5 Characters/Strings colors &lt;- c(&quot;red&quot;, &quot;yellow&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;magenta&quot;, &quot;cyan&quot;) You might want only a portion of the string. substr(colors, start=1, stop=2) ## [1] &quot;re&quot; &quot;ye&quot; &quot;bl&quot; &quot;gr&quot; &quot;ma&quot; &quot;cy&quot; paste0(colors, &quot;flowers&quot;) # combine two strings ## [1] &quot;redflowers&quot; &quot;yellowflowers&quot; &quot;blueflowers&quot; &quot;greenflowers&quot; ## [5] &quot;magentaflowers&quot; &quot;cyanflowers&quot; paste(colors, &quot;flowers&quot;, sep=&quot;&quot;) # combine two strings ## [1] &quot;redflowers&quot; &quot;yellowflowers&quot; &quot;blueflowers&quot; &quot;greenflowers&quot; ## [5] &quot;magentaflowers&quot; &quot;cyanflowers&quot; paste(&quot;I like&quot;, colors, &quot;flowers&quot;) ## [1] &quot;I like red flowers&quot; &quot;I like yellow flowers&quot; &quot;I like blue flowers&quot; ## [4] &quot;I like green flowers&quot; &quot;I like magenta flowers&quot; &quot;I like cyan flowers&quot; paste(&quot;I like&quot;, colors, &quot;flowers&quot;, collapse = &quot;&quot;) ## [1] &quot;I like red flowersI like yellow flowersI like blue flowersI like green flowersI like magenta flowersI like cyan flowers&quot; nchar(colors) #how many characters in each string ## [1] 3 6 4 5 7 4 You may want to divide a string into components extreme.statement &lt;- &quot;Coding is my life&quot; this.out &lt;- strsplit(extreme.statement, split=&quot; &quot;) unlist(this.out) ## [1] &quot;Coding&quot; &quot;is&quot; &quot;my&quot; &quot;life&quot; Use gsub to replace or remove parts of a string gsub(&quot;my life&quot;, &quot;the bee&#39;s knees&quot;, extreme.statement) ## [1] &quot;Coding is the bee&#39;s knees&quot; # less extreme. More true gsub(&quot; is my life&quot;, &quot;&quot;, extreme.statement) ## [1] &quot;Coding&quot; Tips for using strings: If you are doing a lot with strings try the library ‘strgr’, which has some user-friendly functions If you include ‘’ in a string, in many instances it will be a carriage return/new line (e.g., plotting) A very common programming error is to forget a closing quotation mark. This will make the computer think you are still making a giantly long string. 3.6 Matrices A matrix is just a collection of vectors You can combine vectors by column using ‘cbind’ m1 &lt;- cbind(vap, tv) head(m1) ## vap tv ## [1,] 3481823 NA ## [2,] 496387 238307 ## [3,] 4582842 1553032 ## [4,] 2120139 780409 ## [5,] 26955438 8899059 ## [6,] 3617942 1586105 Or you can combine by rows using ‘rbind’ m2 &lt;- rbind(vap, tv) head(m2) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## vap 3481823 496387 4582842 2120139 26955438 3617942 2673154 652189 472143 ## tv NA 238307 1553032 780409 8899059 1586105 1162391 258053 122356 ## [,10] [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] ## vap 14085749 6915512 995937 1073799 9600372 4732010 2265860 2068253 3213141 ## tv 4884544 2143845 348988 458927 3586292 1719351 1071509 864083 1370062 ## [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26] [,27] ## vap 3188765 1033632 4242214 4997677 7620982 3908159 2139918 4426278 731365 ## tv 954896 NA 1809237 2243835 3852008 2217552 NA 2178278 411061 ## [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] ## vap 1321923 1870315 1012033 6598368 1452962 14838076 6752018 494923 8697456 ## tv 610499 586274 418550 2315643 568597 4703830 2036451 220479 4184072 ## [,37] [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45] ## vap 2697855 2850525 9612380 824854 3303593 594599 4636679 17038979 1797941 ## tv NA 1399650 NA 392882 1117311 341105 1868363 NA 582561 ## [,46] [,47] [,48] [,49] [,50] [,51] ## vap 487900 5841335 4876661 1421717 4257230 392344 ## tv 263025 2398589 2085074 473014 2183155 196217 We can access by ‘matrixname[row, column]’ m2[1,2] # first row, second column ## vap ## 496387 m1[,1] # the 1st column ## [1] 3481823 496387 4582842 2120139 26955438 3617942 2673154 652189 ## [9] 472143 14085749 6915512 995937 1073799 9600372 4732010 2265860 ## [17] 2068253 3213141 3188765 1033632 4242214 4997677 7620982 3908159 ## [25] 2139918 4426278 731365 1321923 1870315 1012033 6598368 1452962 ## [33] 14838076 6752018 494923 8697456 2697855 2850525 9612380 824854 ## [41] 3303593 594599 4636679 17038979 1797941 487900 5841335 4876661 ## [49] 1421717 4257230 392344 Can get a submatrix m1[1:5,1:2] ## vap tv ## [1,] 3481823 NA ## [2,] 496387 238307 ## [3,] 4582842 1553032 ## [4,] 2120139 780409 ## [5,] 26955438 8899059 m2[1,1:10] ## [1] 3481823 496387 4582842 2120139 26955438 3617942 2673154 652189 ## [9] 472143 14085749 m2[1,1:10] ## [1] 3481823 496387 4582842 2120139 26955438 3617942 2673154 652189 ## [9] 472143 14085749 m2[1:2, 1:10] ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## vap 3481823 496387 4582842 2120139 26955438 3617942 2673154 652189 472143 ## tv NA 238307 1553032 780409 8899059 1586105 1162391 258053 122356 ## [,10] ## vap 14085749 ## tv 4884544 m2[, 1:10] # same as previous line since there are only two rows. ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## vap 3481823 496387 4582842 2120139 26955438 3617942 2673154 652189 472143 ## tv NA 238307 1553032 780409 8899059 1586105 1162391 258053 122356 ## [,10] ## vap 14085749 ## tv 4884544 class(m2) ## [1] &quot;matrix&quot; &quot;array&quot; Objects in R are simply pieces of data that contain specific attributes. Classes define what attributes an object must have and can have. Matrices always have an attribute that stores the number of rows and columns dim(m1) ## [1] 51 2 For any object you can look at it’s attributes using (helpfully) the ‘attributes’ function attributes(m1) ## $dim ## [1] 51 2 ## ## $dimnames ## $dimnames[[1]] ## NULL ## ## $dimnames[[2]] ## [1] &quot;vap&quot; &quot;tv&quot; We see that the matrix also has another attribute dimnames(m1) ## [[1]] ## NULL ## ## [[2]] ## [1] &quot;vap&quot; &quot;tv&quot; The top refers to the row names (which do not exist) The bottom are the column names (that were borrowed from the vectors used to construct the matrix). You can also access these using functions colnames(m1) ## [1] &quot;vap&quot; &quot;tv&quot; colnames(m2) ## NULL rownames(m1) ## NULL rownames(m2) ## [1] &quot;vap&quot; &quot;tv&quot; And you can assign these using the same functions colnames(m1) &lt;- c(&quot;Voting age population&quot;, &quot;Total Votes&quot;) colnames(m1) ## [1] &quot;Voting age population&quot; &quot;Total Votes&quot; You can also do this with the following dimnames(m1)[[2]][1] &lt;- &quot;Pigglywiggly&quot; head(m1) ## Pigglywiggly Total Votes ## [1,] 3481823 NA ## [2,] 496387 238307 ## [3,] 4582842 1553032 ## [4,] 2120139 780409 ## [5,] 26955438 8899059 ## [6,] 3617942 1586105 We have re-named the first column to have the name “Pigglywiggly” We are able to do this because the output of ‘dimnames’ is a list You can subset matrices with a boolean/logical matrix to get certain values from it just like you would a vector Matrices (and vectors) can only contain one datatype If you try to create one with multiple datatypes, it will convert everything to be the same datatype 3.7 Matrix algebra (optional) A couple of matrices H3 &lt;- matrix(c(1, 1/2, 1/3, 1/2, 1/3, 1/4, 1/3, 1/4, 1/5), nrow=3) H3 ## [,1] [,2] [,3] ## [1,] 1.0000000 0.5000000 0.3333333 ## [2,] 0.5000000 0.3333333 0.2500000 ## [3,] 0.3333333 0.2500000 0.2000000 1/cbind(seq(1,3), seq(2, 4), seq(3,5)) # most basic function continue to be &quot;element wise&quot; ## [,1] [,2] [,3] ## [1,] 1.0000000 0.5000000 0.3333333 ## [2,] 0.5000000 0.3333333 0.2500000 ## [3,] 0.3333333 0.2500000 0.2000000 H3+1 ## [,1] [,2] [,3] ## [1,] 2.000000 1.500000 1.333333 ## [2,] 1.500000 1.333333 1.250000 ## [3,] 1.333333 1.250000 1.200000 H3*2 ## [,1] [,2] [,3] ## [1,] 2.0000000 1.0000000 0.6666667 ## [2,] 1.0000000 0.6666667 0.5000000 ## [3,] 0.6666667 0.5000000 0.4000000 H3^2 ## [,1] [,2] [,3] ## [1,] 1.0000000 0.2500000 0.1111111 ## [2,] 0.2500000 0.1111111 0.0625000 ## [3,] 0.1111111 0.0625000 0.0400000 mean(H3) # others will treat the matrix as a vector no matter what ## [1] 0.4111111 rowSums(H3) # others work on matrices in particular ways (more on this later) ## [1] 1.8333333 1.0833333 0.7833333 colSums(H3) ## [1] 1.8333333 1.0833333 0.7833333 rowMeans(H3) ## [1] 0.6111111 0.3611111 0.2611111 colMeans(H3) ## [1] 0.6111111 0.3611111 0.2611111 Logicals work too H3 == 1 ## [,1] [,2] [,3] ## [1,] TRUE FALSE FALSE ## [2,] FALSE FALSE FALSE ## [3,] FALSE FALSE FALSE H3 == c(1,2,3) # wha...? ## [,1] [,2] [,3] ## [1,] TRUE FALSE FALSE ## [2,] FALSE FALSE FALSE ## [3,] FALSE FALSE FALSE H3 == H3 ## [,1] [,2] [,3] ## [1,] TRUE TRUE TRUE ## [2,] TRUE TRUE TRUE ## [3,] TRUE TRUE TRUE Some work like they do in the math books det(H3) # the determinant -- hard for you ... easy in R ## [1] 0.000462963 diag(H3) # get the diagonal elements of amatrix ## [1] 1.0000000 0.3333333 0.2000000 diag(1, nrow=3) # make a 3 by 3 identity matrix ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 t(H3) # matrix transpose ## [,1] [,2] [,3] ## [1,] 1.0000000 0.5000000 0.3333333 ## [2,] 0.5000000 0.3333333 0.2500000 ## [3,] 0.3333333 0.2500000 0.2000000 Hnew &lt;- H3 Hnew[lower.tri(H3, diag=TRUE)] # extract the lower triangular elements of H3 ## [1] 1.0000000 0.5000000 0.3333333 0.3333333 0.2500000 0.2000000 # Get the trace trace &lt;- function(data) (sum(diag(data))) # our own little function .. more on this next time trace(H3) ## [1] 1.533333 # Matrix multipication is %*% t(H3) %*% H3 ## [,1] [,2] [,3] ## [1,] 1.361111 0.7500000 0.5250000 ## [2,] 0.750000 0.4236111 0.3000000 ## [3,] 0.525000 0.3000000 0.2136111 c(1,2,3) %*% c(1,2,3) # dot product ## [,1] ## [1,] 14 matrix(c(1,2,3), ncol=1) %*% c(1,2,3) # outer product ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 2 4 6 ## [3,] 3 6 9 #matrix inversion solve(H3) ## [,1] [,2] [,3] ## [1,] 9 -36 30 ## [2,] -36 192 -180 ## [3,] 30 -180 180 invH3 &lt;- solve(H3) H3%*%invH3 ## close enough? ## [,1] [,2] [,3] ## [1,] 1.000000e+00 0.000000e+00 0 ## [2,] 8.881784e-16 1.000000e+00 0 ## [3,] 0.000000e+00 -7.105427e-15 1 # Why is it called solve? It can also be used to solve linear systems. b &lt;- c(1,2,3) b ## [1] 1 2 3 solve(H3, b) ## [1] 27 -192 210 3.8 Lists Let’s make a list that contains a matrix, a vector, and an integer. list.a &lt;- list(m1, vap, 3) str(list.a) ## List of 3 ## $ : num [1:51, 1:2] 3481823 496387 4582842 2120139 26955438 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:2] &quot;Pigglywiggly&quot; &quot;Total Votes&quot; ## $ : num [1:51] 3481823 496387 4582842 2120139 26955438 ... ## $ : num 3 This is the advantage of lists. They can contain basically anything, even other lists vector1 &lt;- c(1,2,3) gospels &lt;- c(&quot;matthew&quot;,&quot;mark&quot;,&quot;luke&quot;, &quot;john&quot;) my.matrix &lt;- matrix(c(1:20), nrow=4) my.data &lt;- data.frame(cbind(vap, tv)) my.crazy.list &lt;- list(vector1, gospels, my.matrix, TRUE, list.a) str(my.crazy.list) ## List of 5 ## $ : num [1:3] 1 2 3 ## $ : chr [1:4] &quot;matthew&quot; &quot;mark&quot; &quot;luke&quot; &quot;john&quot; ## $ : int [1:4, 1:5] 1 2 3 4 5 6 7 8 9 10 ... ## $ : logi TRUE ## $ :List of 3 ## ..$ : num [1:51, 1:2] 3481823 496387 4582842 2120139 26955438 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : NULL ## .. .. ..$ : chr [1:2] &quot;Pigglywiggly&quot; &quot;Total Votes&quot; ## ..$ : num [1:51] 3481823 496387 4582842 2120139 26955438 ... ## ..$ : num 3 Lists have attributes but we haven’t set them yet attributes(my.crazy.list) ## NULL This reports the number of major sub-elements in the list. length(my.crazy.list) ## [1] 5 This won’t work for complicated lists dim(my.crazy.list) ## NULL We can give names to elements of a list names(my.crazy.list)&lt;-c(&quot;OneTwoThree&quot;, &quot;Gospels&quot;, &quot;SmallMat&quot;, &quot;OneLogical&quot;, &quot;AnotherList&quot;) str(my.crazy.list) ## List of 5 ## $ OneTwoThree: num [1:3] 1 2 3 ## $ Gospels : chr [1:4] &quot;matthew&quot; &quot;mark&quot; &quot;luke&quot; &quot;john&quot; ## $ SmallMat : int [1:4, 1:5] 1 2 3 4 5 6 7 8 9 10 ... ## $ OneLogical : logi TRUE ## $ AnotherList:List of 3 ## ..$ : num [1:51, 1:2] 3481823 496387 4582842 2120139 26955438 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : NULL ## .. .. ..$ : chr [1:2] &quot;Pigglywiggly&quot; &quot;Total Votes&quot; ## ..$ : num [1:51] 3481823 496387 4582842 2120139 26955438 ... ## ..$ : num 3 Now each part of the list has a name attribute Skip the steps above by doing the following: my.crazy.list&lt;-list(OneTwoThree=vector1, Gospels=gospels, SmallMat=my.matrix, OneLogical=TRUE, AnotherList=list.a) str(my.crazy.list) ## List of 5 ## $ OneTwoThree: num [1:3] 1 2 3 ## $ Gospels : chr [1:4] &quot;matthew&quot; &quot;mark&quot; &quot;luke&quot; &quot;john&quot; ## $ SmallMat : int [1:4, 1:5] 1 2 3 4 5 6 7 8 9 10 ... ## $ OneLogical : logi TRUE ## $ AnotherList:List of 3 ## ..$ : num [1:51, 1:2] 3481823 496387 4582842 2120139 26955438 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : NULL ## .. .. ..$ : chr [1:2] &quot;Pigglywiggly&quot; &quot;Total Votes&quot; ## ..$ : num [1:51] 3481823 496387 4582842 2120139 26955438 ... ## ..$ : num 3 names(my.crazy.list) ## [1] &quot;OneTwoThree&quot; &quot;Gospels&quot; &quot;SmallMat&quot; &quot;OneLogical&quot; &quot;AnotherList&quot; There are at least four ways of accessing elements of a list my.crazy.list[[1]] ## [1] 1 2 3 my.crazy.list$OneTwoThree ## [1] 1 2 3 my.crazy.list[1] ## $OneTwoThree ## [1] 1 2 3 my.crazy.list[&quot;OneTwoThree&quot;] ## $OneTwoThree ## [1] 1 2 3 You can add elements in a similarly confusing number of ways my.crazy.list$hocuspocus&lt;-&quot;hocuspocus&quot; str(my.crazy.list) ## List of 6 ## $ OneTwoThree: num [1:3] 1 2 3 ## $ Gospels : chr [1:4] &quot;matthew&quot; &quot;mark&quot; &quot;luke&quot; &quot;john&quot; ## $ SmallMat : int [1:4, 1:5] 1 2 3 4 5 6 7 8 9 10 ... ## $ OneLogical : logi TRUE ## $ AnotherList:List of 3 ## ..$ : num [1:51, 1:2] 3481823 496387 4582842 2120139 26955438 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : NULL ## .. .. ..$ : chr [1:2] &quot;Pigglywiggly&quot; &quot;Total Votes&quot; ## ..$ : num [1:51] 3481823 496387 4582842 2120139 26955438 ... ## ..$ : num 3 ## $ hocuspocus : chr &quot;hocuspocus&quot; You can also access access/add to/subtract from the sub-elements themselves. We just add what we know about accessing elements of matrices/vectors/etc. to how we access lists. my.crazy.list[[3]][1,] # first row of my.matrix ## [1] 1 5 9 13 17 my.matrix[1,] #the same ## [1] 1 5 9 13 17 But lists don’t play well with basic commands. Only components of lists. #this won&#39;t work #my.crazy.list + 2 #but this will! my.crazy.list[[3]] + 2 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 3 7 11 15 19 ## [2,] 4 8 12 16 20 ## [3,] 5 9 13 17 21 ## [4,] 6 10 14 18 22 3.9 Arrays Arrays are effectively matrices that can have more than 2 dimensions. An array with exactly two dimensions is a matrix. The following example creates an array of two 3x4 matrices each with 3 rows and 4 columns. # Take the sequence from 1 to 24 as input to the array. result &lt;- array(1:24, dim=c(3,4,2)) result ## , , 1 ## ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 ## ## , , 2 ## ## [,1] [,2] [,3] [,4] ## [1,] 13 16 19 22 ## [2,] 14 17 20 23 ## [3,] 15 18 21 24 # You can also name all of the dimensions. column.names &lt;- c(&quot;COL1&quot;,&quot;COL2&quot;,&quot;COL3&quot;,&quot;COL4&quot;) row.names &lt;- c(&quot;ROW1&quot;,&quot;ROW2&quot;,&quot;ROW3&quot;) matrix.names &lt;- c(&quot;Matrix1&quot;,&quot;Matrix2&quot;) result2 &lt;- array(1:24, dim=c(3,4,2), dimnames=list(row.names, column.names, matrix.names)) result2 ## , , Matrix1 ## ## COL1 COL2 COL3 COL4 ## ROW1 1 4 7 10 ## ROW2 2 5 8 11 ## ROW3 3 6 9 12 ## ## , , Matrix2 ## ## COL1 COL2 COL3 COL4 ## ROW1 13 16 19 22 ## ROW2 14 17 20 23 ## ROW3 15 18 21 24 # Print the third row of the second matrix of the array. result[3,,2] ## [1] 15 18 21 24 # Print the element in the 1st row and 3rd column of the 1st matrix. result[1,3,1] ## [1] 7 # Print the 2nd Matrix. result[,,2] ## [,1] [,2] [,3] [,4] ## [1,] 13 16 19 22 ## [2,] 14 17 20 23 ## [3,] 15 18 21 24 3.10 Dataframes They are rectangular like a matrix, but each column can be of a different class and you can access elements of the data frame like it’s a list. turnout &lt;- tv/vap voting.data &lt;- data.frame(tv, vap, turnout) head(voting.data) ## tv vap turnout ## 1 NA 3481823 NA ## 2 238307 496387 0.4800831 ## 3 1553032 4582842 0.3388797 ## 4 780409 2120139 0.3680933 ## 5 8899059 26955438 0.3301397 ## 6 1586105 3617942 0.4383998 str(voting.data) ## &#39;data.frame&#39;: 51 obs. of 3 variables: ## $ tv : num NA 238307 1553032 780409 8899059 ... ## $ vap : num 3481823 496387 4582842 2120139 26955438 ... ## $ turnout: num NA 0.48 0.339 0.368 0.33 ... There are at least four ways to access a variable head(voting.data[[1]]) ## [1] NA 238307 1553032 780409 8899059 1586105 head(voting.data$tv) ## [1] NA 238307 1553032 780409 8899059 1586105 head(voting.data[&quot;tv&quot;]) ## tv ## 1 NA ## 2 238307 ## 3 1553032 ## 4 780409 ## 5 8899059 ## 6 1586105 head(voting.data[,1]) ## [1] NA 238307 1553032 780409 8899059 1586105 names(voting.data) #access the column names ## [1] &quot;tv&quot; &quot;vap&quot; &quot;turnout&quot; # NOTE: dataframes always have column names!! colnames(voting.data) ## [1] &quot;tv&quot; &quot;vap&quot; &quot;turnout&quot; We can change back and forth between a matrix and a data frame as.data.frame(my.matrix) ## V1 V2 V3 V4 V5 ## 1 1 5 9 13 17 ## 2 2 6 10 14 18 ## 3 3 7 11 15 19 ## 4 4 8 12 16 20 data.frame(my.matrix) ## X1 X2 X3 X4 X5 ## 1 1 5 9 13 17 ## 2 2 6 10 14 18 ## 3 3 7 11 15 19 ## 4 4 8 12 16 20 head(as.matrix(voting.data)) ## tv vap turnout ## [1,] NA 3481823 NA ## [2,] 238307 496387 0.4800831 ## [3,] 1553032 4582842 0.3388797 ## [4,] 780409 2120139 0.3680933 ## [5,] 8899059 26955438 0.3301397 ## [6,] 1586105 3617942 0.4383998 Sometimes all of this can be a bit cumbersome The ‘with’ command will run a function with the dataset slightly easier # these are equivalent mean(voting.data$vap) ## [1] 4430673 with(voting.data, mean(vap)) ## [1] 4430673 3.11 General info You can determine the type of a variable by calling class on it as follows class(vap) ## [1] &quot;numeric&quot; There are functions to move back and forth between types (‘as.class’ functions) grp &lt;- c(&quot;control&quot;, &quot;treatment&quot;, &quot;control&quot;, &quot;treatment&quot;) grp &lt;- factor(grp) as.integer(grp) ## [1] 1 2 1 2 You can also check if an object is a specific class. is.integer(vap) ## [1] FALSE is.numeric(vap) ## [1] TRUE There are several types of these as.character(0:5) # Turning numbers into characters ## [1] &quot;0&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; as.logical(0:5) # All numbers but &#39;0&#39; become TRUE ## [1] FALSE TRUE TRUE TRUE TRUE TRUE But not all of them work as.numeric(c(&#39;1&#39;, &#39;2&#39;, &#39;3&#39;)) ## [1] 1 2 3 as.numeric(c(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;)) ## Warning: NAs introduced by coercion ## [1] NA NA NA "],
["section-files-for-loops-and-functions.html", "4 Files, For Loops, and Functions 4.1 The working directory 4.2 Input and output 4.3 if, else, ifelse 4.4 repeat and while 4.5 for loops 4.6 Functions 4.7 Setting defaults 4.8 Scope 4.9 Debugging 4.10 Benchmarking and code improvements", " 4 Files, For Loops, and Functions 4.1 The working directory First we want to change the working directory. This is the folder where R will save and look for data by default (although you can always override this) Note that the file path (in Windows at least) requires slashes. You can either use two back slashes to separate folders or one forward slash. setwd(&quot;~/Dropbox/Classes/R Programming&quot;) ## For Windows, will need to be something like setwd(&quot;C:\\\\Documents and Settings\\\\Jacob Montgomery\\\\My Documents\\\\RWork&quot;) ## or setwd(&quot;C:/Documents and Settings/Jacob Montgomery/My Documents/RWork&quot;) ## or setwd(file.path(&quot;C:&quot;, &quot;Documents and Settings&quot;, &quot;Jacob Montgomery&quot;, &quot;My Documents&quot;, &quot;RWork&quot;)) 4.2 Input and output voting.data&lt;-data.frame(vap, tv) Now let’s save our newly created dataset dump(&quot;voting.data&quot;, &quot;voting.data.R&quot;) # inputs are string. Note that you must put the *.R in yourself You might also want to save multiple objects dump(c(&quot;vap&quot;, &quot;tv&quot;), &quot;voting.data.vectors.R&quot;) dump(list=objects(), &quot;everything.R&quot;) ## an alternative we can just use the command save.image(&quot;everything.RData&quot;) # saves an image of your current workspace Now we can clear our wokspace, and load these objects rm(list=ls()) source(&quot;voting.data.vectors.R&quot;) ls() # only saved objects present To read/write dataframes from/to a csv (comma separated values) file, the following commands will be useful votes.06 &lt;- read.csv(&quot;~/Dropbox/Classes/R Programming/R Scripts/VotingData2006.csv&quot;, header=T) write.csv(votes.06, file=&quot;VD06.csv&quot;) Tips: All of the functions for reading in data are actually using the scan() function. No matter how crazy your data look, you can always read it into R by clever use of scan(). 4.3 if, else, ifelse The basic syntax for an if call is if(condition){ commands to run } The inputs in the parentheses needs can be anything that returns a logical. You can put any code inside the braces The simplest examples possible: if(TRUE) { print(&quot;I got here&quot;) } ## [1] &quot;I got here&quot; if(FALSE){ print(&quot;I can&#39;t get here&quot;) } You can combine this with else if(condition) { commands to run when condition is TRUE } else { # notice that these are on the same line commands to run when the condition is FALSE } x = 10 if(x &gt; 2) { print(&quot;X is larger than 2&quot;) } else { print(&quot;X is 2 or smaller&quot;) } ## [1] &quot;X is larger than 2&quot; But this set up will not play well with vectors. It runs, but is confusing if (c(3, 1) &gt; 2){ print(&quot;This won&#39;t work&quot;) } ## Warning in if (c(3, 1) &gt; 2) {: the condition has length &gt; 1 and only the first ## element will be used ## [1] &quot;This won&#39;t work&quot; The ifelse command works nicely with vectors, but syntax is different. This command is equivalent to a ternary in other languages if you’re familiar with that term. ifelse(condition, return when condition T, return when condition F) x &lt;- c(0, 2) ifelse(x &gt; 1, &quot;yes&quot;, &quot;no&quot;) ## [1] &quot;no&quot; &quot;yes&quot; WARNING If your outputs are vectors ifelse will work element-wise yes = c(&quot;yes1&quot;, &quot;yes2&quot;) no = c(&quot;no1&quot;, &quot;no2&quot;) ifelse(x &gt; 1, yes, no) ## [1] &quot;no1&quot; &quot;yes2&quot; ifelse(x &lt; 2, yes, no) ## [1] &quot;yes1&quot; &quot;no2&quot; Note that the curly braces are not technically necessary if you have only a one line command BUT you should use them anyway so someone can easily read your code! x &lt;- 3 if (x &gt; 2) y &lt;- 2 * x else y &lt;- 3 * x y ## [1] 6 4.4 repeat and while Repeat just repeats commands in the braces until it sees a BREAK command If you don’t include BREAK your computer will be in an infinite loop. Save your work before using! Or maybe just don’t use it repeat is equivalent to a while(true) loop repeat{ stuff to do until it sees BREAK } # make a blank plot with the limits set by those vectors plot(NULL, xlim=c(0, 100), ylim=c(0, 1), xlab=&quot;x&quot;, ylab=&quot;1/x&quot;) x = 1 repeat { y = 1 / x x = x + 1 points(x, y) if (x == 100) { break } } A while loop is just a repeat, where the break condition is specified at the top while(condition){ commands that repeat until condition becomes FALSE } plot(NULL, xlim=c(0, 100), ylim=c(0, 1), xlab=&quot;x&quot;, ylab=&quot;1/x&quot;) x = 1 while(x &lt; 100) { y = 1 / x x = x + 1 points(x, y) } 4.5 for loops The for command is probably the most common flow control option. It has three basic parts: An object name that will be used in the following commands A vector that we will “loop over” Commands that will be executed for each value of the vector for (name in vector){ execute these commands using each value of the vecotr } for (monkey in c(&quot;Spider&quot;, &quot;Howler&quot;, &quot;Rhesus&quot;)) { ## Each loop does the equivalent of: monkey = &quot;Spider&quot; or monkey = &quot;Howler&quot; or ... print(monkey) } ## [1] &quot;Spider&quot; ## [1] &quot;Howler&quot; ## [1] &quot;Rhesus&quot; Or more commonly for (i in 1:10){ print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 ## for loops are very useful in many situations plot(NULL, xlim=c(0, 100), ylim=c(0, 1)) for (i in 1:100) { points(i, 1 / i) } 4.5.1 next and break Sometimes you might not want to execute the commands for every element in the vector use the next command to skip (you can also use the break) some.odds = NULL for (i in 1:200) { if (i %% 2 == 0) { next } some.odds = c(some.odds, i) } some.odds ## [1] 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 ## [19] 37 39 41 43 45 47 49 51 53 55 57 59 61 63 65 67 69 71 ## [37] 73 75 77 79 81 83 85 87 89 91 93 95 97 99 101 103 105 107 ## [55] 109 111 113 115 117 119 121 123 125 127 129 131 133 135 137 139 141 143 ## [73] 145 147 149 151 153 155 157 159 161 163 165 167 169 171 173 175 177 179 ## [91] 181 183 185 187 189 191 193 195 197 199 Technically, you don’t have to be so formal. But the indenting and braces are there for the protection of your future self. for (i in 1:10) print(i) ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 4.6 Functions 4.6.1 The basics Use the ‘function’ command and assign it to an object If you need, specify the expected inputs Do some stuff (optional) Return something (and only one thing) to the global environment. my.function = function(x) { do stuff here return(output) } Here’s an example: countThrees = function(vector) { threes &lt;- 0 for(i in vector) { if(i == 3) { threes &lt;- threes + 1 } } return(threes) } v &lt;- c(1, 2, 3, 4, 3, 3) countThrees(v) ## [1] 3 What would happen if we passed in one number instead of a vector? countThrees(2) ## [1] 0 countThrees(3) ## [1] 1 Or a matrix? m &lt;- matrix(1:24, 4, 6) countThrees(m) ## [1] 1 If you want to return multiple values in R, you must put them in a vector or list and return the data structure instead. Remember that a list is essentially a vector that can contain different data types. Sometimes a function will return a value, but other times it will just execute a command like print or plot. 4.7 Setting defaults You can set default values for some of your arguments or all of them gaga.equation = function(num.rah=2, num.ah=3, num.ga=2, num.la=2, num.oo=1) { rahs = paste(rep(&quot;RAH&quot;, num.rah), collapse=&quot;, &quot;) ahs = paste(rep(&quot;AH&quot;, num.ah), collapse=&quot;, &quot;) gas = paste(rep(&quot;GA&quot;, num.ga), collapse=&quot;, &quot;) oo = paste(rep(&quot;OO&quot;, num.oo), collapse=&quot;, &quot;) las = paste(rep(&quot;LA&quot;, num.la), collapse=&quot;, &quot;) paste(rahs, &quot;,&quot;, ahs, &quot;! ROMA, ROMAMA!&quot;, gas, &quot;,&quot;, oo, las) } gaga.equation() ## [1] &quot;RAH, RAH , AH, AH, AH ! ROMA, ROMAMA! GA, GA , OO LA, LA&quot; gaga.equation(num.la=5) ## [1] &quot;RAH, RAH , AH, AH, AH ! ROMA, ROMAMA! GA, GA , OO LA, LA, LA, LA, LA&quot; If an argument (sometimes called a parameter) does not have a default value, it must be specified when the function is called. 4.8 Scope In the example above, if we tried to print rahs outside of the function, we would get an error. The variable rahs doesn’t exist in the global environment. This concept of limiting which parts of code can access a variable or function is called scope. print(rahs) What happens in the function, stays in the function One exception is that for loop variables can be accessed after the loop. for(i in 1:4) { print(i+2) } ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 print(i) ## [1] 4 This for loop could be written as a while loop which would demonstrate why we can access i outside the loop. In fact, when the code is broken down from our nicely readable R code into standard machine code that any computer can run, all for loops are converted into while loops. i &lt;- 0 while(i &lt; 4) { i &lt;- i+1 print(i+2) } ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 print(i) ## [1] 4 Note that i equals 4 (its last loop value) after the loop is finished. This is different from functions because the only way to retain access to a local variable that was created in the function is to return it. a &lt;- 10 outer_func &lt;- function() { a &lt;- 20 inner_func &lt;- function() { a &lt;- 30 print(a) } inner_func() print(a) } outer_func() ## [1] 30 ## [1] 20 a ## [1] 10 If you try to run inner_func(), it will say “Error in inner_func() : could not find function”inner_func\"\" because that function is actually a local object inside the function outer_func. That means that outer_func is the only place that can use inner_func. Scope kind of works like a bunch of one way mirrors. If I write a function, it can access all global variables and local (to the function) variables, but the global environment can only access global variables. In the above code example, inner_func can access all of its variables, all of outer_func’s variables, and the global environment variables as well. 4.8.1 Advanced scope topics In R, there’s a superassignment operator which is &lt;&lt;- This assigns the variable to one environment up from its current location. Check out these examples and try to follow along with the environments. This example will replace the global variable a (which used to store 10) with 20 from the outer_func code. Printing a at the end of the script will print 20 this time. a &lt;- 10 outer_func &lt;- function() { a &lt;&lt;- 20 inner_func &lt;- function() { a &lt;- 30 print(a) } inner_func() print(a) } outer_func() ## [1] 30 ## [1] 20 a ## [1] 20 This example will replace a from the outer_func environment with the value 30 so when outer_func prints a, it will now contain 30. a &lt;- 10 outer_func &lt;- function() { a &lt;- 20 inner_func &lt;- function() { a &lt;&lt;- 30 print(a) } inner_func() print(a) } outer_func() ## [1] 30 ## [1] 30 a ## [1] 10 4.9 Debugging traceback will help you identify the function that is failing debug, debugonce will go through a function one line at a time browser This let’s you work within the function environment starting at a specified point Here’s an example to help us practice our debugging skills: webData&lt;-url(&quot;http://pages.wustl.edu/montgomery/incumbents2.txt&quot;) OOS &lt;- read.table(webData) summary(OOS) ## x year congress chalspend ## Min. : 1 Min. :1956 Min. : 84.00 Min. : 8.517 ## 1st Qu.:1672 1st Qu.:1968 1st Qu.: 90.00 1st Qu.: 9.315 ## Median :3347 Median :1978 Median : 95.00 Median :10.998 ## Mean :3348 Mean :1977 Mean : 94.42 Mean :10.880 ## 3rd Qu.:5024 3rd Qu.:1988 3rd Qu.:100.00 3rd Qu.:12.366 ## Max. :6695 Max. :1996 Max. :104.00 Max. :15.039 ## NA&#39;s :3380 ## incspend difflog presvote voteshare ## Min. : 8.586 Min. :-3.060 Min. :0.06565 Min. :0.3476 ## 1st Qu.:12.304 1st Qu.: 0.686 1st Qu.:0.46894 1st Qu.:0.5740 ## Median :12.839 Median : 1.651 Median :0.55519 Median :0.6450 ## Mean :12.759 Mean : 1.858 Mean :0.55335 Mean :0.6470 ## 3rd Qu.:13.266 3rd Qu.: 3.025 3rd Qu.:0.63425 3rd Qu.:0.7141 ## Max. :15.422 Max. : 5.856 Max. :0.96061 Max. :0.9997 ## NA&#39;s :3374 NA&#39;s :3436 NA&#39;s :125 ## inparty incparty seniority midterm ## Min. :0.0000 Min. :0.000 Min. : 1.000 Min. :0.0000 ## 1st Qu.:0.0000 1st Qu.:0.000 1st Qu.: 2.000 1st Qu.:0.0000 ## Median :0.0000 Median :1.000 Median : 4.000 Median :0.0000 ## Mean :0.4942 Mean :0.563 Mean : 4.949 Mean :0.2327 ## 3rd Qu.:1.0000 3rd Qu.:1.000 3rd Qu.: 7.000 3rd Qu.:0.0000 ## Max. :1.0000 Max. :1.000 Max. :26.000 Max. :1.0000 ## ## chalquality south population urban ## Min. :0.0000 Min. :0.0000 Min. :11.98 Min. : 5.956 ## 1st Qu.:0.0000 1st Qu.:0.0000 1st Qu.:12.92 1st Qu.:12.325 ## Median :0.0000 Median :0.0000 Median :13.06 Median :12.728 ## Mean :0.2348 Mean :0.2378 Mean :13.04 Mean :12.612 ## 3rd Qu.:0.0000 3rd Qu.:0.0000 3rd Qu.:13.16 3rd Qu.:13.006 ## Max. :1.0000 Max. :1.0000 Max. :17.08 Max. :16.481 ## NA&#39;s :5 NA&#39;s :2 NA&#39;s :2 ## age65 milpop unemployed incspend2 ## Min. : 8.164 Min. : 3.045 Min. : 7.425 Min. : 5355 ## 1st Qu.:10.452 1st Qu.: 5.517 1st Qu.: 8.816 1st Qu.: 220620 ## Median :10.760 Median : 6.246 Median : 9.142 Median : 376741 ## Mean :10.819 Mean : 6.777 Mean : 9.170 Mean : 450124 ## 3rd Qu.:11.215 3rd Qu.: 8.077 3rd Qu.: 9.544 3rd Qu.: 577283 ## Max. :14.552 Max. :12.691 Max. :13.147 Max. :4987593 ## NA&#39;s :2 NA&#39;s :2 NA&#39;s :2 NA&#39;s :3374 boxplot(voteshare~inparty, data=OOS[OOS$year==1956,]) plot(voteshare~inparty, data=OOS[OOS$year==1956,]) Running a regression by year: Do you see a pattern? output.vector&lt;-NULL for (i in unique(OOS$year)){ output.vector[which(unique(OOS$year) == i)]&lt;- lm(voteshare ~ inparty, data=OOS[OOS$year == i,])$coefficients[2] } plot(unique(OOS$year), output.vector, type=&quot;l&quot;, ylab=&quot;Coefficient (President&#39;s party)&quot;, xlab=&quot;Year&quot;) abline(h=0, lty=3) Now we might think – there is a more general case where I want to get a bunch of seperate regression estimates on defined subsets of the data by.var.lm&lt;-function(by.var, formula, data, coef.num){ output.vector&lt;-NULL for (i in unique(by.var)){ output.vector[which(unique(by.var) == i)] &lt;- lm(formula, data=data[by.var == i,])$coefficients[coef.num] } return(output.vector) } plot(by.var.lm(year, voteshare~inparty, OOS, 2), type=&quot;l&quot;) This is not working, what to do? As a first step, figure out where it’s broken using traceback traceback(by.var.lm(year, voteshare~inparty, OOS, 2)) A second thing to try is to use debug debug(by.var.lm) A second thing to try is to put a broswer function into the function itself browser() Another thing to try is just put in some various print commands by.var.lm&lt;-function(by.var, formula, data, coef.num){ output.vector&lt;-NULL; print(&quot;one&quot;) for (i in unique(by.var)){ print(i) # I can see where I get to in the loop output.vector[which(unique(by.var) == i)]&lt;- lm(formula, data=data[by.var == i,])$coefficients[coef.num] } print(&#39;got out of loop&#39;) return(output.vector) } plot(by.var.lm(year, voteshare~inparty, OOS, 2), type=&quot;l&quot;) So now I have a function that works, can I use it again? website&lt;-url(&quot;http://pages.wustl.edu/montgomery/titanic&quot;) titanic&lt;-read.delim(website) table(titanic$Gender) table(titanic$Class) Modify the following code to make this work by.var.lm(Class, (as.numeric(Survived)-1) ~ Gender, titanic, 2) 4.10 Benchmarking and code improvements x &lt;- runif(500) system.time(sqrt(x)) ## user system elapsed ## 0 0 0 The goal is to see how long a function takes to evaluate Much better: microbenchmark library(microbenchmark) microbenchmark(sqrt(x)) # evaluates 100 times per default ## Unit: microseconds ## expr min lq mean median uq max neval ## sqrt(x) 4.4 4.5 4.882 4.6 4.8 14.8 100 microbenchmark(sqrt(x), times=1000) ## Unit: microseconds ## expr min lq mean median uq max neval ## sqrt(x) 3.4 3.6 4.445 3.8 4.1 230.6 1000 Now we can compare different functions microbenchmark(sqrt(x), x^0.5, times=1000) ## Unit: microseconds ## expr min lq mean median uq max neval ## sqrt(x) 3.5 4.30 5.6671 5.2 6.2 32.9 1000 ## x^0.5 23.0 26.05 32.3059 31.5 37.4 86.8 1000 microbenchmark(sqrt(x), x^0.5, x^(1/2), exp(log(x)/2), times=1000) ## Unit: microseconds ## expr min lq mean median uq max neval ## sqrt(x) 3.6 3.9 6.9096 6.1 6.4 595.7 1000 ## x^0.5 23.2 24.5 34.6756 39.7 40.2 325.5 1000 ## x^(1/2) 23.4 24.8 35.1421 40.3 40.8 249.2 1000 ## exp(log(x)/2) 63.1 67.0 92.3819 107.9 108.9 169.7 1000 And also completely different functions microbenchmark(sqrt(x), x^4-3*x) ## Unit: microseconds ## expr min lq mean median uq max neval ## sqrt(x) 3.7 3.8 4.050 3.8 4.1 7.7 100 ## x^4 - 3 * x 34.4 34.7 35.218 34.9 35.3 46.4 100 For ease of interpretation, if a microbenchmark takes - 1 millisecond, then 1,000 calls take a second - 1 microsecond, then 1,000,000 calls take a second - 1 nanosecond, then 1,000,000,000 calls take a second Or use unit=eps for evaluations per second microbenchmark(sqrt(x), x^0.5, x^(1/2), exp(log(x)/2), unit=&quot;eps&quot;) ## Unit: evaluations per second ## expr min lq mean median uq max ## sqrt(x) 163934.43 240998.84 255123.21 263157.89 270270.27 285714.29 ## x^0.5 36630.04 40983.61 41782.66 42372.88 42918.45 43290.04 ## x^(1/2) 26737.97 40650.41 41107.66 41666.67 42372.88 42735.04 ## exp(log(x)/2) 11402.51 15197.57 15483.84 15772.87 15847.86 15923.57 ## neval ## 100 ## 100 ## 100 ## 100 Evaluating every function takes time Evaluating () or {} takes time Even specifying useless arguments in functions takes time! f0 &lt;- function() NULL f1 &lt;- function(a=1) NULL f2 &lt;- function(a=1, b=2) NULL f3 &lt;- function(a=1, b=2, c=3) NULL f4 &lt;- function(a=1, b=2, c=3, d=4) NULL f5 &lt;- function(a=1, b=2, c=3, d=4, e=5) NULL microbenchmark(f0(), f1(), f2(), f3(), f4(), f5(), times=10000) ## Unit: nanoseconds ## expr min lq mean median uq max neval ## f0() 300 500 788.54 500 600 1705600 10000 ## f1() 400 600 892.91 700 700 1351800 10000 ## f2() 500 700 1036.47 800 900 1656400 10000 ## f3() 500 800 1111.03 900 900 1389800 10000 ## f4() 600 900 1209.50 900 1100 1462400 10000 ## f5() 700 1000 1336.36 1000 1200 1561300 10000 Extracting one element of a data frame microbenchmark( &quot;[32, 11]&quot; = mtcars[32,11], &quot;$carb[32]&quot; = mtcars$carb[32], &quot;[[c(11, 32)]]&quot; = mtcars[[c(11,32)]], &quot;[[11]][32]&quot; = mtcars[[11]][32], &quot;.subset2&quot; = .subset2(mtcars,11)[32]) ## Unit: nanoseconds ## expr min lq mean median uq max neval ## [32, 11] 16500 17000 20365 17400 21850 65700 100 ## $carb[32] 1600 2100 2623 2300 2500 10000 100 ## [[c(11, 32)]] 8300 9050 14257 9600 10450 384400 100 ## [[11]][32] 8100 8500 10953 9100 10300 65800 100 ## .subset2 500 600 769 600 700 4900 100 4.10.1 Vectorizing The key idea behind vectorizing your code is to think about entire vectors instead of thinking about their components. Using apply and co instead of for loops is a start, but does not really solve this issue. Truly vectorized functions will make use of code written in C instead of R. Loops in C are much faster because they have much less overhead. Addition on each element of a data frame rm(list=ls()) m=5 n=5 matrix1 &lt;- replicate(m, rnorm(n)) # create matrix matdf &lt;- matdf1 &lt;- data.frame(matrix1) matdf ## X1 X2 X3 X4 X5 ## 1 -0.7824547 0.25598597 0.01284530 -1.0915781 -0.1631552 ## 2 -0.4751539 -1.04169912 -0.18581853 -0.6593323 -0.1046822 ## 3 -0.2744757 -1.67842419 0.45769294 -1.0718766 1.5019671 ## 4 0.9377746 0.08036026 -0.31900260 0.5670033 1.6098025 ## 5 -0.4290179 -0.05465640 -0.06238108 -0.0359416 -0.6344422 for (i in 1:m) { for (j in 1:n) { matdf1[i,j] &lt;- matdf1[i,j] + 1.87*cos(.25)*pi # addition } } matdf1 ## X1 X2 X3 X4 X5 ## 1 4.909691 5.948132 5.704991 4.600568 5.528990 ## 2 5.216992 4.650447 5.506327 5.032813 5.587463 ## 3 5.417670 4.013721 6.149839 4.620269 7.194113 ## 4 6.629920 5.772506 5.373143 6.259149 7.301948 ## 5 5.263128 5.637489 5.629765 5.656204 5.057703 matdf2&lt;-data.frame(matrix1) matdf2 &lt;- matdf2 + 1.87*cos(.25)*pi matdf2 ## X1 X2 X3 X4 X5 ## 1 4.909691 5.948132 5.704991 4.600568 5.528990 ## 2 5.216992 4.650447 5.506327 5.032813 5.587463 ## 3 5.417670 4.013721 6.149839 4.620269 7.194113 ## 4 6.629920 5.772506 5.373143 6.259149 7.301948 ## 5 5.263128 5.637489 5.629765 5.656204 5.057703 microbenchmark( &quot;loop&quot; = for (i in 1:m) { for (j in 1:n) { matdf[i,j] &lt;- matdf[i,j] + 1.87*cos(.25)*pi } }, &quot;vectorized&quot; = matdf &lt;- matdf + 1.87*cos(.25)*pi ) ## Unit: microseconds ## expr min lq mean median uq max neval ## loop 10208.4 18421.25 20590.01 19269.9 20197.65 85180.5 100 ## vectorized 525.4 963.40 1116.23 1078.2 1134.80 2987.8 100 mat1 &lt;- matrix(abs(rnorm(2500))+pi, ncol=50) apply(mat1, 1, function(x) sum(x)) ## [1] 201.4760 198.4106 196.4269 192.8845 195.5522 199.8087 194.0044 198.9135 ## [9] 188.2862 197.7271 200.4519 191.8800 200.5671 196.8226 191.8065 200.8013 ## [17] 197.6897 200.0398 196.1424 198.2996 194.4522 190.3822 198.6736 195.2876 ## [25] 192.0073 198.3157 185.7921 201.9774 202.1451 198.8651 203.4932 194.6939 ## [33] 198.9695 198.2261 201.7890 194.4241 202.1621 193.2276 196.6065 192.8621 ## [41] 196.0571 188.1541 211.0039 195.9336 198.2533 198.9841 196.4598 195.9275 ## [49] 197.8557 192.3531 rowSums(mat1) ## [1] 201.4760 198.4106 196.4269 192.8845 195.5522 199.8087 194.0044 198.9135 ## [9] 188.2862 197.7271 200.4519 191.8800 200.5671 196.8226 191.8065 200.8013 ## [17] 197.6897 200.0398 196.1424 198.2996 194.4522 190.3822 198.6736 195.2876 ## [25] 192.0073 198.3157 185.7921 201.9774 202.1451 198.8651 203.4932 194.6939 ## [33] 198.9695 198.2261 201.7890 194.4241 202.1621 193.2276 196.6065 192.8621 ## [41] 196.0571 188.1541 211.0039 195.9336 198.2533 198.9841 196.4598 195.9275 ## [49] 197.8557 192.3531 microbenchmark(apply(mat1, 1, function(x) sum(x)), rowSums(mat1)) ## Unit: microseconds ## expr min lq mean median uq max ## apply(mat1, 1, function(x) sum(x)) 179.5 234.3 330.671 248.05 314.4 2850.5 ## rowSums(mat1) 11.3 14.9 22.671 18.60 28.5 60.6 ## neval ## 100 ## 100 Even for basic tasks, think about the actual calculations you perform mat2 &lt;- matrix(sample(1:7, 90000, replace=T), ncol=300) mat3 &lt;- matrix(sample(2:6, 90000, replace=T), ncol=300) ys &lt;- sample(3:5, 300, replace=T) all.equal(mat2 %*% mat3 %*% ys , mat2 %*% (mat3 %*% ys)) ## [1] TRUE microbenchmark(mat2 %*% mat3 %*% ys, mat2 %*% (mat3 %*% ys)) ## Unit: microseconds ## expr min lq mean median uq max ## mat2 %*% mat3 %*% ys 22868.0 62402.85 92741.38 88060.55 98732.8 295829.6 ## mat2 %*% (mat3 %*% ys) 822.5 10626.75 37091.54 18351.95 27888.7 229087.2 ## neval ## 100 ## 100 Why? Think through the dimensionality 4.10.2 Paste/collapse and copies random_states &lt;- function() { paste(sample(state.name,10,replace =TRUE),collapse =&quot;&quot;) } states10 &lt;- replicate(10, random_states()) states10 ## [1] &quot;North CarolinaLouisianaWyomingNew JerseyUtahFloridaNevadaIdahoWyomingWisconsin&quot; ## [2] &quot;Rhode IslandWest VirginiaPennsylvaniaIowaKentuckyOklahomaOklahomaVermontOklahomaSouth Carolina&quot; ## [3] &quot;West VirginiaMarylandMassachusettsAlaskaMississippiMichiganNew JerseyAlaskaVirginiaConnecticut&quot; ## [4] &quot;DelawareRhode IslandVermontSouth DakotaPennsylvaniaNebraskaVirginiaFloridaNevadaMassachusetts&quot; ## [5] &quot;MissouriNevadaCaliforniaCaliforniaNew MexicoFloridaMichiganMichiganWashingtonKansas&quot; ## [6] &quot;New HampshireLouisianaWest VirginiaWisconsinMarylandMassachusettsUtahTennesseeOhioArkansas&quot; ## [7] &quot;New MexicoWyomingWyomingNew HampshireArkansasMissouriTexasIdahoPennsylvaniaNebraska&quot; ## [8] &quot;MontanaWyomingColoradoIowaWest VirginiaOhioIllinoisSouth CarolinaIllinoisVermont&quot; ## [9] &quot;New HampshireOhioMissouriLouisianaDelawareTennesseeIllinoisCaliforniaArkansasMississippi&quot; ## [10] &quot;OregonIdahoAlaskaOhioAlabamaKentuckyCaliforniaIllinoisMichiganHawaii&quot; states100 &lt;- replicate(100, random_states()) collapse &lt;- function(states) { out &lt;- &quot;&quot; for (x in states) { out &lt;- paste0(out, x) # same as paste(..., sep=&quot;&quot;, collapse) } out } microbenchmark( &quot;loop10&quot; = collapse(states10), &quot;vec10&quot; = paste(states10, collapse =&quot;&quot;), &quot;loop100&quot; = collapse(states100), &quot;vec100&quot; = paste(states100, collapse =&quot;&quot;) ) ## Unit: microseconds ## expr min lq mean median uq max neval ## loop10 42.1 43.85 52.637 46.80 51.2 136.3 100 ## vec10 9.7 11.00 14.265 12.80 16.5 29.0 100 ## loop100 1573.9 1648.10 1933.224 1703.05 1798.1 13385.8 100 ## vec100 84.4 87.60 101.157 92.65 104.8 193.7 100 PRO TIP: Allocate memory and fill, don’t append to the end Here, we are not only getting around using the loop, but also avoiding copies. Whenever you append(), cbind(), rbind(), or paste() to create a bigger object, R must first allocate space for the new object and then copy the old object to its new home. If you’re repeating this many times, like in a for loop, this can be quite computationally expensive. Instead, allocate an object of the largest size you think you’ll need, and fill it up as you go. Fun fact: in Java (another programming language), arrays function the same way as vectors in R. Java has a data structure called an ArrayList, which allows users to add and remove as needed. When it gets full, it doubles the size/capacity of the underlying array and copies everything over. When an element is removed, it doesn’t decrease the size of the underlying array because copying things over and allocating new space is inefficient. "],
["section-version-control.html", "5 Version Control 5.1 Why version control? 5.2 Setup 5.3 Workflow 5.4 Branches 5.5 Forking 5.6 Reverting 5.7 Conflict management 5.8 Version Control in RStudio", " 5 Version Control 5.1 Why version control? Have you ever … (Source: Stack overflow) - Made a change to code, realised it was a mistake and wanted to revert back? - Lost code or had a backup that was too old? - Had to maintain multiple versions of a product? - Wanted to see the difference between two (or more) versions of your code? - Wanted to prove that a particular change broke or fixed a piece of code? - Wanted to review the history of some code? - Wanted to submit a change to someone else’s code? - Wanted to share your code, or let other people work on your code? - Wanted to see how much work is being done, and where, when and by whom? - Wanted to experiment with a new feature without interfering with working code? 5.2 Setup Install Git: http://git-scm.com/book/en/Getting-Started-Installing-Git Sign up for GitHub Install the GUI http://mac.github.com/ http://windows.github.com/ 5.3 Workflow Every time you work on a group project that is stored in git, you should pull right away, do your work, then commit and push immediately. I recommend doing this workflow every time you make a significant change or start working on something else. It backs up your work so that you can go back to an earlier version in case you mess something up later. 5.3.1 Pull This updates your repository to the newest version so that when other people on your team make changes, you will be able to have the most recently updated version of the code/repository. In the picture above, to pull, click on “Fetch origin”. 5.3.2 Commit This is step 1 of adding your changes to GitHub (push is step 2). Committing your changes requires that you specify which changed files you want to commit as well as a brief commit message summarizing what you changed since the last commit. In the picture above, to commit, click on “Commit to master” once you’ve selected the files to commit in the left pane. 5.3.3 Push This is step 2 of adding your changes to GitHub. You can think of committimg as wrapping up a gift that you made and pushing is mailing it to the recipient. They don’t “open” the gift until they Pull and see what you pushed. Pushing uploads all un-pushed commits to GitHub so that you can see them in GitHub desktop as well as your repository online. In the picture above, after you’ve committed, to push, click on “Push origin”. 5.4 Branches Large software projects use branches to have a production environment that is always working, and other branches for new feature development. Once a feature is ready, the development branch can be merged with the production branch (typically master). You’ll have to manually choose which version of each file to keep. In the picture above, to switch branches, click on the “Current branch” button and either create a new branch or select an existing one. 5.5 Forking If you see someone’s repository and want to copy it and make your own changes without affecting their repository (and potentially without them even knowing), you can “fork” the repository. This gives you your own copy of it to mess with as you please. 5.6 Reverting If you make a mistake and commit and push it (or your team member does), you can revert to a prior commit. This is why it’s super useful to commit and push regularly!!! You can revert from any commit which will set your current repository state back to exactly what it looked like at that exact moment in time. 5.7 Conflict management To create a conflict, you’ll need two people with access to the same repository (or two computers that both have the repo cloned). - Both people/computers sync back up - Person/computer 1 should add a new file - Person/computer 1 should commit and push their new file - Person/computer 2 should pull and see the new file - Person/computer 1 should type something in this file like “hello world” - Person/computer 1 should commit and push - Person/computer 2 should type something in this file like “helol world” - When person/computer 2 commits and pushes, it will cause a conflict that they will have to resolve Here’s a gif of how merge conflicts work in GitHub desktop 5.8 Version Control in RStudio If you have a GitHub repository that you want to open in RStudio, here are the steps: Open RStudio File -&gt; New Project -&gt; Version Control -&gt; Git Paste your repository URL and change where you want it to clone the repository on your local machine All of the files in the repository are now found in the “files” tab in RStudio. To pull, open the “git” tab and click the blue down arrow. To commit, select the check boxes next to the changed files you wish to commit and click the “commit” button. To push, click the green up arrow. Note: if you want to switch branches, you can click on “master” in the git tab. To switch projects, click on the cube icon in the top right corner. This gives you options for creating a new project, viewing recent projects, and opening existing projects. For more info, check out this link or this video: "],
["section-data-visualization-in-r.html", "6 Data Visualization in R 6.1 Intro to ggplot2 6.2 More geom options", " 6 Data Visualization in R 6.1 Intro to ggplot2 ggplot2 is a powerful way to build both simple and complex data visualizations Takes care of a lot of the stupid aspects of plot building Provides a language for layering visual elements Has been extended in dozens of ways to handle all kinds of data Integrates easily into the rest of the tidyverse 6.1.1 Example Data comes from 538.com library(ggplot2) primaryPolls&lt;-read.csv(&#39;https://jmontgomery.github.io/PDS/Datasets/president_primary_polls_feb2020.csv&#39;, stringsAsFactors = F) primaryPolls$start_date&lt;-as.Date(primaryPolls$start_date, &quot;%m/%d/%Y&quot;) primaryPolls&lt;-primaryPolls[primaryPolls$state==&quot;New Hampshire&quot;,] primaryPolls&lt;-primaryPolls[primaryPolls$candidate_name%in%c(&quot;Amy Klobuchar&quot;, &quot;Bernard Sanders&quot;, &quot;Elizabeth Warren&quot;, &quot;Joseph R. Biden Jr.&quot;, &quot;Michael Bloomberg&quot;, &quot;Pete Buttigieg&quot;),] ggplot(data=primaryPolls)+ geom_point(mapping = aes(x=start_date, y=pct)) The first line is always ggplot, which sets up the basic object that we will layer onto Then we use the + to add layers. In this case geom_points. THe + must be at the end of each line - not the beginning. Any geom layer requires a mapping argument, which itself comes with an aes argument explaining what goes on the x and y coordinates. Conveniently, the dataset only needs to be specified once. 6.1.2 Aesthetics The aes stands for aesthetics. The nice part is we can easily make this more comlex And ggplot has pretty good defaults to handle things like color choices, legends, etc. ggplot(data=primaryPolls)+ geom_point(mapping = aes(x=start_date, y=pct, color= candidate_name)) Note that ggplot automatically chose a unique color. It can do the same with shapes, point size, and transparency (alpha). alpha = 1 is completely opaque, alpha = 0.2 would be almost entirely see-through You can also set aesthetic characteristics manually (as I do with alpha here) Note that shapes only works by default with 6 categories ggplot(data=primaryPolls)+ geom_point(mapping = aes(x=start_date, y=pct, shape= candidate_name, color=candidate_name), alpha=.8) 6.1.3 Facets Perhaps most helpfully we can parse the data by features using a simple line ggplot will then arrange everything else so it looks pretty OK ggplot(data=primaryPolls)+ geom_point(mapping = aes(x=start_date, y=pct)) + facet_wrap(~ candidate_name, nrow=4) You can also make a grid of plots, one dimension has each population level and the other has the candidate names ggplot(data=primaryPolls)+ geom_point(mapping = aes(x=start_date, y=pct)) + facet_wrap(population ~ candidate_name, nrow=2) 6.2 More geom options ggplot(data=primaryPolls)+ geom_smooth(mapping = aes(x=start_date, y=pct, color=candidate_name)) + facet_wrap(~ candidate_name, nrow=2) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplot(data=primaryPolls)+ geom_smooth(mapping = aes(x=start_date, y=pct, color=candidate_name)) + geom_point(mapping = aes(x=start_date, y=pct, color=candidate_name), alpha=.4) + facet_wrap(~ candidate_name, nrow=2) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplot(data=primaryPolls)+ geom_smooth(mapping = aes(x=start_date, y=pct, color=candidate_name, linetype=candidate_name)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ANd maybe we don’t want a legend? ggplot(data=primaryPolls)+ geom_smooth(mapping = aes(x=start_date, y=pct, group=candidate_name)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggplot(data=primaryPolls)+ geom_smooth(mapping = aes(x=start_date, y=pct, color=candidate_name), show.legend=FALSE) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; We can combine multiple geom objects ggplot(data=primaryPolls)+ geom_smooth(mapping = aes(x=start_date, y=pct, color=candidate_name))+ geom_point(mapping = aes(x=start_date, y=pct, color=candidate_name), alpha=.4) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; But we can also do this in a way easier to change (less copying and pasting) ggplot(data=primaryPolls, mapping=aes(x=start_date, y=pct, color=candidate_name))+ geom_smooth()+ geom_point(alpha=.4) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; And you can add specifics elments to a sub-layer ggplot(data=primaryPolls, mapping=aes(x=start_date, y=pct, color=candidate_name))+ geom_smooth()+ geom_point(aes(size=sample_size), alpha=.4) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; A lot of other options you are interested require knowing the right geom function ggplot(data=primaryPolls, mapping=aes(x=fte_grade))+ geom_bar() This is also the same as counting the number of obserervations in each bin ggplot(data=primaryPolls, mapping=aes(x=fte_grade))+ stat_count() This is also the same as counting the number of obserervations in each bin ggplot(data=primaryPolls, mapping=aes(x=candidate_name, y=pct))+ stat_summary( fun.ymin=min, fun.ymax=max, fun.y=median ) ## Warning: `fun.y` is deprecated. Use `fun` instead. ## Warning: `fun.ymin` is deprecated. Use `fun.min` instead. ## Warning: `fun.ymax` is deprecated. Use `fun.max` instead. You can also flip the coordinate system fairly quickly ggplot(data=primaryPolls, mapping=aes(x=candidate_name, y=pct))+ geom_boxplot() ggplot(data=primaryPolls, mapping=aes(x=candidate_name, y=pct))+ geom_boxplot() + coord_flip() "],
["section-dplyr-and-tidy.html", "7 dplyr and tidy 7.1 Into the tidyverse 7.2 Back to dplyr 7.3 Relational data 7.4 stringr 7.5 Pipes and maps 7.6 map 7.7 walk, purrr, and more", " 7 dplyr and tidy 7.1 Into the tidyverse dplyr is a handy package for changing data tidyr is a handy package for reshaping data In combination they offer a powerful way to quickly extract insight from your data 7.1.1 Our example Data comes from fivethirtyeight.com library(dplyr) library(tidyr) library(readr) primaryPolls&lt;-read_csv(&#39;https://jmontgomery.github.io/PDS/Datasets/president_primary_polls_feb2020.csv&#39;) primaryPolls$start_date&lt;-as.Date(primaryPolls$start_date, &quot;%m/%d/%y&quot;) class(primaryPolls) ## [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; A tibble is basically a data frame They are both friendlier and more structured than traditional data frames primaryPolls ## # A tibble: 16,661 x 33 ## question_id poll_id cycle state pollster_id pollster sponsor_ids sponsors ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 116805 63512 2020 New ~ 1515 Data fo~ NA &lt;NA&gt; ## 2 116805 63512 2020 New ~ 1515 Data fo~ NA &lt;NA&gt; ## 3 116805 63512 2020 New ~ 1515 Data fo~ NA &lt;NA&gt; ## 4 116805 63512 2020 New ~ 1515 Data fo~ NA &lt;NA&gt; ## 5 116805 63512 2020 New ~ 1515 Data fo~ NA &lt;NA&gt; ## 6 116805 63512 2020 New ~ 1515 Data fo~ NA &lt;NA&gt; ## 7 116805 63512 2020 New ~ 1515 Data fo~ NA &lt;NA&gt; ## 8 116805 63512 2020 New ~ 1515 Data fo~ NA &lt;NA&gt; ## 9 116799 63511 2020 &lt;NA&gt; 744 Ipsos 71 Reuters ## 10 116799 63511 2020 &lt;NA&gt; 744 Ipsos 71 Reuters ## # ... with 16,651 more rows, and 25 more variables: display_name &lt;chr&gt;, ## # pollster_rating_id &lt;dbl&gt;, pollster_rating_name &lt;chr&gt;, fte_grade &lt;chr&gt;, ## # sample_size &lt;dbl&gt;, population &lt;chr&gt;, population_full &lt;chr&gt;, ## # methodology &lt;chr&gt;, office_type &lt;chr&gt;, start_date &lt;date&gt;, end_date &lt;chr&gt;, ## # sponsor_candidate &lt;lgl&gt;, internal &lt;lgl&gt;, partisan &lt;lgl&gt;, tracking &lt;lgl&gt;, ## # nationwide_batch &lt;lgl&gt;, created_at &lt;chr&gt;, notes &lt;chr&gt;, url &lt;chr&gt;, ## # stage &lt;chr&gt;, party &lt;chr&gt;, answer &lt;chr&gt;, candidate_id &lt;dbl&gt;, ## # candidate_name &lt;chr&gt;, pct &lt;dbl&gt; You can also print the first 10 rows of all the columns print(primaryPolls, width=Inf) ## # A tibble: 16,661 x 33 ## question_id poll_id cycle state pollster_id pollster ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 116805 63512 2020 New Hampshire 1515 Data for Progress ## 2 116805 63512 2020 New Hampshire 1515 Data for Progress ## 3 116805 63512 2020 New Hampshire 1515 Data for Progress ## 4 116805 63512 2020 New Hampshire 1515 Data for Progress ## 5 116805 63512 2020 New Hampshire 1515 Data for Progress ## 6 116805 63512 2020 New Hampshire 1515 Data for Progress ## 7 116805 63512 2020 New Hampshire 1515 Data for Progress ## 8 116805 63512 2020 New Hampshire 1515 Data for Progress ## 9 116799 63511 2020 &lt;NA&gt; 744 Ipsos ## 10 116799 63511 2020 &lt;NA&gt; 744 Ipsos ## sponsor_ids sponsors display_name pollster_rating_id ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 NA &lt;NA&gt; Data for Progress 522 ## 2 NA &lt;NA&gt; Data for Progress 522 ## 3 NA &lt;NA&gt; Data for Progress 522 ## 4 NA &lt;NA&gt; Data for Progress 522 ## 5 NA &lt;NA&gt; Data for Progress 522 ## 6 NA &lt;NA&gt; Data for Progress 522 ## 7 NA &lt;NA&gt; Data for Progress 522 ## 8 NA &lt;NA&gt; Data for Progress 522 ## 9 71 Reuters Ipsos 154 ## 10 71 Reuters Ipsos 154 ## pollster_rating_name fte_grade sample_size population population_full ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Data for Progress B/C 1295 lv lv-d ## 2 Data for Progress B/C 1295 lv lv-d ## 3 Data for Progress B/C 1295 lv lv-d ## 4 Data for Progress B/C 1295 lv lv-d ## 5 Data for Progress B/C 1295 lv lv-d ## 6 Data for Progress B/C 1295 lv lv-d ## 7 Data for Progress B/C 1295 lv lv-d ## 8 Data for Progress B/C 1295 lv lv-d ## 9 Ipsos B- 556 rv rv-d ## 10 Ipsos B- 556 rv rv-d ## methodology office_type start_date end_date sponsor_candidate internal ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 Text U.S. President 2020-02-07 2/10/20 NA FALSE ## 2 Text U.S. President 2020-02-07 2/10/20 NA FALSE ## 3 Text U.S. President 2020-02-07 2/10/20 NA FALSE ## 4 Text U.S. President 2020-02-07 2/10/20 NA FALSE ## 5 Text U.S. President 2020-02-07 2/10/20 NA FALSE ## 6 Text U.S. President 2020-02-07 2/10/20 NA FALSE ## 7 Text U.S. President 2020-02-07 2/10/20 NA FALSE ## 8 Text U.S. President 2020-02-07 2/10/20 NA FALSE ## 9 Online U.S. President 2020-02-06 2/10/20 NA FALSE ## 10 Online U.S. President 2020-02-06 2/10/20 NA FALSE ## partisan tracking nationwide_batch created_at notes ## &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 NA FALSE FALSE 2/10/20 21:47 &lt;NA&gt; ## 2 NA FALSE FALSE 2/10/20 21:47 &lt;NA&gt; ## 3 NA FALSE FALSE 2/10/20 21:47 &lt;NA&gt; ## 4 NA FALSE FALSE 2/10/20 21:47 &lt;NA&gt; ## 5 NA FALSE FALSE 2/10/20 21:47 &lt;NA&gt; ## 6 NA FALSE FALSE 2/10/20 21:47 &lt;NA&gt; ## 7 NA FALSE FALSE 2/10/20 21:47 &lt;NA&gt; ## 8 NA FALSE FALSE 2/10/20 21:47 &lt;NA&gt; ## 9 NA FALSE FALSE 2/10/20 18:27 &lt;NA&gt; ## 10 NA FALSE FALSE 2/10/20 18:27 &lt;NA&gt; ## url ## &lt;chr&gt; ## 1 http://filesforprogress.org/datasets/2020/2/nh/new_hampshire_primary.pdf ## 2 http://filesforprogress.org/datasets/2020/2/nh/new_hampshire_primary.pdf ## 3 http://filesforprogress.org/datasets/2020/2/nh/new_hampshire_primary.pdf ## 4 http://filesforprogress.org/datasets/2020/2/nh/new_hampshire_primary.pdf ## 5 http://filesforprogress.org/datasets/2020/2/nh/new_hampshire_primary.pdf ## 6 http://filesforprogress.org/datasets/2020/2/nh/new_hampshire_primary.pdf ## 7 http://filesforprogress.org/datasets/2020/2/nh/new_hampshire_primary.pdf ## 8 http://filesforprogress.org/datasets/2020/2/nh/new_hampshire_primary.pdf ## 9 https://www.ipsos.com/sites/default/files/ct/news/documents/2020-02/topline_~ ## 10 https://www.ipsos.com/sites/default/files/ct/news/documents/2020-02/topline_~ ## stage party answer candidate_id candidate_name pct ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 primary DEM Sanders 13257 Bernard Sanders 28 ## 2 primary DEM Yang 13329 Andrew Yang 5 ## 3 primary DEM Buttigieg 13345 Pete Buttigieg 26 ## 4 primary DEM Biden 13256 Joseph R. Biden Jr. 9 ## 5 primary DEM Gabbard 13343 Tulsi Gabbard 3 ## 6 primary DEM Klobuchar 13310 Amy Klobuchar 13 ## 7 primary DEM Warren 13258 Elizabeth Warren 14 ## 8 primary DEM Steyer 13327 Tom Steyer 3 ## 9 primary DEM Sanders 13257 Bernard Sanders 20 ## 10 primary DEM Biden 13256 Joseph R. Biden Jr. 17 ## # ... with 16,651 more rows You can access elements basically the same way They never allow partial matching (which is evil anyways) Partial matching allows you to type the first few characters of a column name and it will figure out which one you meant You can use as.tibble or as.data.frame to jump back and forth 7.2 Back to dplyr Subset data by rows: filter Reorder by rows: arrange Subset data by column: select Create new variables as a function of other variables: mutate Collapse values down (or extract statistic): summarise We can use group_by to make changes in the scope 7.2.1 filter filter allows you to easily subset data using conditions as we have done before using accessors filter(primaryPolls, candidate_name == c(&quot;Amy Klobuchar&quot;)) ## # A tibble: 874 x 33 ## question_id poll_id cycle state pollster_id pollster sponsor_ids sponsors ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 116805 63512 2020 New ~ 1515 Data fo~ NA &lt;NA&gt; ## 2 116799 63511 2020 &lt;NA&gt; 744 Ipsos 71 Reuters ## 3 116732 63489 2020 New ~ 1102 Emerson~ 43 7News ## 4 116733 63490 2020 New ~ 458 Suffolk~ 68323 Boston ~ ## 5 116743 63493 2020 New ~ 23 America~ NA &lt;NA&gt; ## 6 116798 63510 2020 New ~ 1529 Elucd NA &lt;NA&gt; ## 7 116745 63494 2020 New ~ 1470 Univers~ 143 CNN ## 8 116792 63508 2020 Cali~ 1491 Capitol~ NA &lt;NA&gt; ## 9 116763 63496 2020 &lt;NA&gt; 396 Quinnip~ NA &lt;NA&gt; ## 10 116714 63483 2020 New ~ 1102 Emerson~ 43 7News ## # ... with 864 more rows, and 25 more variables: display_name &lt;chr&gt;, ## # pollster_rating_id &lt;dbl&gt;, pollster_rating_name &lt;chr&gt;, fte_grade &lt;chr&gt;, ## # sample_size &lt;dbl&gt;, population &lt;chr&gt;, population_full &lt;chr&gt;, ## # methodology &lt;chr&gt;, office_type &lt;chr&gt;, start_date &lt;date&gt;, end_date &lt;chr&gt;, ## # sponsor_candidate &lt;lgl&gt;, internal &lt;lgl&gt;, partisan &lt;lgl&gt;, tracking &lt;lgl&gt;, ## # nationwide_batch &lt;lgl&gt;, created_at &lt;chr&gt;, notes &lt;chr&gt;, url &lt;chr&gt;, ## # stage &lt;chr&gt;, party &lt;chr&gt;, answer &lt;chr&gt;, candidate_id &lt;dbl&gt;, ## # candidate_name &lt;chr&gt;, pct &lt;dbl&gt; Or you can include multiple conditions filter(primaryPolls, candidate_name == c(&quot;Amy Klobuchar&quot;), state==&quot;New Hampshire&quot;) ## # A tibble: 84 x 33 ## question_id poll_id cycle state pollster_id pollster sponsor_ids sponsors ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 116805 63512 2020 New ~ 1515 Data fo~ NA &lt;NA&gt; ## 2 116732 63489 2020 New ~ 1102 Emerson~ 43 7News ## 3 116733 63490 2020 New ~ 458 Suffolk~ 68323 Boston ~ ## 4 116743 63493 2020 New ~ 23 America~ NA &lt;NA&gt; ## 5 116798 63510 2020 New ~ 1529 Elucd NA &lt;NA&gt; ## 6 116745 63494 2020 New ~ 1470 Univers~ 143 CNN ## 7 116714 63483 2020 New ~ 1102 Emerson~ 43 7News ## 8 116715 63484 2020 New ~ 458 Suffolk~ 68323 Boston ~ ## 9 116717 63485 2020 New ~ 1470 Univers~ 143 CNN ## 10 116718 63486 2020 New ~ 568 YouGov 133 CBS News ## # ... with 74 more rows, and 25 more variables: display_name &lt;chr&gt;, ## # pollster_rating_id &lt;dbl&gt;, pollster_rating_name &lt;chr&gt;, fte_grade &lt;chr&gt;, ## # sample_size &lt;dbl&gt;, population &lt;chr&gt;, population_full &lt;chr&gt;, ## # methodology &lt;chr&gt;, office_type &lt;chr&gt;, start_date &lt;date&gt;, end_date &lt;chr&gt;, ## # sponsor_candidate &lt;lgl&gt;, internal &lt;lgl&gt;, partisan &lt;lgl&gt;, tracking &lt;lgl&gt;, ## # nationwide_batch &lt;lgl&gt;, created_at &lt;chr&gt;, notes &lt;chr&gt;, url &lt;chr&gt;, ## # stage &lt;chr&gt;, party &lt;chr&gt;, answer &lt;chr&gt;, candidate_id &lt;dbl&gt;, ## # candidate_name &lt;chr&gt;, pct &lt;dbl&gt; Everything you have already learned about boolean operators applies here. 7.2.2 arrange arrange does the same thing, but organizes data by rows insead of subsetting by rows. arrange(primaryPolls, state, pollster_id) ## # A tibble: 16,661 x 33 ## question_id poll_id cycle state pollster_id pollster sponsor_ids sponsors ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 99478 58691 2020 Alab~ 1193 SurveyM~ 132 NBC News ## 2 99478 58691 2020 Alab~ 1193 SurveyM~ 132 NBC News ## 3 99478 58691 2020 Alab~ 1193 SurveyM~ 132 NBC News ## 4 99478 58691 2020 Alab~ 1193 SurveyM~ 132 NBC News ## 5 99478 58691 2020 Alab~ 1193 SurveyM~ 132 NBC News ## 6 99478 58691 2020 Alab~ 1193 SurveyM~ 132 NBC News ## 7 99478 58691 2020 Alab~ 1193 SurveyM~ 132 NBC News ## 8 99478 58691 2020 Alab~ 1193 SurveyM~ 132 NBC News ## 9 99478 58691 2020 Alab~ 1193 SurveyM~ 132 NBC News ## 10 99478 58691 2020 Alab~ 1193 SurveyM~ 132 NBC News ## # ... with 16,651 more rows, and 25 more variables: display_name &lt;chr&gt;, ## # pollster_rating_id &lt;dbl&gt;, pollster_rating_name &lt;chr&gt;, fte_grade &lt;chr&gt;, ## # sample_size &lt;dbl&gt;, population &lt;chr&gt;, population_full &lt;chr&gt;, ## # methodology &lt;chr&gt;, office_type &lt;chr&gt;, start_date &lt;date&gt;, end_date &lt;chr&gt;, ## # sponsor_candidate &lt;lgl&gt;, internal &lt;lgl&gt;, partisan &lt;lgl&gt;, tracking &lt;lgl&gt;, ## # nationwide_batch &lt;lgl&gt;, created_at &lt;chr&gt;, notes &lt;chr&gt;, url &lt;chr&gt;, ## # stage &lt;chr&gt;, party &lt;chr&gt;, answer &lt;chr&gt;, candidate_id &lt;dbl&gt;, ## # candidate_name &lt;chr&gt;, pct &lt;dbl&gt; Or in descending order arrange(primaryPolls, state, desc(pollster_id)) ## # A tibble: 16,661 x 33 ## question_id poll_id cycle state pollster_id pollster sponsor_ids sponsors ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 93812 57768 2020 Alab~ 1365 Change ~ NA &lt;NA&gt; ## 2 93812 57768 2020 Alab~ 1365 Change ~ NA &lt;NA&gt; ## 3 93812 57768 2020 Alab~ 1365 Change ~ NA &lt;NA&gt; ## 4 93812 57768 2020 Alab~ 1365 Change ~ NA &lt;NA&gt; ## 5 93812 57768 2020 Alab~ 1365 Change ~ NA &lt;NA&gt; ## 6 93812 57768 2020 Alab~ 1365 Change ~ NA &lt;NA&gt; ## 7 93812 57768 2020 Alab~ 1365 Change ~ NA &lt;NA&gt; ## 8 93812 57768 2020 Alab~ 1365 Change ~ NA &lt;NA&gt; ## 9 93812 57768 2020 Alab~ 1365 Change ~ NA &lt;NA&gt; ## 10 93812 57768 2020 Alab~ 1365 Change ~ NA &lt;NA&gt; ## # ... with 16,651 more rows, and 25 more variables: display_name &lt;chr&gt;, ## # pollster_rating_id &lt;dbl&gt;, pollster_rating_name &lt;chr&gt;, fte_grade &lt;chr&gt;, ## # sample_size &lt;dbl&gt;, population &lt;chr&gt;, population_full &lt;chr&gt;, ## # methodology &lt;chr&gt;, office_type &lt;chr&gt;, start_date &lt;date&gt;, end_date &lt;chr&gt;, ## # sponsor_candidate &lt;lgl&gt;, internal &lt;lgl&gt;, partisan &lt;lgl&gt;, tracking &lt;lgl&gt;, ## # nationwide_batch &lt;lgl&gt;, created_at &lt;chr&gt;, notes &lt;chr&gt;, url &lt;chr&gt;, ## # stage &lt;chr&gt;, party &lt;chr&gt;, answer &lt;chr&gt;, candidate_id &lt;dbl&gt;, ## # candidate_name &lt;chr&gt;, pct &lt;dbl&gt; 7.2.3 select Select is just a much easier way to subset by column select(primaryPolls, state, candidate_name, start_date) ## # A tibble: 16,661 x 3 ## state candidate_name start_date ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 New Hampshire Bernard Sanders 2020-02-07 ## 2 New Hampshire Andrew Yang 2020-02-07 ## 3 New Hampshire Pete Buttigieg 2020-02-07 ## 4 New Hampshire Joseph R. Biden Jr. 2020-02-07 ## 5 New Hampshire Tulsi Gabbard 2020-02-07 ## 6 New Hampshire Amy Klobuchar 2020-02-07 ## 7 New Hampshire Elizabeth Warren 2020-02-07 ## 8 New Hampshire Tom Steyer 2020-02-07 ## 9 &lt;NA&gt; Bernard Sanders 2020-02-06 ## 10 &lt;NA&gt; Joseph R. Biden Jr. 2020-02-06 ## # ... with 16,651 more rows It comes with a nice syntax for doing this without just listing var1:var20 will select all columns between these two -var1:var20 will select everything except that range starts_with(\"cand\") will select columns that start with “cand” Similar functionality for ends_with, contains, and matches A fun trick to move your favorite variables to the front, but keep it all: select(primaryPolls, state, candidate_name, start_date, everything()) ## # A tibble: 16,661 x 33 ## state candidate_name start_date question_id poll_id cycle pollster_id ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 New ~ Bernard Sande~ 2020-02-07 116805 63512 2020 1515 ## 2 New ~ Andrew Yang 2020-02-07 116805 63512 2020 1515 ## 3 New ~ Pete Buttigieg 2020-02-07 116805 63512 2020 1515 ## 4 New ~ Joseph R. Bid~ 2020-02-07 116805 63512 2020 1515 ## 5 New ~ Tulsi Gabbard 2020-02-07 116805 63512 2020 1515 ## 6 New ~ Amy Klobuchar 2020-02-07 116805 63512 2020 1515 ## 7 New ~ Elizabeth War~ 2020-02-07 116805 63512 2020 1515 ## 8 New ~ Tom Steyer 2020-02-07 116805 63512 2020 1515 ## 9 &lt;NA&gt; Bernard Sande~ 2020-02-06 116799 63511 2020 744 ## 10 &lt;NA&gt; Joseph R. Bid~ 2020-02-06 116799 63511 2020 744 ## # ... with 16,651 more rows, and 26 more variables: pollster &lt;chr&gt;, ## # sponsor_ids &lt;dbl&gt;, sponsors &lt;chr&gt;, display_name &lt;chr&gt;, ## # pollster_rating_id &lt;dbl&gt;, pollster_rating_name &lt;chr&gt;, fte_grade &lt;chr&gt;, ## # sample_size &lt;dbl&gt;, population &lt;chr&gt;, population_full &lt;chr&gt;, ## # methodology &lt;chr&gt;, office_type &lt;chr&gt;, end_date &lt;chr&gt;, ## # sponsor_candidate &lt;lgl&gt;, internal &lt;lgl&gt;, partisan &lt;lgl&gt;, tracking &lt;lgl&gt;, ## # nationwide_batch &lt;lgl&gt;, created_at &lt;chr&gt;, notes &lt;chr&gt;, url &lt;chr&gt;, ## # stage &lt;chr&gt;, party &lt;chr&gt;, answer &lt;chr&gt;, candidate_id &lt;dbl&gt;, pct &lt;dbl&gt; Note here that rename is the mythical easy way to rename a column, although syntax seems backwards You have to specify the new column name on the left side of the equal sign and the old column name on the right basicPolls&lt;-select(primaryPolls, state, candidate_name, start_date, pct) rename(basicPolls, candidate = candidate_name) ## # A tibble: 16,661 x 4 ## state candidate start_date pct ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; ## 1 New Hampshire Bernard Sanders 2020-02-07 28 ## 2 New Hampshire Andrew Yang 2020-02-07 5 ## 3 New Hampshire Pete Buttigieg 2020-02-07 26 ## 4 New Hampshire Joseph R. Biden Jr. 2020-02-07 9 ## 5 New Hampshire Tulsi Gabbard 2020-02-07 3 ## 6 New Hampshire Amy Klobuchar 2020-02-07 13 ## 7 New Hampshire Elizabeth Warren 2020-02-07 14 ## 8 New Hampshire Tom Steyer 2020-02-07 3 ## 9 &lt;NA&gt; Bernard Sanders 2020-02-06 20 ## 10 &lt;NA&gt; Joseph R. Biden Jr. 2020-02-06 17 ## # ... with 16,651 more rows 7.2.4 mutate mutate allows us to create a new variable that is a function of the others If you want to add the new column(s) to your existing tibble, you must use the assignment operator. Otherwise it will just print it out and throw the results away mutate(basicPolls, proportion=pct/100) ## # A tibble: 16,661 x 5 ## state candidate_name start_date pct proportion ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 New Hampshire Bernard Sanders 2020-02-07 28 0.28 ## 2 New Hampshire Andrew Yang 2020-02-07 5 0.05 ## 3 New Hampshire Pete Buttigieg 2020-02-07 26 0.26 ## 4 New Hampshire Joseph R. Biden Jr. 2020-02-07 9 0.09 ## 5 New Hampshire Tulsi Gabbard 2020-02-07 3 0.03 ## 6 New Hampshire Amy Klobuchar 2020-02-07 13 0.13 ## 7 New Hampshire Elizabeth Warren 2020-02-07 14 0.14 ## 8 New Hampshire Tom Steyer 2020-02-07 3 0.03 ## 9 &lt;NA&gt; Bernard Sanders 2020-02-06 20 0.2 ## 10 &lt;NA&gt; Joseph R. Biden Jr. 2020-02-06 17 0.17 ## # ... with 16,651 more rows basicPolls &lt;- mutate(basicPolls, proportion=pct/100) transmute creates and returns a new tibble with only the mutated variable(s) transmute(basicPolls, proportion=pct/100) ## # A tibble: 16,661 x 1 ## proportion ## &lt;dbl&gt; ## 1 0.28 ## 2 0.05 ## 3 0.26 ## 4 0.09 ## 5 0.03 ## 6 0.13 ## 7 0.14 ## 8 0.03 ## 9 0.2 ## 10 0.17 ## # ... with 16,651 more rows transmute(primaryPolls, numberRespondents=round((pct/100)*sample_size)) ## # A tibble: 16,661 x 1 ## numberRespondents ## &lt;dbl&gt; ## 1 363 ## 2 65 ## 3 337 ## 4 117 ## 5 39 ## 6 168 ## 7 181 ## 8 39 ## 9 111 ## 10 95 ## # ... with 16,651 more rows transmute(primaryPolls, proportion=pct/100, numberRespondents=round(proportion*sample_size), ) ## # A tibble: 16,661 x 2 ## proportion numberRespondents ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.28 363 ## 2 0.05 65 ## 3 0.26 337 ## 4 0.09 117 ## 5 0.03 39 ## 6 0.13 168 ## 7 0.14 181 ## 8 0.03 39 ## 9 0.2 111 ## 10 0.17 95 ## # ... with 16,651 more rows Note that you can use a ton of the basic functions we have already covered like sum, mean, sqrt, etc. A useful one is n(), which just counts the number of observations This includes logical comparisons mutate(basicPolls, top_tier=(pct&gt;10)*1) ## # A tibble: 16,661 x 6 ## state candidate_name start_date pct proportion top_tier ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 New Hampshire Bernard Sanders 2020-02-07 28 0.28 1 ## 2 New Hampshire Andrew Yang 2020-02-07 5 0.05 0 ## 3 New Hampshire Pete Buttigieg 2020-02-07 26 0.26 1 ## 4 New Hampshire Joseph R. Biden Jr. 2020-02-07 9 0.09 0 ## 5 New Hampshire Tulsi Gabbard 2020-02-07 3 0.03 0 ## 6 New Hampshire Amy Klobuchar 2020-02-07 13 0.13 1 ## 7 New Hampshire Elizabeth Warren 2020-02-07 14 0.14 1 ## 8 New Hampshire Tom Steyer 2020-02-07 3 0.03 0 ## 9 &lt;NA&gt; Bernard Sanders 2020-02-06 20 0.2 1 ## 10 &lt;NA&gt; Joseph R. Biden Jr. 2020-02-06 17 0.17 1 ## # ... with 16,651 more rows 7.2.5 summarise We can easily extract summary statistics summarise(basicPolls, average_candidate=mean(pct), count=n()) ## # A tibble: 1 x 2 ## average_candidate count ## &lt;dbl&gt; &lt;int&gt; ## 1 6.34 16661 This is more powerful when also using the group_by function basicPolls_grouped&lt;-group_by(basicPolls, candidate_name) summarise(basicPolls_grouped, average_candidate=mean(pct), count=n()) ## # A tibble: 76 x 3 ## candidate_name average_candidate count ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Amy Klobuchar 2.32 874 ## 2 Andrew Cuomo 0.433 12 ## 3 Andrew Yang 2.34 831 ## 4 Barack Obama 0 2 ## 5 Ben Sasse 0.711 36 ## 6 Bernard Sanders 18.3 960 ## 7 Beto O&#39;Rourke 3.76 653 ## 8 Bill de Blasio 0.534 318 ## 9 Bob Corker 0.611 36 ## 10 Charles D. Baker 31.5 2 ## # ... with 66 more rows basicPolls_grouped&lt;-group_by(basicPolls, candidate_name, state) summarise(basicPolls_grouped, average_candidate=mean(pct), count=n()) ## # A tibble: 1,024 x 4 ## # Groups: candidate_name [76] ## candidate_name state average_candidate count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Amy Klobuchar Alabama 1 3 ## 2 Amy Klobuchar Arizona 1.18 6 ## 3 Amy Klobuchar California 1.74 40 ## 4 Amy Klobuchar Colorado 0.1 1 ## 5 Amy Klobuchar Delaware 1 1 ## 6 Amy Klobuchar Florida 1.98 13 ## 7 Amy Klobuchar Georgia 1.27 4 ## 8 Amy Klobuchar Illinois 2.15 2 ## 9 Amy Klobuchar Indiana 0 1 ## 10 Amy Klobuchar Iowa 5.97 61 ## # ... with 1,014 more rows 7.2.6 Piping The tidyverse includes a nice syntax for combining multiple commands so we don’t have to create new objects all of the time. The %?% syntax allows us to pass on the results of one line to another The result from the left side of the %&gt;% is passed as the first parameter of the next function basicPolls %&gt;% group_by(candidate_name, state) %&gt;% summarise(average_candidate=mean(pct), count=n()) ## # A tibble: 1,024 x 4 ## # Groups: candidate_name [76] ## candidate_name state average_candidate count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Amy Klobuchar Alabama 1 3 ## 2 Amy Klobuchar Arizona 1.18 6 ## 3 Amy Klobuchar California 1.74 40 ## 4 Amy Klobuchar Colorado 0.1 1 ## 5 Amy Klobuchar Delaware 1 1 ## 6 Amy Klobuchar Florida 1.98 13 ## 7 Amy Klobuchar Georgia 1.27 4 ## 8 Amy Klobuchar Illinois 2.15 2 ## 9 Amy Klobuchar Indiana 0 1 ## 10 Amy Klobuchar Iowa 5.97 61 ## # ... with 1,014 more rows The above code is equivalent to the following: summarise(group_by(basicPolls, candidate_name, state), average_candidate=mean(pct), count=n()) ## # A tibble: 1,024 x 4 ## # Groups: candidate_name [76] ## candidate_name state average_candidate count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Amy Klobuchar Alabama 1 3 ## 2 Amy Klobuchar Arizona 1.18 6 ## 3 Amy Klobuchar California 1.74 40 ## 4 Amy Klobuchar Colorado 0.1 1 ## 5 Amy Klobuchar Delaware 1 1 ## 6 Amy Klobuchar Florida 1.98 13 ## 7 Amy Klobuchar Georgia 1.27 4 ## 8 Amy Klobuchar Illinois 2.15 2 ## 9 Amy Klobuchar Indiana 0 1 ## 10 Amy Klobuchar Iowa 5.97 61 ## # ... with 1,014 more rows Which is equivalent to this: grouped_by &lt;- group_by(basicPolls, candidate_name, state) summarise(grouped_by, average_candidate=mean(pct), count=n()) ## # A tibble: 1,024 x 4 ## # Groups: candidate_name [76] ## candidate_name state average_candidate count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Amy Klobuchar Alabama 1 3 ## 2 Amy Klobuchar Arizona 1.18 6 ## 3 Amy Klobuchar California 1.74 40 ## 4 Amy Klobuchar Colorado 0.1 1 ## 5 Amy Klobuchar Delaware 1 1 ## 6 Amy Klobuchar Florida 1.98 13 ## 7 Amy Klobuchar Georgia 1.27 4 ## 8 Amy Klobuchar Illinois 2.15 2 ## 9 Amy Klobuchar Indiana 0 1 ## 10 Amy Klobuchar Iowa 5.97 61 ## # ... with 1,014 more rows See assigned readings for more useful summary variables and ungroup basicPolls %&gt;% group_by(candidate_name, state) %&gt;% summarise(average_candidate=mean(pct), count=n()) %&gt;% filter(count&gt;10) ## # A tibble: 206 x 4 ## # Groups: candidate_name [44] ## candidate_name state average_candidate count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Amy Klobuchar California 1.74 40 ## 2 Amy Klobuchar Florida 1.98 13 ## 3 Amy Klobuchar Iowa 5.97 61 ## 4 Amy Klobuchar Nevada 1.71 15 ## 5 Amy Klobuchar New Hampshire 5.42 84 ## 6 Amy Klobuchar Pennsylvania 1.53 11 ## 7 Amy Klobuchar South Carolina 1.27 34 ## 8 Amy Klobuchar Texas 1.47 23 ## 9 Amy Klobuchar Wisconsin 2.52 16 ## 10 Amy Klobuchar &lt;NA&gt; 1.62 503 ## # ... with 196 more rows primaryPolls %&gt;% group_by(candidate_name, state) %&gt;% summarise(average_candidate=mean(pct), count=n()) %&gt;% filter(count&gt;10) %&gt;% mutate(average_prop=average_candidate/100) %&gt;% select(average_prop, candidate_name, state, count) ## # A tibble: 206 x 4 ## # Groups: candidate_name [44] ## average_prop candidate_name state count ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 0.0174 Amy Klobuchar California 40 ## 2 0.0198 Amy Klobuchar Florida 13 ## 3 0.0597 Amy Klobuchar Iowa 61 ## 4 0.0171 Amy Klobuchar Nevada 15 ## 5 0.0542 Amy Klobuchar New Hampshire 84 ## 6 0.0153 Amy Klobuchar Pennsylvania 11 ## 7 0.0127 Amy Klobuchar South Carolina 34 ## 8 0.0147 Amy Klobuchar Texas 23 ## 9 0.0252 Amy Klobuchar Wisconsin 16 ## 10 0.0162 Amy Klobuchar &lt;NA&gt; 503 ## # ... with 196 more rows 7.2.7 Pivots In many cases the data is not quite organized the way we want. Right now we have each poll as a separate row. But what if we want each candidate to be a row so we can analyze their trends in polls over time? Or what if we get the trend line and want to instead reorganize to look at each poll separately? The key commands here are pivot_wider and pivot_longer See Chapter 12 for some additional (but less universally useful functions) In our running example we have one entry for each poll. How could we combine these to list the result from each unique poll togehter? The key here is that the values of interest are in the pct column and the groupings of interest are in the candidates column nevadaPrimaries&lt;-primaryPolls %&gt;% filter(candidate_name %in% c(&quot;Amy Klobuchar&quot;, &quot;Bernard Sanders&quot;, &quot;Elizabeth Warren&quot;, &quot;Joseph R. Biden Jr.&quot;, &quot;Michael Bloomberg&quot;, &quot;Pete Buttigieg&quot;)) %&gt;% filter(state==&quot;Nevada&quot;) %&gt;% select(candidate_name, pct, start_date, sample_size) print(nevadaPrimaries, n=Inf) ## # A tibble: 76 x 4 ## candidate_name pct start_date sample_size ## &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; ## 1 Joseph R. Biden Jr. 19.4 2020-01-08 500 ## 2 Bernard Sanders 17.6 2020-01-08 500 ## 3 Elizabeth Warren 10.6 2020-01-08 500 ## 4 Pete Buttigieg 8.2 2020-01-08 500 ## 5 Amy Klobuchar 3.6 2020-01-08 500 ## 6 Elizabeth Warren 14 2020-01-06 600 ## 7 Bernard Sanders 29 2020-01-06 600 ## 8 Joseph R. Biden Jr. 28 2020-01-06 600 ## 9 Pete Buttigieg 6 2020-01-06 600 ## 10 Amy Klobuchar 4 2020-01-06 600 ## 11 Joseph R. Biden Jr. 23 2020-01-05 635 ## 12 Bernard Sanders 17 2020-01-05 635 ## 13 Elizabeth Warren 12 2020-01-05 635 ## 14 Pete Buttigieg 6 2020-01-05 635 ## 15 Michael Bloomberg 2 2020-01-05 635 ## 16 Amy Klobuchar 2 2020-01-05 635 ## 17 Joseph R. Biden Jr. 24 2019-11-10 627 ## 18 Bernard Sanders 18 2019-11-10 627 ## 19 Elizabeth Warren 18 2019-11-10 627 ## 20 Pete Buttigieg 8 2019-11-10 627 ## 21 Amy Klobuchar 2 2019-11-10 627 ## 22 Joseph R. Biden Jr. 33 2019-11-06 626 ## 23 Bernard Sanders 23 2019-11-06 626 ## 24 Elizabeth Warren 21 2019-11-06 626 ## 25 Pete Buttigieg 9 2019-11-06 626 ## 26 Amy Klobuchar 2 2019-11-06 626 ## 27 Joseph R. Biden Jr. 29.9 2019-10-31 451 ## 28 Bernard Sanders 18.8 2019-10-31 451 ## 29 Elizabeth Warren 22.2 2019-10-31 451 ## 30 Pete Buttigieg 4.9 2019-10-31 451 ## 31 Amy Klobuchar 0.7 2019-10-31 451 ## 32 Joseph R. Biden Jr. 29.1 2019-10-28 600 ## 33 Pete Buttigieg 7.3 2019-10-28 600 ## 34 Amy Klobuchar 2.5 2019-10-28 600 ## 35 Bernard Sanders 19.1 2019-10-28 600 ## 36 Elizabeth Warren 19.2 2019-10-28 600 ## 37 Joseph R. Biden Jr. 22 2019-09-22 324 ## 38 Bernard Sanders 22 2019-09-22 324 ## 39 Elizabeth Warren 18 2019-09-22 324 ## 40 Pete Buttigieg 4 2019-09-22 324 ## 41 Amy Klobuchar 1 2019-09-22 324 ## 42 Joseph R. Biden Jr. 23.2 2019-09-19 500 ## 43 Pete Buttigieg 3.4 2019-09-19 500 ## 44 Amy Klobuchar 0.4 2019-09-19 500 ## 45 Bernard Sanders 14.2 2019-09-19 500 ## 46 Elizabeth Warren 19.4 2019-09-19 500 ## 47 Bernard Sanders 29 2019-08-28 563 ## 48 Joseph R. Biden Jr. 27 2019-08-28 563 ## 49 Elizabeth Warren 18 2019-08-28 563 ## 50 Pete Buttigieg 4 2019-08-28 563 ## 51 Amy Klobuchar 0 2019-08-28 563 ## 52 Joseph R. Biden Jr. 25 2019-08-14 382 ## 53 Elizabeth Warren 15 2019-08-14 382 ## 54 Bernard Sanders 10 2019-08-14 382 ## 55 Pete Buttigieg 5 2019-08-14 382 ## 56 Amy Klobuchar 2 2019-08-14 382 ## 57 Joseph R. Biden Jr. 26 2019-08-02 439 ## 58 Elizabeth Warren 23 2019-08-02 439 ## 59 Bernard Sanders 22 2019-08-02 439 ## 60 Pete Buttigieg 7 2019-08-02 439 ## 61 Amy Klobuchar 1 2019-08-02 439 ## 62 Joseph R. Biden Jr. 36 2019-06-06 370 ## 63 Elizabeth Warren 19 2019-06-06 370 ## 64 Bernard Sanders 13 2019-06-06 370 ## 65 Pete Buttigieg 7 2019-06-06 370 ## 66 Amy Klobuchar 1 2019-06-06 370 ## 67 Joseph R. Biden Jr. 29 2019-05-09 389 ## 68 Bernard Sanders 24 2019-05-09 389 ## 69 Pete Buttigieg 13 2019-05-09 389 ## 70 Elizabeth Warren 12 2019-05-09 389 ## 71 Amy Klobuchar 1 2019-05-09 389 ## 72 Elizabeth Warren 9.9 2019-03-28 310 ## 73 Pete Buttigieg 4.7 2019-03-28 310 ## 74 Joseph R. Biden Jr. 26.4 2019-03-28 310 ## 75 Bernard Sanders 22.5 2019-03-28 310 ## 76 Amy Klobuchar 2.4 2019-03-28 310 setting n=Inf in the print will ensure that all rows are shown (the equivalent for columns is width=Inf) nevadaPrimaries contains one row for every combination of a Nevada poll (specified by start_date and sample_size) and a candidate wideNevada&lt;-pivot_wider(nevadaPrimaries, names_from = candidate_name, values_from = pct) print(wideNevada, width=Inf) ## # A tibble: 15 x 8 ## start_date sample_size `Joseph R. Biden Jr.` `Bernard Sanders` ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2020-01-08 500 19.4 17.6 ## 2 2020-01-06 600 28 29 ## 3 2020-01-05 635 23 17 ## 4 2019-11-10 627 24 18 ## 5 2019-11-06 626 33 23 ## 6 2019-10-31 451 29.9 18.8 ## 7 2019-10-28 600 29.1 19.1 ## 8 2019-09-22 324 22 22 ## 9 2019-09-19 500 23.2 14.2 ## 10 2019-08-28 563 27 29 ## 11 2019-08-14 382 25 10 ## 12 2019-08-02 439 26 22 ## 13 2019-06-06 370 36 13 ## 14 2019-05-09 389 29 24 ## 15 2019-03-28 310 26.4 22.5 ## `Elizabeth Warren` `Pete Buttigieg` `Amy Klobuchar` `Michael Bloomberg` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10.6 8.2 3.6 NA ## 2 14 6 4 NA ## 3 12 6 2 2 ## 4 18 8 2 NA ## 5 21 9 2 NA ## 6 22.2 4.9 0.7 NA ## 7 19.2 7.3 2.5 NA ## 8 18 4 1 NA ## 9 19.4 3.4 0.4 NA ## 10 18 4 0 NA ## 11 15 5 2 NA ## 12 23 7 1 NA ## 13 19 7 1 NA ## 14 12 13 1 NA ## 15 9.9 4.7 2.4 NA wideNevada contains one row for every Nevada poll and a column for the pct each candidate got dim(nevadaPrimaries) ## [1] 76 4 dim(wideNevada) ## [1] 15 8 Or we could organize it into a time series …. timeNevada&lt;-pivot_wider(nevadaPrimaries, id_cols=candidate_name, names_from = c(start_date), values_from = pct) print(timeNevada, width=Inf) ## # A tibble: 6 x 16 ## candidate_name `2020-01-08` `2020-01-06` `2020-01-05` `2019-11-10` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Joseph R. Biden Jr. 19.4 28 23 24 ## 2 Bernard Sanders 17.6 29 17 18 ## 3 Elizabeth Warren 10.6 14 12 18 ## 4 Pete Buttigieg 8.2 6 6 8 ## 5 Amy Klobuchar 3.6 4 2 2 ## 6 Michael Bloomberg NA NA 2 NA ## `2019-11-06` `2019-10-31` `2019-10-28` `2019-09-22` `2019-09-19` `2019-08-28` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 33 29.9 29.1 22 23.2 27 ## 2 23 18.8 19.1 22 14.2 29 ## 3 21 22.2 19.2 18 19.4 18 ## 4 9 4.9 7.3 4 3.4 4 ## 5 2 0.7 2.5 1 0.4 0 ## 6 NA NA NA NA NA NA ## `2019-08-14` `2019-08-02` `2019-06-06` `2019-05-09` `2019-03-28` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 25 26 36 29 26.4 ## 2 10 22 13 24 22.5 ## 3 15 23 19 12 9.9 ## 4 5 7 7 13 4.7 ## 5 2 1 1 1 2.4 ## 6 NA NA NA NA NA Of course sometimes we want to do the reverse using pivot_longer In this case, we no longer have a single indicator variable for the values of start_date timeNevada %&gt;% select(candidate_name, `2020-01-08`, `2020-01-06`) %&gt;% pivot_longer(c(`2020-01-08`, `2020-01-06`), names_to = &quot;start_date_test&quot;, values_to = &quot;pct_test&quot;) ## # A tibble: 12 x 3 ## candidate_name start_date_test pct_test ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Joseph R. Biden Jr. 2020-01-08 19.4 ## 2 Joseph R. Biden Jr. 2020-01-06 28 ## 3 Bernard Sanders 2020-01-08 17.6 ## 4 Bernard Sanders 2020-01-06 29 ## 5 Elizabeth Warren 2020-01-08 10.6 ## 6 Elizabeth Warren 2020-01-06 14 ## 7 Pete Buttigieg 2020-01-08 8.2 ## 8 Pete Buttigieg 2020-01-06 6 ## 9 Amy Klobuchar 2020-01-08 3.6 ## 10 Amy Klobuchar 2020-01-06 4 ## 11 Michael Bloomberg 2020-01-08 NA ## 12 Michael Bloomberg 2020-01-06 NA This will partially undo the pivot_wider command above Now, each row is a combination of one of the six candidates and one of the two start dates Note that to get this, we used names_to and values_to instead of names_from and values_from 7.3 Relational data Most complex analyses involve more than one table of data. Certainly most active databases are not just rectangles. Modern databases are relational, where we know how rows in each rectangle are related to each other. With some slight mind bending, we can learn how to work cleanly with such data. A classic example of relational data is a social media website like Twitter. They would have multiple tables/datasets - one for each user and their user information, one for each tweet and its author, one for each like, the user who did the like, and the tweet that was liked, etc. From this data, we can use relational queries to count the average number of likes Donald Trump gets on his tweets that are tweeted between 1 and 2 AM. The primary relational querying language is SQL (Structured Query Language) but most of the same techniques can be applied in R using tidy. Download and unzip the following: https://github.com/jmontgomery/jmontgomery.github.io/blob/master/PDS/Datasets/Tweets.csv.zip Read these in using the correct file address library(tidyverse) mayors &lt;- read_csv(&quot;../Datasets/Mayors.csv&quot;) tweets&lt;-read_csv(&quot;C:/Users/maria/OneDrive/WUSTL/2019-20/PDS/Tweets.csv&quot;) print(object.size(mayors), units=&quot;auto&quot;) ## 1.5 Mb print(object.size(tweets), units=&quot;auto&quot;) ## 180.1 Mb There are various ways we might want to work across levels Mutating joins, where you make a new variable constructed form matched observations in the other. E.g., how many tweets for each mayor Filtering joins, where you filter cases based on whether they match across. E.g., filter to mayors who have tweets. Combining the datasets in various ways to construct new databases with rows and columns that meet desired specifications. E.g., Tweets of all moyors from Indiana. mayors ## # A tibble: 1,473 x 51 ## X1 MayorID FullName LastName FirstName GenderMale GenderFemale RaceWhite ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 Marty H~ Handlon Marty 0 1 1 ## 2 2 2 Bill Ha~ Ham Jr. Bill 1 0 1 ## 3 3 3 Randall~ Woodfin Randall 1 0 0 ## 4 4 4 Tab Bow~ Bowling Tab 1 0 1 ## 5 5 5 Mark Sa~ Saliba Mark 1 0 1 ## 6 6 6 Steve H~ Holt Steve 1 0 1 ## 7 7 7 Sherman~ Guyton Sherman 1 0 1 ## 8 8 8 Frank V~ Brocato Frank V. 1 0 1 ## 9 9 9 Tommy B~ Battle Tommy 1 0 1 ## 10 10 10 Paul Fi~ Finley Paul 1 0 1 ## # ... with 1,463 more rows, and 43 more variables: RaceBlack &lt;dbl&gt;, ## # RaceHispanic &lt;dbl&gt;, RaceOther &lt;dbl&gt;, PartyRepublican &lt;dbl&gt;, ## # PartyDemocrat &lt;dbl&gt;, PartyNonPartisan &lt;dbl&gt;, PartyOther &lt;dbl&gt;, ## # Ideology &lt;dbl&gt;, IdeologySD &lt;dbl&gt;, LastElectionDate &lt;chr&gt;, ## # PercentVote &lt;dbl&gt;, YearsCurrentPosition &lt;dbl&gt;, CityManager &lt;dbl&gt;, ## # CouncilManager &lt;dbl&gt;, MayorCouncil &lt;dbl&gt;, Title &lt;chr&gt;, CityID &lt;dbl&gt;, ## # CityName &lt;chr&gt;, CityNameFull &lt;chr&gt;, CensusID &lt;chr&gt;, CensusID2 &lt;dbl&gt;, ## # State &lt;chr&gt;, StateAB &lt;chr&gt;, Region &lt;dbl&gt;, Division &lt;dbl&gt;, StateFIPS &lt;dbl&gt;, ## # Population &lt;dbl&gt;, Latitude &lt;dbl&gt;, Longitude &lt;dbl&gt;, CityAge &lt;dbl&gt;, ## # CityMale &lt;dbl&gt;, CityFemale &lt;dbl&gt;, CityWhite &lt;dbl&gt;, CityBlack &lt;dbl&gt;, ## # CityHispanic &lt;dbl&gt;, CityOwner &lt;dbl&gt;, CityRenter &lt;dbl&gt;, GovWebsite &lt;chr&gt;, ## # FacebookPageName &lt;chr&gt;, FacebookPageID &lt;dbl&gt;, FacebookLink &lt;chr&gt;, ## # TwitterHandle &lt;chr&gt;, TwitterLink &lt;chr&gt; tweets ## # A tibble: 604,818 x 17 ## X1 TweetID ScreenName Text CreatedTime Favorited FavoritesCount ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 9.90e17 a_silberb~ &quot;Man~ 2018-04-28 22:26:48 0 9 ## 2 2 9.90e17 a_silberb~ &quot;Suc~ 2018-04-28 22:13:38 0 8 ## 3 3 9.90e17 a_silberb~ &quot;Won~ 2018-04-28 19:42:14 0 11 ## 4 4 9.90e17 a_silberb~ &quot;I e~ 2018-04-28 14:42:20 0 8 ## 5 5 9.90e17 a_silberb~ &quot;At ~ 2018-04-28 13:58:39 0 7 ## 6 6 9.90e17 a_silberb~ &quot;Won~ 2018-04-28 13:53:21 0 6 ## 7 7 9.90e17 a_silberb~ &quot;RT ~ 2018-04-28 12:26:17 0 0 ## 8 8 9.90e17 a_silberb~ &quot;My ~ 2018-04-27 16:31:34 0 4 ## 9 9 9.90e17 a_silberb~ &quot;At ~ 2018-04-27 16:31:23 0 6 ## 10 10 9.90e17 a_silberb~ &quot;I e~ 2018-04-27 03:19:42 0 6 ## # ... with 604,808 more rows, and 10 more variables: IsRetweet &lt;dbl&gt;, ## # RetweetCount &lt;dbl&gt;, Retweeted &lt;dbl&gt;, ReplyToSN &lt;chr&gt;, ReplyToSID &lt;dbl&gt;, ## # ReplyToUID &lt;dbl&gt;, Truncated &lt;dbl&gt;, StatusSource &lt;chr&gt;, Longitude &lt;dbl&gt;, ## # Latitude &lt;dbl&gt; This is a simple relational databset that connects tweets to mayors. The variable ‘TwitterHandle’ in the mayors dataset should match up to ‘ScreenName’ in the tweets data. But it will help you get the basics here. These variables will serve to link across datasets. Let’s rename so they are the same tweets &lt;- rename(tweets, TwitterHandle=ScreenName) tweets ## # A tibble: 604,818 x 17 ## X1 TweetID TwitterHandle Text CreatedTime Favorited ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 1 9.90e17 a_silberberg &quot;Man~ 2018-04-28 22:26:48 0 ## 2 2 9.90e17 a_silberberg &quot;Suc~ 2018-04-28 22:13:38 0 ## 3 3 9.90e17 a_silberberg &quot;Won~ 2018-04-28 19:42:14 0 ## 4 4 9.90e17 a_silberberg &quot;I e~ 2018-04-28 14:42:20 0 ## 5 5 9.90e17 a_silberberg &quot;At ~ 2018-04-28 13:58:39 0 ## 6 6 9.90e17 a_silberberg &quot;Won~ 2018-04-28 13:53:21 0 ## 7 7 9.90e17 a_silberberg &quot;RT ~ 2018-04-28 12:26:17 0 ## 8 8 9.90e17 a_silberberg &quot;My ~ 2018-04-27 16:31:34 0 ## 9 9 9.90e17 a_silberberg &quot;At ~ 2018-04-27 16:31:23 0 ## 10 10 9.90e17 a_silberberg &quot;I e~ 2018-04-27 03:19:42 0 ## # ... with 604,808 more rows, and 11 more variables: FavoritesCount &lt;dbl&gt;, ## # IsRetweet &lt;dbl&gt;, RetweetCount &lt;dbl&gt;, Retweeted &lt;dbl&gt;, ReplyToSN &lt;chr&gt;, ## # ReplyToSID &lt;dbl&gt;, ReplyToUID &lt;dbl&gt;, Truncated &lt;dbl&gt;, StatusSource &lt;chr&gt;, ## # Longitude &lt;dbl&gt;, Latitude &lt;dbl&gt; But each dataset will also need a key – a unique identier Let’s actually look to see if these are unique identifiers? mayors %&gt;% count(TwitterHandle) %&gt;% filter(n&gt;1) ## # A tibble: 3 x 2 ## TwitterHandle n ## &lt;chr&gt; &lt;int&gt; ## 1 robertgarcialb 2 ## 2 rodhiggins2017 2 ## 3 &lt;NA&gt; 743 tweets %&gt;% count(TweetID) %&gt;% filter(n&gt;1) ## # A tibble: 2 x 2 ## TweetID n ## &lt;dbl&gt; &lt;int&gt; ## 1 4.13e17 2 ## 2 7.78e17 2 Hmmm … you might want to figure out these duplicates before running any analyses. 7.3.1 Mutating join A mutating join adds variables to the right on your datset tweets %&gt;% left_join(select(mayors, TwitterHandle, LastName), by=&quot;TwitterHandle&quot;) ## # A tibble: 608,006 x 18 ## X1 TweetID TwitterHandle Text CreatedTime Favorited ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 1 9.90e17 a_silberberg &quot;Man~ 2018-04-28 22:26:48 0 ## 2 2 9.90e17 a_silberberg &quot;Suc~ 2018-04-28 22:13:38 0 ## 3 3 9.90e17 a_silberberg &quot;Won~ 2018-04-28 19:42:14 0 ## 4 4 9.90e17 a_silberberg &quot;I e~ 2018-04-28 14:42:20 0 ## 5 5 9.90e17 a_silberberg &quot;At ~ 2018-04-28 13:58:39 0 ## 6 6 9.90e17 a_silberberg &quot;Won~ 2018-04-28 13:53:21 0 ## 7 7 9.90e17 a_silberberg &quot;RT ~ 2018-04-28 12:26:17 0 ## 8 8 9.90e17 a_silberberg &quot;My ~ 2018-04-27 16:31:34 0 ## 9 9 9.90e17 a_silberberg &quot;At ~ 2018-04-27 16:31:23 0 ## 10 10 9.90e17 a_silberberg &quot;I e~ 2018-04-27 03:19:42 0 ## # ... with 607,996 more rows, and 12 more variables: FavoritesCount &lt;dbl&gt;, ## # IsRetweet &lt;dbl&gt;, RetweetCount &lt;dbl&gt;, Retweeted &lt;dbl&gt;, ReplyToSN &lt;chr&gt;, ## # ReplyToSID &lt;dbl&gt;, ReplyToUID &lt;dbl&gt;, Truncated &lt;dbl&gt;, StatusSource &lt;chr&gt;, ## # Longitude &lt;dbl&gt;, Latitude &lt;dbl&gt;, LastName &lt;chr&gt; 7.3.2 Joins topography “Inner joins” produce results only where the linking variable appears on both tables “Outer joins” produce results with all data, and fills in missing values as necessary Left join keeps only observations from the first specified data if their is no match. Right join keeps only observations from the second data Full join keeps everything Let’s draw up the venn diagrams for: Inner joins Left joins Right joins Full joins 7.3.3 Many and one-to-many The join we did above was a one-to-many join, where the last name was added to all of the tweets for that mayor. But I did this on purpose and it can go awry when the keys are not as unique as you think they are. Joins when there are duplicates lead to creations of new rows for all possible combinations. 7.3.4 Filtering joins semi_join(x, y) keeps all observations in x that have a match in y. anti_join(x, y) drops all observations in x that have a match in y. The latter is very useful for figuring out why your joins are not working as you expect. For important data merges, you should always run these to be sure that these are returning the values and numbers you expect. 7.3.5 Set operations Tidy also includes functions where it expects datasets with the same columns. intersect(x, y) will return only observations in both x and y. union(x, y) will return unique observations in both x and y. setdiff(x, y) will return observations in x, but not in y. This is useful for deduplication tasks and also for identifying problems from specific merge commands. 7.3.6 Exercise Add a third set of data — mentions mentions &lt;-read_csv(&quot;../Datasets/TwitterMentions.csv&quot;) For each mayor, calculate the number of times they were mentioned mentions ## # A tibble: 61,570 x 18 ## X1 TweetID ScreenName Text MayorHandle CreatedTime Favorited ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 1 9.91e17 visitdelr~ Join~ a_silberbe~ 2018-04-30 16:59:14 0 ## 2 2 9.91e17 SatirclAlx .@AC~ a_silberbe~ 2018-04-30 15:44:37 0 ## 3 3 9.91e17 Madelined~ @A_S~ a_silberbe~ 2018-04-30 00:28:15 0 ## 4 4 9.90e17 Her_Grace_ Anot~ a_silberbe~ 2018-04-29 04:02:13 0 ## 5 5 9.90e17 jamesjhare @nat~ a_silberbe~ 2018-04-29 00:04:25 0 ## 6 6 9.90e17 A_Silberb~ Many~ a_silberbe~ 2018-04-28 22:26:48 0 ## 7 7 9.90e17 rossi4va @hok~ a_silberbe~ 2018-04-28 22:22:11 0 ## 8 8 9.90e17 hokiesmas~ @nat~ a_silberbe~ 2018-04-28 22:21:00 0 ## 9 9 9.90e17 rossi4va RT @~ a_silberbe~ 2018-04-28 22:19:42 0 ## 10 10 9.90e17 A_Silberb~ Such~ a_silberbe~ 2018-04-28 22:13:38 0 ## # ... with 61,560 more rows, and 11 more variables: FavoritesCount &lt;dbl&gt;, ## # IsRetweet &lt;dbl&gt;, RetweetCount &lt;dbl&gt;, Retweeted &lt;dbl&gt;, ReplyToSN &lt;chr&gt;, ## # ReplyToSID &lt;dbl&gt;, ReplyToUID &lt;dbl&gt;, Truncated &lt;dbl&gt;, StatusSource &lt;chr&gt;, ## # Longitude &lt;dbl&gt;, Latitude &lt;dbl&gt; mayors ## # A tibble: 1,473 x 51 ## X1 MayorID FullName LastName FirstName GenderMale GenderFemale RaceWhite ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 Marty H~ Handlon Marty 0 1 1 ## 2 2 2 Bill Ha~ Ham Jr. Bill 1 0 1 ## 3 3 3 Randall~ Woodfin Randall 1 0 0 ## 4 4 4 Tab Bow~ Bowling Tab 1 0 1 ## 5 5 5 Mark Sa~ Saliba Mark 1 0 1 ## 6 6 6 Steve H~ Holt Steve 1 0 1 ## 7 7 7 Sherman~ Guyton Sherman 1 0 1 ## 8 8 8 Frank V~ Brocato Frank V. 1 0 1 ## 9 9 9 Tommy B~ Battle Tommy 1 0 1 ## 10 10 10 Paul Fi~ Finley Paul 1 0 1 ## # ... with 1,463 more rows, and 43 more variables: RaceBlack &lt;dbl&gt;, ## # RaceHispanic &lt;dbl&gt;, RaceOther &lt;dbl&gt;, PartyRepublican &lt;dbl&gt;, ## # PartyDemocrat &lt;dbl&gt;, PartyNonPartisan &lt;dbl&gt;, PartyOther &lt;dbl&gt;, ## # Ideology &lt;dbl&gt;, IdeologySD &lt;dbl&gt;, LastElectionDate &lt;chr&gt;, ## # PercentVote &lt;dbl&gt;, YearsCurrentPosition &lt;dbl&gt;, CityManager &lt;dbl&gt;, ## # CouncilManager &lt;dbl&gt;, MayorCouncil &lt;dbl&gt;, Title &lt;chr&gt;, CityID &lt;dbl&gt;, ## # CityName &lt;chr&gt;, CityNameFull &lt;chr&gt;, CensusID &lt;chr&gt;, CensusID2 &lt;dbl&gt;, ## # State &lt;chr&gt;, StateAB &lt;chr&gt;, Region &lt;dbl&gt;, Division &lt;dbl&gt;, StateFIPS &lt;dbl&gt;, ## # Population &lt;dbl&gt;, Latitude &lt;dbl&gt;, Longitude &lt;dbl&gt;, CityAge &lt;dbl&gt;, ## # CityMale &lt;dbl&gt;, CityFemale &lt;dbl&gt;, CityWhite &lt;dbl&gt;, CityBlack &lt;dbl&gt;, ## # CityHispanic &lt;dbl&gt;, CityOwner &lt;dbl&gt;, CityRenter &lt;dbl&gt;, GovWebsite &lt;chr&gt;, ## # FacebookPageName &lt;chr&gt;, FacebookPageID &lt;dbl&gt;, FacebookLink &lt;chr&gt;, ## # TwitterHandle &lt;chr&gt;, TwitterLink &lt;chr&gt; mentions &lt;- rename(mentions, TwitterHandle = &quot;ScreenName&quot;) mentionsCount &lt;- mentions %&gt;% group_by(MayorHandle) %&gt;% tally() answer1 &lt;- mayors %&gt;% left_join(mentionsCount, by=c(&quot;TwitterHandle&quot; = &quot;MayorHandle&quot;)) %&gt;% select(TwitterHandle, FullName, n) %&gt;% arrange(desc(n)) answer1 ## # A tibble: 1,473 x 3 ## TwitterHandle FullName n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 markfarrellsf Mark Farrell 1000 ## 2 sam_hindi Sam Hindi 1000 ## 3 andrewgillum Andrew D. Gillum 1000 ## 4 keishabottoms Keisha Lance Bottoms 1000 ## 5 chicagosmayor Rahm Emmanuel 1000 ## 6 indymayorjoe Joseph &#39;Joe&#39; H. Hogsett 1000 ## 7 louisvillemayor Greg Fischer 1000 ## 8 billdeblasio Bill de Blasio 1000 ## 9 billpeduto William Peduto 1000 ## 10 mayorbriley David Briley 999 ## # ... with 1,463 more rows Add to the mayors datset the number of times each mayor tweeted. tweetCounts &lt;- tweets %&gt;% group_by(TwitterHandle) %&gt;% tally() answer2 &lt;- mayors %&gt;% left_join(tweetCounts, by=&quot;TwitterHandle&quot;) %&gt;% arrange(desc(n)) answer2 ## # A tibble: 1,473 x 52 ## X1 MayorID FullName LastName FirstName GenderMale GenderFemale RaceWhite ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 11 11 William~ Stimpson William ~ 1 0 1 ## 2 443 443 Thomas ~ Masters Thomas A. 1 0 0 ## 3 498 498 Rahm Em~ Emmanuel Rahm 1 0 1 ## 4 778 778 Emily L~ Larson Emily 0 1 1 ## 5 812 812 Jason L~ Shelton Jason L. 1 0 1 ## 6 902 902 Derek A~ Armstead Derek 1 0 0 ## 7 1070 1071 Greg Pe~ Peterson Greg 1 0 1 ## 8 1140 1141 Jim Ken~ Kenney Jim 1 0 1 ## 9 1142 1143 Ed Pawl~ Pawlows~ Ed 1 0 1 ## 10 37 37 Greg St~ Stanton Greg 1 0 1 ## # ... with 1,463 more rows, and 44 more variables: RaceBlack &lt;dbl&gt;, ## # RaceHispanic &lt;dbl&gt;, RaceOther &lt;dbl&gt;, PartyRepublican &lt;dbl&gt;, ## # PartyDemocrat &lt;dbl&gt;, PartyNonPartisan &lt;dbl&gt;, PartyOther &lt;dbl&gt;, ## # Ideology &lt;dbl&gt;, IdeologySD &lt;dbl&gt;, LastElectionDate &lt;chr&gt;, ## # PercentVote &lt;dbl&gt;, YearsCurrentPosition &lt;dbl&gt;, CityManager &lt;dbl&gt;, ## # CouncilManager &lt;dbl&gt;, MayorCouncil &lt;dbl&gt;, Title &lt;chr&gt;, CityID &lt;dbl&gt;, ## # CityName &lt;chr&gt;, CityNameFull &lt;chr&gt;, CensusID &lt;chr&gt;, CensusID2 &lt;dbl&gt;, ## # State &lt;chr&gt;, StateAB &lt;chr&gt;, Region &lt;dbl&gt;, Division &lt;dbl&gt;, StateFIPS &lt;dbl&gt;, ## # Population &lt;dbl&gt;, Latitude &lt;dbl&gt;, Longitude &lt;dbl&gt;, CityAge &lt;dbl&gt;, ## # CityMale &lt;dbl&gt;, CityFemale &lt;dbl&gt;, CityWhite &lt;dbl&gt;, CityBlack &lt;dbl&gt;, ## # CityHispanic &lt;dbl&gt;, CityOwner &lt;dbl&gt;, CityRenter &lt;dbl&gt;, GovWebsite &lt;chr&gt;, ## # FacebookPageName &lt;chr&gt;, FacebookPageID &lt;dbl&gt;, FacebookLink &lt;chr&gt;, ## # TwitterHandle &lt;chr&gt;, TwitterLink &lt;chr&gt;, n &lt;int&gt; Create a combined dataset of all tweets from the tweets and mentions data. Subset down to overlapping columns (and rename where needed) to make this easy. allData &lt;- full_join(tweets, mentions) ## Joining, by = c(&quot;X1&quot;, &quot;TweetID&quot;, &quot;TwitterHandle&quot;, &quot;Text&quot;, &quot;CreatedTime&quot;, &quot;Favorited&quot;, &quot;FavoritesCount&quot;, &quot;IsRetweet&quot;, &quot;RetweetCount&quot;, &quot;Retweeted&quot;, &quot;ReplyToSN&quot;, &quot;ReplyToSID&quot;, &quot;ReplyToUID&quot;, &quot;Truncated&quot;, &quot;StatusSource&quot;, &quot;Longitude&quot;, &quot;Latitude&quot;) allData ## # A tibble: 666,388 x 18 ## X1 TweetID TwitterHandle Text CreatedTime Favorited ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 1 9.90e17 a_silberberg &quot;Man~ 2018-04-28 22:26:48 0 ## 2 2 9.90e17 a_silberberg &quot;Suc~ 2018-04-28 22:13:38 0 ## 3 3 9.90e17 a_silberberg &quot;Won~ 2018-04-28 19:42:14 0 ## 4 4 9.90e17 a_silberberg &quot;I e~ 2018-04-28 14:42:20 0 ## 5 5 9.90e17 a_silberberg &quot;At ~ 2018-04-28 13:58:39 0 ## 6 6 9.90e17 a_silberberg &quot;Won~ 2018-04-28 13:53:21 0 ## 7 7 9.90e17 a_silberberg &quot;RT ~ 2018-04-28 12:26:17 0 ## 8 8 9.90e17 a_silberberg &quot;My ~ 2018-04-27 16:31:34 0 ## 9 9 9.90e17 a_silberberg &quot;At ~ 2018-04-27 16:31:23 0 ## 10 10 9.90e17 a_silberberg &quot;I e~ 2018-04-27 03:19:42 0 ## # ... with 666,378 more rows, and 12 more variables: FavoritesCount &lt;dbl&gt;, ## # IsRetweet &lt;dbl&gt;, RetweetCount &lt;dbl&gt;, Retweeted &lt;dbl&gt;, ReplyToSN &lt;chr&gt;, ## # ReplyToSID &lt;dbl&gt;, ReplyToUID &lt;dbl&gt;, Truncated &lt;dbl&gt;, StatusSource &lt;chr&gt;, ## # Longitude &lt;dbl&gt;, Latitude &lt;dbl&gt;, MayorHandle &lt;chr&gt; 7.4 stringr stringr is a tidy package that is used for processing strings and text data library(tidyverse) tweets&lt;-read_csv(&quot;C:/Users/maria/OneDrive/WUSTL/2019-20/PDS/Tweets.csv&quot;) aTweet&lt;-tweets[1,]$Text aTweet ## [1] &quot;Many thanks to everyone involved in Alexandria&#39;s #EarthDay celebration. Together, we are reducing pollution and was… https://t.co/wwMOIg4t1w&quot; str_length(aTweet) ## [1] 140 words&lt;-str_split(aTweet, pattern = &quot; &quot;) words ## [[1]] ## [1] &quot;Many&quot; &quot;thanks&quot; ## [3] &quot;to&quot; &quot;everyone&quot; ## [5] &quot;involved&quot; &quot;in&quot; ## [7] &quot;Alexandria&#39;s&quot; &quot;#EarthDay&quot; ## [9] &quot;celebration.&quot; &quot;Together,&quot; ## [11] &quot;we&quot; &quot;are&quot; ## [13] &quot;reducing&quot; &quot;pollution&quot; ## [15] &quot;and&quot; &quot;was…&quot; ## [17] &quot;https://t.co/wwMOIg4t1w&quot; str_c(words) ## [1] &quot;c(\\&quot;Many\\&quot;, \\&quot;thanks\\&quot;, \\&quot;to\\&quot;, \\&quot;everyone\\&quot;, \\&quot;involved\\&quot;, \\&quot;in\\&quot;, \\&quot;Alexandria&#39;s\\&quot;, \\&quot;#EarthDay\\&quot;, \\&quot;celebration.\\&quot;, \\&quot;Together,\\&quot;, \\&quot;we\\&quot;, \\&quot;are\\&quot;, \\&quot;reducing\\&quot;, \\&quot;pollution\\&quot;, \\&quot;and\\&quot;, \\&quot;was…\\&quot;, \\&quot;https://t.co/wwMOIg4t1w\\&quot;)&quot; str_c(unlist(words)) ## [1] &quot;Many&quot; &quot;thanks&quot; ## [3] &quot;to&quot; &quot;everyone&quot; ## [5] &quot;involved&quot; &quot;in&quot; ## [7] &quot;Alexandria&#39;s&quot; &quot;#EarthDay&quot; ## [9] &quot;celebration.&quot; &quot;Together,&quot; ## [11] &quot;we&quot; &quot;are&quot; ## [13] &quot;reducing&quot; &quot;pollution&quot; ## [15] &quot;and&quot; &quot;was…&quot; ## [17] &quot;https://t.co/wwMOIg4t1w&quot; str_c(unlist(words), &quot;Added&quot;) ## [1] &quot;ManyAdded&quot; &quot;thanksAdded&quot; ## [3] &quot;toAdded&quot; &quot;everyoneAdded&quot; ## [5] &quot;involvedAdded&quot; &quot;inAdded&quot; ## [7] &quot;Alexandria&#39;sAdded&quot; &quot;#EarthDayAdded&quot; ## [9] &quot;celebration.Added&quot; &quot;Together,Added&quot; ## [11] &quot;weAdded&quot; &quot;areAdded&quot; ## [13] &quot;reducingAdded&quot; &quot;pollutionAdded&quot; ## [15] &quot;andAdded&quot; &quot;was…Added&quot; ## [17] &quot;https://t.co/wwMOIg4t1wAdded&quot; str_c(unlist(words), &quot;Added&quot;, sep=&quot; &quot;) ## [1] &quot;Many Added&quot; &quot;thanks Added&quot; ## [3] &quot;to Added&quot; &quot;everyone Added&quot; ## [5] &quot;involved Added&quot; &quot;in Added&quot; ## [7] &quot;Alexandria&#39;s Added&quot; &quot;#EarthDay Added&quot; ## [9] &quot;celebration. Added&quot; &quot;Together, Added&quot; ## [11] &quot;we Added&quot; &quot;are Added&quot; ## [13] &quot;reducing Added&quot; &quot;pollution Added&quot; ## [15] &quot;and Added&quot; &quot;was… Added&quot; ## [17] &quot;https://t.co/wwMOIg4t1w Added&quot; str_c(&quot;Before&quot;, unlist(words), &quot;Added&quot;, sep=&quot; &quot;) ## [1] &quot;Before Many Added&quot; ## [2] &quot;Before thanks Added&quot; ## [3] &quot;Before to Added&quot; ## [4] &quot;Before everyone Added&quot; ## [5] &quot;Before involved Added&quot; ## [6] &quot;Before in Added&quot; ## [7] &quot;Before Alexandria&#39;s Added&quot; ## [8] &quot;Before #EarthDay Added&quot; ## [9] &quot;Before celebration. Added&quot; ## [10] &quot;Before Together, Added&quot; ## [11] &quot;Before we Added&quot; ## [12] &quot;Before are Added&quot; ## [13] &quot;Before reducing Added&quot; ## [14] &quot;Before pollution Added&quot; ## [15] &quot;Before and Added&quot; ## [16] &quot;Before was… Added&quot; ## [17] &quot;Before https://t.co/wwMOIg4t1w Added&quot; Or you may want to assemble it all into all back into a coherent whole str_c(unlist(words), collapse=&quot; &quot;) ## [1] &quot;Many thanks to everyone involved in Alexandria&#39;s #EarthDay celebration. Together, we are reducing pollution and was… https://t.co/wwMOIg4t1w&quot; 7.4.1 Things you haven’t thought about strings Strings in memory are different than how they appear on the screen You can use the function writeLines to see how a function would atually render to a reader (as opposed to how it is stored in the computer.) The big confusion is that special characters have to be “escaped” or else they get in the way (From R4DS) x &lt;- c(&quot;\\&quot;&quot;, &quot;\\\\&quot;) x ## [1] &quot;\\&quot;&quot; &quot;\\\\&quot; writeLines(x) ## &quot; ## \\ Others to watch out for are \"\\n\" and \"\\t\" for next line and tab na is a problem, so you might use str_replace_na stringr replicates a lot of the functions we have already used for characters with (sometimes) slightly different syntax colors &lt;- c(&quot;red&quot;, &quot;yellow&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;magenta&quot;, &quot;cyan&quot;) str_sub(colors, 1, 3) ## [1] &quot;red&quot; &quot;yel&quot; &quot;blu&quot; &quot;gre&quot; &quot;mag&quot; &quot;cya&quot; str_sub(colors, -3, -1) #count back from the end ## [1] &quot;red&quot; &quot;low&quot; &quot;lue&quot; &quot;een&quot; &quot;nta&quot; &quot;yan&quot; str_sub(colors, 1, 10) # Robust to going out of index ## [1] &quot;red&quot; &quot;yellow&quot; &quot;blue&quot; &quot;green&quot; &quot;magenta&quot; &quot;cyan&quot; str_sub(colors, 1, 3)&lt;-str_to_upper(str_sub(colors, 1, 3)) colors ## [1] &quot;RED&quot; &quot;YELlow&quot; &quot;BLUe&quot; &quot;GREen&quot; &quot;MAGenta&quot; &quot;CYAn&quot; colors&lt;-str_to_lower(colors) colors ## [1] &quot;red&quot; &quot;yellow&quot; &quot;blue&quot; &quot;green&quot; &quot;magenta&quot; &quot;cyan&quot; Example: Filter down to all of the tweets from Mayor Lyda Krewson in our data Find the mean number of words in her tweets. Find the total. Take all of the words she has used and reduce them down to only their first five letters. Repeat (2) and compare. Question 2 hertweets &lt;- filter(tweets, ScreenName == &quot;lydakrewson&quot;)[&quot;Text&quot;] numWords &lt;- hertweets %&gt;% rowwise() %&gt;% transmute(numWords= length(unlist(str_split(Text, &quot; &quot;)))) numWords ## Source: local data frame [3,191 x 1] ## Groups: &lt;by row&gt; ## ## # A tibble: 3,191 x 1 ## numWords ## &lt;int&gt; ## 1 20 ## 2 25 ## 3 20 ## 4 20 ## 5 17 ## 6 21 ## 7 8 ## 8 11 ## 9 19 ## 10 13 ## # ... with 3,181 more rows mean(numWords$numWords) ## [1] 16.58602 length(unique(unlist(str_split(hertweets$Text, &quot; &quot;)))) ## [1] 15960 Question 3 length(unique(str_sub(unlist(str_split(hertweets$Text, &quot; &quot;)), 1, 5))) ## [1] 9831 7.5 Pipes and maps Download: https://www.voterstudygroup.org/publication/2019-voter-survey-full-data-set Codebook: https://www.voterstudygroup.org/publication/2019-voter-survey-full-data-set library(tidyverse) VSG&lt;-read_csv(&quot;C:/Users/maria/OneDrive/WUSTL/2019-20/PDS/VOTER_Survey_Jan217_Release1-csv.csv&quot;, col_types=cols(weight_18_24_2018=&quot;d&quot;)) Question: Is there a lane? We are going to: Subset the data to just favorability raitings for Elizabeth Warren and Bernie Sanders Recode these variables into “approve” and “not approve” Find the proportion of respondents who either approve or disapprove of both One strategy is just to make new datsets for each step VSG.two &lt;- select(VSG, fav_sanders_2019, fav_warren_2019) VSG.three&lt;-mutate(VSG.two, fav_sanders_binary = (fav_sanders_2019&lt;=2)) VSG.four&lt;-mutate(VSG.three, fav_sanders_binary= na_if(fav_sanders_2019, fav_sanders_2019&gt;4)) This is bad because we’re wasting a lot of memory in our environment that we’ll never use again. Another strategy is just to overwrite the same dataset VSG &lt;- select(VSG, fav_sanders_2019, fav_warren_2019) VSG &lt; -mutate(VSG, fav_sanders_binary = (fav_sanders_2019&lt;=2)*1) VSG &lt;- mutate(VSG, fav_sanders_binary= na_if(fav_sanders_2019, fav_sanders_2019&gt;4)) This still wastes a lot of memory because of the way memory is managed in R. It also would be terrible to debug if something goes wrong. An even more difficult way would be write a function where the output of each stage are passed onto an outer layer. Or we can use the pipes VSG %&gt;% select(fav_sanders_2019, fav_warren_2019) %&gt;% mutate(fav_sanders_binary= na_if(fav_sanders_2019, fav_sanders_2019&gt;4)) %&gt;% mutate(fav_sanders_binary = (fav_sanders_2019&lt;=2)*1) 7.5.1 Limitations of Pipes Piping works by creating a new function where object manipulations are passed from one to another This means that piping won’t work in cases where: The function uses the current environment such as assign, load, or get Functions that use lazy evaluation such as tryCatch, try, or supressMessages Pipes are not good in all situations If you do more than 4 or 5 pipes, go ahead and make in intermediate object. Otherwise debugging is a pain. If you are not actually working on just one object but instead combinig multiple objects, this is just not worth it. Where ther are interdepencies (e.g., complex recoding based on multiple variables) The magrittr package comes with some other tools for using in specific situations The %T&gt;% pipe works by calling the function to the right, but returning the function to the left library(magrittr) VSG&lt;-read_csv(&quot;C:/Users/maria/OneDrive/WUSTL/2019-20/PDS/VOTER_Survey_Jan217_Release1-csv.csv&quot;, col_types=cols(weight_18_24_2018=&quot;d&quot;)) VSG %&gt;% select(fav_sanders_2019, fav_warren_2019) %&gt;% mutate(fav_sanders_binary= na_if(fav_sanders_2019, fav_sanders_2019&gt;4)) %&gt;% mutate(fav_sanders_binary = (fav_sanders_2019&lt;=2)*1) %T&gt;% plot() ## # A tibble: 9,548 x 3 ## fav_sanders_2019 fav_warren_2019 fav_sanders_binary ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 ## 2 NA NA NA ## 3 2 1 1 ## 4 4 3 0 ## 5 3 4 0 ## 6 3 1 0 ## 7 3 2 0 ## 8 NA NA NA ## 9 NA NA NA ## 10 2 1 1 ## # ... with 9,538 more rows We may also want to use specific elements from a dataframe using the %$% operator VSG&lt;- VSG %&gt;% select(fav_sanders_2019, fav_warren_2019) %&gt;% mutate(fav_sanders_binary= na_if(fav_sanders_2019, fav_sanders_2019&gt;4)) %&gt;% mutate(fav_sanders_binary = (fav_sanders_2019&lt;=2)*1) %&gt;% mutate(fav_warren_binary= na_if(fav_warren_2019, fav_warren_2019&gt;4)) %&gt;% mutate(fav_warren_binary = (fav_warren_2019&lt;=2)*1) library(magrittr) VSG&lt;- VSG %$% plot(table(fav_sanders_binary, fav_warren_binary)) 7.6 map VSG&lt;-read_csv(&quot;C:/Users/maria/OneDrive/WUSTL/2019-20/PDS/VOTER_Survey_Jan217_Release1-csv.csv&quot;, col_types=cols(weight_18_24_2018=&quot;d&quot;)) VSG.fav&lt;-VSG %&gt;% select(starts_with(&quot;fav&quot;)) We want to loop over these data and recode all of the columns according to our rules Anything above a 4 gets moved to NA Then we turn them into a binary based on whether they are 2 and less or not output&lt;-as.data.frame(matrix(NA, nrow(VSG.fav), ncol(VSG.fav))) for(i in seq_along(VSG.fav)){ output[,i]&lt;-(VSG.fav[,i]&lt;=2)*1 } We could do this without making a new object for(i in seq_along(VSG.fav)){ VSG.fav[,i]= na_if(VSG.fav[[i]],VSG.fav[[i]]&gt;4) VSG.fav[,i]&lt;-(VSG.fav[,i]&lt;=2)*1 } head(VSG.fav) ## # A tibble: 6 x 68 ## fav_trump_2019 fav_obama_2019 fav_hrc_2019 fav_sanders_2019 fav_putin_2019 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 1 1 0 ## 2 NA NA NA NA NA ## 3 0 1 1 1 0 ## 4 0 1 0 0 0 ## 5 1 1 0 0 0 ## 6 0 1 1 0 0 ## # ... with 63 more variables: fav_schumer_2019 &lt;dbl&gt;, fav_pelosi_2019 &lt;dbl&gt;, ## # fav_comey_2019 &lt;dbl&gt;, fav_mueller_2019 &lt;dbl&gt;, fav_mcconnell_2019 &lt;dbl&gt;, ## # fav_kavanaugh_2019 &lt;dbl&gt;, fav_biden_2019 &lt;dbl&gt;, fav_warren_2019 &lt;dbl&gt;, ## # fav_harris_2019 &lt;dbl&gt;, fav_gillibrand_2019 &lt;dbl&gt;, fav_patrick_2019 &lt;dbl&gt;, ## # fav_booker_2019 &lt;dbl&gt;, fav_garcetti_2019 &lt;dbl&gt;, fav_klobuchar_2019 &lt;dbl&gt;, ## # fav_gorsuch_2019 &lt;dbl&gt;, fav_kasich_2019 &lt;dbl&gt;, fav_haley_2019 &lt;dbl&gt;, ## # fav_bloomberg_2019 &lt;dbl&gt;, fav_holder_2019 &lt;dbl&gt;, fav_avenatti_2019 &lt;dbl&gt;, ## # fav_castro_2019 &lt;dbl&gt;, fav_landrieu_2019 &lt;dbl&gt;, fav_orourke_2019 &lt;dbl&gt;, ## # fav_hickenlooper_2019 &lt;dbl&gt;, fav_pence_2019 &lt;dbl&gt;, favorhealth_2019 &lt;dbl&gt;, ## # fav_trump_2018 &lt;dbl&gt;, fav_ryan_2018 &lt;dbl&gt;, fav_obama_2018 &lt;dbl&gt;, ## # fav_hrc_2018 &lt;dbl&gt;, fav_sanders_2018 &lt;dbl&gt;, fav_putin_2018 &lt;dbl&gt;, ## # fav_schumer_2018 &lt;dbl&gt;, fav_pelosi_2018 &lt;dbl&gt;, fav_comey_2018 &lt;dbl&gt;, ## # fav_mueller_2018 &lt;dbl&gt;, fav_mcconnell_2018 &lt;dbl&gt;, fav_trump_2017 &lt;dbl&gt;, ## # fav_ryan_2017 &lt;dbl&gt;, fav_obama_2017 &lt;dbl&gt;, fav_hrc_2017 &lt;dbl&gt;, ## # fav_sanders_2017 &lt;dbl&gt;, fav_putin_2017 &lt;dbl&gt;, fav_trump_2016 &lt;dbl&gt;, ## # fav_cruz_2016 &lt;dbl&gt;, fav_ryan_2016 &lt;dbl&gt;, fav_romn_2016 &lt;dbl&gt;, ## # fav_obama_2016 &lt;dbl&gt;, fav_hrc_2016 &lt;dbl&gt;, fav_sanders_2016 &lt;dbl&gt;, ## # fav_rubio_2016 &lt;dbl&gt;, fav_grid_row_rnd_2016 &lt;dbl&gt;, ## # fav_grid_col_rnd_2016 &lt;dbl&gt;, fav_romn_baseline &lt;dbl&gt;, ## # fav_ging_baseline &lt;dbl&gt;, fav_hunt_baseline &lt;dbl&gt;, fav_bach_baseline &lt;dbl&gt;, ## # fav_ronp_baseline &lt;dbl&gt;, fav_sant_baseline &lt;dbl&gt;, fav_perr_baseline &lt;dbl&gt;, ## # fav_obama_baseline &lt;dbl&gt;, fav_hrc_baseline &lt;dbl&gt;, fav_biden_baseline &lt;dbl&gt; We can also loop where we aren’t sure how long the final output is This can be done by adding each element to the end of a vector as we did in some previous lectures This is computationally expensive A better solution is to add all of them to a list and then combine them back at the end. See Chapter 23.3.3 in the book for more Or Chapter 23.3.4 for while loops (which we have already covered) Looping is such a pervasive task, that there are functions to make it easier The map family will do this, and you can carefully control the output map makes a list map_lgl makes a logical vector map_int makes an integer map_dbl makes a double vector map_chr makes a character These functions takes in a vector as an input and applies that function Let’s say that we want to find the mean level of favorability in our dataset This is very similar to lapply in base R For a comparison, check this link out map(VSG.fav, mean, na.rm=TRUE) ## $fav_trump_2019 ## [1] 0.4214486 ## ## $fav_obama_2019 ## [1] 0.5311993 ## ## $fav_hrc_2019 ## [1] 0.3907656 ## ## $fav_sanders_2019 ## [1] 0.4724886 ## ## $fav_putin_2019 ## [1] 0.08172297 ## ## $fav_schumer_2019 ## [1] 0.3249742 ## ## $fav_pelosi_2019 ## [1] 0.3808821 ## ## $fav_comey_2019 ## [1] 0.2996017 ## ## $fav_mueller_2019 ## [1] 0.4549344 ## ## $fav_mcconnell_2019 ## [1] 0.2793922 ## ## $fav_kavanaugh_2019 ## [1] 0.4047795 ## ## $fav_biden_2019 ## [1] 0.5340021 ## ## $fav_warren_2019 ## [1] 0.4173182 ## ## $fav_harris_2019 ## [1] 0.322909 ## ## $fav_gillibrand_2019 ## [1] 0.2838177 ## ## $fav_patrick_2019 ## [1] 0.1606432 ## ## $fav_booker_2019 ## [1] 0.3462163 ## ## $fav_garcetti_2019 ## [1] 0.1048827 ## ## $fav_klobuchar_2019 ## [1] 0.2283523 ## ## $fav_gorsuch_2019 ## [1] 0.354182 ## ## $fav_kasich_2019 ## [1] 0.3192211 ## ## $fav_haley_2019 ## [1] 0.420416 ## ## $fav_bloomberg_2019 ## [1] 0.3218764 ## ## $fav_holder_2019 ## [1] 0.3221714 ## ## $fav_avenatti_2019 ## [1] 0.1737719 ## ## $fav_castro_2019 ## [1] 0.2015046 ## ## $fav_landrieu_2019 ## [1] 0.1274524 ## ## $fav_orourke_2019 ## [1] 0.364213 ## ## $fav_hickenlooper_2019 ## [1] 0.1295176 ## ## $fav_pence_2019 ## [1] 0.4437233 ## ## $favorhealth_2019 ## [1] 0.476914 ## ## $fav_trump_2018 ## [1] 0.3953372 ## ## $fav_ryan_2018 ## [1] 0.2812656 ## ## $fav_obama_2018 ## [1] 0.5322231 ## ## $fav_hrc_2018 ## [1] 0.3900083 ## ## $fav_sanders_2018 ## [1] 0.4814321 ## ## $fav_putin_2018 ## [1] 0.07693589 ## ## $fav_schumer_2018 ## [1] 0.3080766 ## ## $fav_pelosi_2018 ## [1] 0.3233972 ## ## $fav_comey_2018 ## [1] 0.3172356 ## ## $fav_mueller_2018 ## [1] 0.4066611 ## ## $fav_mcconnell_2018 ## [1] 0.1567027 ## ## $fav_trump_2017 ## [1] 0.43642 ## ## $fav_ryan_2017 ## [1] 0.2748614 ## ## $fav_obama_2017 ## [1] 0.4928269 ## ## $fav_hrc_2017 ## [1] 0.4029997 ## ## $fav_sanders_2017 ## [1] 0.4768503 ## ## $fav_putin_2017 ## [1] 0.1092273 ## ## $fav_trump_2016 ## [1] 0.4416231 ## ## $fav_cruz_2016 ## [1] 0.3446602 ## ## $fav_ryan_2016 ## [1] 0.3635798 ## ## $fav_romn_2016 ## [1] 0.3480209 ## ## $fav_obama_2016 ## [1] 0.4887976 ## ## $fav_hrc_2016 ## [1] 0.418347 ## ## $fav_sanders_2016 ## [1] 0.5373413 ## ## $fav_rubio_2016 ## [1] 0.4218322 ## ## $fav_grid_row_rnd_2016 ## [1] 0 ## ## $fav_grid_col_rnd_2016 ## [1] 0 ## ## $fav_romn_baseline ## [1] 0.4337814 ## ## $fav_ging_baseline ## [1] 0.3297237 ## ## $fav_hunt_baseline ## [1] 0.3512572 ## ## $fav_bach_baseline ## [1] 0.3188947 ## ## $fav_ronp_baseline ## [1] 0.3409261 ## ## $fav_sant_baseline ## [1] 0.377645 ## ## $fav_perr_baseline ## [1] 0.3066965 ## ## $fav_obama_baseline ## [1] 0.4814538 ## ## $fav_hrc_baseline ## [1] 0.5892457 ## ## $fav_biden_baseline ## [1] 0.4600448 map_dbl(VSG.fav, mean, na.rm=TRUE) ## fav_trump_2019 fav_obama_2019 fav_hrc_2019 ## 0.42144859 0.53119929 0.39076560 ## fav_sanders_2019 fav_putin_2019 fav_schumer_2019 ## 0.47248857 0.08172297 0.32497418 ## fav_pelosi_2019 fav_comey_2019 fav_mueller_2019 ## 0.38088214 0.29960171 0.45493436 ## fav_mcconnell_2019 fav_kavanaugh_2019 fav_biden_2019 ## 0.27939224 0.40477947 0.53400207 ## fav_warren_2019 fav_harris_2019 fav_gillibrand_2019 ## 0.41731819 0.32290898 0.28381767 ## fav_patrick_2019 fav_booker_2019 fav_garcetti_2019 ## 0.16064316 0.34621626 0.10488273 ## fav_klobuchar_2019 fav_gorsuch_2019 fav_kasich_2019 ## 0.22835226 0.35418203 0.31922112 ## fav_haley_2019 fav_bloomberg_2019 fav_holder_2019 ## 0.42041599 0.32187638 0.32217141 ## fav_avenatti_2019 fav_castro_2019 fav_landrieu_2019 ## 0.17377194 0.20150465 0.12745243 ## fav_orourke_2019 fav_hickenlooper_2019 fav_pence_2019 ## 0.36421301 0.12951763 0.44372326 ## favorhealth_2019 fav_trump_2018 fav_ryan_2018 ## 0.47691400 0.39533722 0.28126561 ## fav_obama_2018 fav_hrc_2018 fav_sanders_2018 ## 0.53222315 0.39000833 0.48143214 ## fav_putin_2018 fav_schumer_2018 fav_pelosi_2018 ## 0.07693589 0.30807660 0.32339717 ## fav_comey_2018 fav_mueller_2018 fav_mcconnell_2018 ## 0.31723564 0.40666112 0.15670275 ## fav_trump_2017 fav_ryan_2017 fav_obama_2017 ## 0.43641995 0.27486143 0.49282687 ## fav_hrc_2017 fav_sanders_2017 fav_putin_2017 ## 0.40299967 0.47685034 0.10922726 ## fav_trump_2016 fav_cruz_2016 fav_ryan_2016 ## 0.44162310 0.34466019 0.36357979 ## fav_romn_2016 fav_obama_2016 fav_hrc_2016 ## 0.34802091 0.48879761 0.41834703 ## fav_sanders_2016 fav_rubio_2016 fav_grid_row_rnd_2016 ## 0.53734130 0.42183221 0.00000000 ## fav_grid_col_rnd_2016 fav_romn_baseline fav_ging_baseline ## 0.00000000 0.43378143 0.32972367 ## fav_hunt_baseline fav_bach_baseline fav_ronp_baseline ## 0.35125716 0.31889470 0.34092606 ## fav_sant_baseline fav_perr_baseline fav_obama_baseline ## 0.37764501 0.30669654 0.48145382 ## fav_hrc_baseline fav_biden_baseline ## 0.58924571 0.46004481 map_chr(VSG.fav, mean, na.rm=TRUE) ## fav_trump_2019 fav_obama_2019 fav_hrc_2019 ## &quot;0.421449&quot; &quot;0.531199&quot; &quot;0.390766&quot; ## fav_sanders_2019 fav_putin_2019 fav_schumer_2019 ## &quot;0.472489&quot; &quot;0.081723&quot; &quot;0.324974&quot; ## fav_pelosi_2019 fav_comey_2019 fav_mueller_2019 ## &quot;0.380882&quot; &quot;0.299602&quot; &quot;0.454934&quot; ## fav_mcconnell_2019 fav_kavanaugh_2019 fav_biden_2019 ## &quot;0.279392&quot; &quot;0.404779&quot; &quot;0.534002&quot; ## fav_warren_2019 fav_harris_2019 fav_gillibrand_2019 ## &quot;0.417318&quot; &quot;0.322909&quot; &quot;0.283818&quot; ## fav_patrick_2019 fav_booker_2019 fav_garcetti_2019 ## &quot;0.160643&quot; &quot;0.346216&quot; &quot;0.104883&quot; ## fav_klobuchar_2019 fav_gorsuch_2019 fav_kasich_2019 ## &quot;0.228352&quot; &quot;0.354182&quot; &quot;0.319221&quot; ## fav_haley_2019 fav_bloomberg_2019 fav_holder_2019 ## &quot;0.420416&quot; &quot;0.321876&quot; &quot;0.322171&quot; ## fav_avenatti_2019 fav_castro_2019 fav_landrieu_2019 ## &quot;0.173772&quot; &quot;0.201505&quot; &quot;0.127452&quot; ## fav_orourke_2019 fav_hickenlooper_2019 fav_pence_2019 ## &quot;0.364213&quot; &quot;0.129518&quot; &quot;0.443723&quot; ## favorhealth_2019 fav_trump_2018 fav_ryan_2018 ## &quot;0.476914&quot; &quot;0.395337&quot; &quot;0.281266&quot; ## fav_obama_2018 fav_hrc_2018 fav_sanders_2018 ## &quot;0.532223&quot; &quot;0.390008&quot; &quot;0.481432&quot; ## fav_putin_2018 fav_schumer_2018 fav_pelosi_2018 ## &quot;0.076936&quot; &quot;0.308077&quot; &quot;0.323397&quot; ## fav_comey_2018 fav_mueller_2018 fav_mcconnell_2018 ## &quot;0.317236&quot; &quot;0.406661&quot; &quot;0.156703&quot; ## fav_trump_2017 fav_ryan_2017 fav_obama_2017 ## &quot;0.436420&quot; &quot;0.274861&quot; &quot;0.492827&quot; ## fav_hrc_2017 fav_sanders_2017 fav_putin_2017 ## &quot;0.403000&quot; &quot;0.476850&quot; &quot;0.109227&quot; ## fav_trump_2016 fav_cruz_2016 fav_ryan_2016 ## &quot;0.441623&quot; &quot;0.344660&quot; &quot;0.363580&quot; ## fav_romn_2016 fav_obama_2016 fav_hrc_2016 ## &quot;0.348021&quot; &quot;0.488798&quot; &quot;0.418347&quot; ## fav_sanders_2016 fav_rubio_2016 fav_grid_row_rnd_2016 ## &quot;0.537341&quot; &quot;0.421832&quot; &quot;0.000000&quot; ## fav_grid_col_rnd_2016 fav_romn_baseline fav_ging_baseline ## &quot;0.000000&quot; &quot;0.433781&quot; &quot;0.329724&quot; ## fav_hunt_baseline fav_bach_baseline fav_ronp_baseline ## &quot;0.351257&quot; &quot;0.318895&quot; &quot;0.340926&quot; ## fav_sant_baseline fav_perr_baseline fav_obama_baseline ## &quot;0.377645&quot; &quot;0.306697&quot; &quot;0.481454&quot; ## fav_hrc_baseline fav_biden_baseline ## &quot;0.589246&quot; &quot;0.460045&quot; 7.7 walk, purrr, and more See Chapter 21.8 for a discussion of walk Main idea here is when you want to use a function for its side operations (e.g., plot) rather than to manipulate the data See also Chapter 21.9.1 for some additional functionality that is occasionally useful: keep discard some every See Chapter 21.9.2 for even more advanced approaches to extracting data "],
["section-more-useful-packages.html", "8 More useful packages 8.1 stringr", " 8 More useful packages 8.1 stringr stringr is a tidy package that is used for processing strings and text data library(tidyverse) tweets&lt;-read_csv(&quot;C:/Users/maria/OneDrive/WUSTL/2019-20/PDS/Tweets.csv&quot;) aTweet&lt;-tweets[1,]$Text aTweet ## [1] &quot;Many thanks to everyone involved in Alexandria&#39;s #EarthDay celebration. Together, we are reducing pollution and was… https://t.co/wwMOIg4t1w&quot; str_length(aTweet) ## [1] 140 words&lt;-str_split(aTweet, pattern = &quot; &quot;) words ## [[1]] ## [1] &quot;Many&quot; &quot;thanks&quot; ## [3] &quot;to&quot; &quot;everyone&quot; ## [5] &quot;involved&quot; &quot;in&quot; ## [7] &quot;Alexandria&#39;s&quot; &quot;#EarthDay&quot; ## [9] &quot;celebration.&quot; &quot;Together,&quot; ## [11] &quot;we&quot; &quot;are&quot; ## [13] &quot;reducing&quot; &quot;pollution&quot; ## [15] &quot;and&quot; &quot;was…&quot; ## [17] &quot;https://t.co/wwMOIg4t1w&quot; str_c(words) ## [1] &quot;c(\\&quot;Many\\&quot;, \\&quot;thanks\\&quot;, \\&quot;to\\&quot;, \\&quot;everyone\\&quot;, \\&quot;involved\\&quot;, \\&quot;in\\&quot;, \\&quot;Alexandria&#39;s\\&quot;, \\&quot;#EarthDay\\&quot;, \\&quot;celebration.\\&quot;, \\&quot;Together,\\&quot;, \\&quot;we\\&quot;, \\&quot;are\\&quot;, \\&quot;reducing\\&quot;, \\&quot;pollution\\&quot;, \\&quot;and\\&quot;, \\&quot;was…\\&quot;, \\&quot;https://t.co/wwMOIg4t1w\\&quot;)&quot; str_c(unlist(words)) ## [1] &quot;Many&quot; &quot;thanks&quot; ## [3] &quot;to&quot; &quot;everyone&quot; ## [5] &quot;involved&quot; &quot;in&quot; ## [7] &quot;Alexandria&#39;s&quot; &quot;#EarthDay&quot; ## [9] &quot;celebration.&quot; &quot;Together,&quot; ## [11] &quot;we&quot; &quot;are&quot; ## [13] &quot;reducing&quot; &quot;pollution&quot; ## [15] &quot;and&quot; &quot;was…&quot; ## [17] &quot;https://t.co/wwMOIg4t1w&quot; str_c(unlist(words), &quot;Added&quot;) ## [1] &quot;ManyAdded&quot; &quot;thanksAdded&quot; ## [3] &quot;toAdded&quot; &quot;everyoneAdded&quot; ## [5] &quot;involvedAdded&quot; &quot;inAdded&quot; ## [7] &quot;Alexandria&#39;sAdded&quot; &quot;#EarthDayAdded&quot; ## [9] &quot;celebration.Added&quot; &quot;Together,Added&quot; ## [11] &quot;weAdded&quot; &quot;areAdded&quot; ## [13] &quot;reducingAdded&quot; &quot;pollutionAdded&quot; ## [15] &quot;andAdded&quot; &quot;was…Added&quot; ## [17] &quot;https://t.co/wwMOIg4t1wAdded&quot; str_c(unlist(words), &quot;Added&quot;, sep=&quot; &quot;) ## [1] &quot;Many Added&quot; &quot;thanks Added&quot; ## [3] &quot;to Added&quot; &quot;everyone Added&quot; ## [5] &quot;involved Added&quot; &quot;in Added&quot; ## [7] &quot;Alexandria&#39;s Added&quot; &quot;#EarthDay Added&quot; ## [9] &quot;celebration. Added&quot; &quot;Together, Added&quot; ## [11] &quot;we Added&quot; &quot;are Added&quot; ## [13] &quot;reducing Added&quot; &quot;pollution Added&quot; ## [15] &quot;and Added&quot; &quot;was… Added&quot; ## [17] &quot;https://t.co/wwMOIg4t1w Added&quot; str_c(&quot;Before&quot;, unlist(words), &quot;Added&quot;, sep=&quot; &quot;) ## [1] &quot;Before Many Added&quot; ## [2] &quot;Before thanks Added&quot; ## [3] &quot;Before to Added&quot; ## [4] &quot;Before everyone Added&quot; ## [5] &quot;Before involved Added&quot; ## [6] &quot;Before in Added&quot; ## [7] &quot;Before Alexandria&#39;s Added&quot; ## [8] &quot;Before #EarthDay Added&quot; ## [9] &quot;Before celebration. Added&quot; ## [10] &quot;Before Together, Added&quot; ## [11] &quot;Before we Added&quot; ## [12] &quot;Before are Added&quot; ## [13] &quot;Before reducing Added&quot; ## [14] &quot;Before pollution Added&quot; ## [15] &quot;Before and Added&quot; ## [16] &quot;Before was… Added&quot; ## [17] &quot;Before https://t.co/wwMOIg4t1w Added&quot; Or you may want to assemble it all into all back into a coherent whole str_c(unlist(words), collapse=&quot; &quot;) ## [1] &quot;Many thanks to everyone involved in Alexandria&#39;s #EarthDay celebration. Together, we are reducing pollution and was… https://t.co/wwMOIg4t1w&quot; 8.1.1 Things you haven’t thought about strings Strings in memory are different than how they appear on the screen You can use the function writeLines to see how a function would atually render to a reader (as opposed to how it is stored in the computer.) The big confusion is that special characters have to be “escaped” or else they get in the way (From R4DS) x &lt;- c(&quot;\\&quot;&quot;, &quot;\\\\&quot;) x ## [1] &quot;\\&quot;&quot; &quot;\\\\&quot; writeLines(x) ## &quot; ## \\ Others to watch out for are \"\\n\" and \"\\t\" for next line and tab na is a problem, so you might use str_replace_na stringr replicates a lot of the functions we have already used for characters with (sometimes) slightly different syntax colors &lt;- c(&quot;red&quot;, &quot;yellow&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;magenta&quot;, &quot;cyan&quot;) str_sub(colors, 1, 3) ## [1] &quot;red&quot; &quot;yel&quot; &quot;blu&quot; &quot;gre&quot; &quot;mag&quot; &quot;cya&quot; str_sub(colors, -3, -1) #count back from the end ## [1] &quot;red&quot; &quot;low&quot; &quot;lue&quot; &quot;een&quot; &quot;nta&quot; &quot;yan&quot; str_sub(colors, 1, 10) # Robust to going out of index ## [1] &quot;red&quot; &quot;yellow&quot; &quot;blue&quot; &quot;green&quot; &quot;magenta&quot; &quot;cyan&quot; str_sub(colors, 1, 3)&lt;-str_to_upper(str_sub(colors, 1, 3)) colors ## [1] &quot;RED&quot; &quot;YELlow&quot; &quot;BLUe&quot; &quot;GREen&quot; &quot;MAGenta&quot; &quot;CYAn&quot; colors&lt;-str_to_lower(colors) colors ## [1] &quot;red&quot; &quot;yellow&quot; &quot;blue&quot; &quot;green&quot; &quot;magenta&quot; &quot;cyan&quot; Example: Filter down to all of the tweets from Mayor Lyda Krewson in our data Find the mean number of words in her tweets. Find the total. Take all of the words she has used and reduce them down to only their first five letters. Repeat (2) and compare. Question 2 hertweets &lt;- filter(tweets, ScreenName == &quot;lydakrewson&quot;)[&quot;Text&quot;] numWords &lt;- hertweets %&gt;% rowwise() %&gt;% transmute(numWords= length(unlist(str_split(Text, &quot; &quot;)))) numWords ## Source: local data frame [3,191 x 1] ## Groups: &lt;by row&gt; ## ## # A tibble: 3,191 x 1 ## numWords ## &lt;int&gt; ## 1 20 ## 2 25 ## 3 20 ## 4 20 ## 5 17 ## 6 21 ## 7 8 ## 8 11 ## 9 19 ## 10 13 ## # ... with 3,181 more rows mean(numWords$numWords) ## [1] 16.58602 length(unique(unlist(str_split(hertweets$Text, &quot; &quot;)))) ## [1] 15960 Question 3 length(unique(str_sub(unlist(str_split(hertweets$Text, &quot; &quot;)), 1, 5))) ## [1] 9831 "],
["section-functions-and-packages.html", "9 Functions and Packages 9.1 apply and its friends 9.2 plyr", " 9 Functions and Packages 9.1 apply and its friends 9.1.1 apply apply is a fundamental function in R and this family can be dead useful in many situations apply(array, margin, function , ...) An array (including potentially a matrix) The margin argument controls how each matrix is analyzed. Should the function execute on each row (margin = 1) or each column (margin = 2)? The function is what you want done on each row/column/whatever. IMPORTANTLY, the … refers to any arguments you want to pass to the function mat1 &lt;- matrix(rep(seq(4), 4), ncol = 4) mat1 ## [,1] [,2] [,3] [,4] ## [1,] 1 1 1 1 ## [2,] 2 2 2 2 ## [3,] 3 3 3 3 ## [4,] 4 4 4 4 Let’s calculate the sum of each row apply(mat1, 1, sum) ## [1] 4 8 12 16 Now the sum of each column apply(mat1, 2, sum) ## [1] 10 10 10 10 Usefully, we are not constrained to the functions that already exist in R. We can write our own functions. #using a user defined function sum.plus.2 &lt;- function(x){ sum(x) + 2 } Now we can use that function on the rows of our matrix apply(mat1, 1, sum.plus.2) ## [1] 6 10 14 18 We can generalize this to add some generic number to our sum. And we can also create anonymous functions that are never assigned into memory What’s going on here? apply(mat1, 1, function(x, y) sum(x) + y, y=3) ## [1] 7 11 15 19 And here? apply(mat1, 2, function(x, y) sum(x) + y, y=5) ## [1] 15 15 15 15 9.1.2 lapply There are several related functions in R that work the same but with different inputs. lapply(list, function, ...) Many functions in R produce lists and even dataframes are related to lists. mat1.df &lt;- data.frame(mat1) mat1.df ## X1 X2 X3 X4 ## 1 1 1 1 1 ## 2 2 2 2 2 ## 3 3 3 3 3 ## 4 4 4 4 4 is.list(mat1.df) ## [1] TRUE So lapply can help you work withs lists and data.frames lapply(mat1.df, sum) ## $X1 ## [1] 10 ## ## $X2 ## [1] 10 ## ## $X3 ## [1] 10 ## ## $X4 ## [1] 10 Note that the output of this is a list Another useful application of the lapply function is with a “dummy sequence”. The list argument is the dummy sequence and it is only used to specify how many iterations we would like to have the function executed. When the lapply functions is used in this way it can replace a for loop very easily, although often the map functions discussed last class are better for this. unlist(lapply(1:5, function(i) 5+i )) ## [1] 6 7 8 9 10 9.1.3 sapply This is a “simplified” version of lapply The key difference is that it changes hat kind of object is returned depending on what the outcomes look like. If the output is a scalar, the result is a vector If the output is all vectors of the same length, it will return a matrix sapply(list, function, ..., simplify) sapply(mat1.df, function(x, y) sum(x) + y, y = 5) ## X1 X2 X3 X4 ## 15 15 15 15 This is a vector, not a list 9.1.4 tapply This is less intuitive, but can be very useful for recoding tasks, handling data, etc. The key here is to understand that the “indices” here are the values of some other object. tapply(array, indicies, function, ..., simplify) x1 &lt;- runif(16) x1 ## [1] 0.86272839 0.33303542 0.43787677 0.73891511 0.64758089 0.01598454 ## [7] 0.21143839 0.51353548 0.94328719 0.14927398 0.43405366 0.02427562 ## [13] 0.01928115 0.11707327 0.62745750 0.34742038 cat1 &lt;- rep(1:4, 4) cat1 ## [1] 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 cat2 &lt;- c(rep(1, 8), rep(2, 8)) cat2 ## [1] 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 mat2.df &lt;- data.frame(x1) names(mat2.df) &lt;- c(&quot;x1&quot;) mat2.df$cat1 &lt;- cat1 mat2.df$cat2 &lt;- cat2 mat2.df ## x1 cat1 cat2 ## 1 0.86272839 1 1 ## 2 0.33303542 2 1 ## 3 0.43787677 3 1 ## 4 0.73891511 4 1 ## 5 0.64758089 1 1 ## 6 0.01598454 2 1 ## 7 0.21143839 3 1 ## 8 0.51353548 4 1 ## 9 0.94328719 1 2 ## 10 0.14927398 2 2 ## 11 0.43405366 3 2 ## 12 0.02427562 4 2 ## 13 0.01928115 1 2 ## 14 0.11707327 2 2 ## 15 0.62745750 3 2 ## 16 0.34742038 4 2 tapply(mat2.df$x1, INDEX = mat2.df$cat1, FUN=mean) ## 1 2 3 4 ## 0.6182194 0.1538418 0.4277066 0.4060366 And you can do this for combinations of values of variables tapply(mat2.df$x1, list(mat2.df$cat1, mat2.df$cat2), mean) ## 1 2 ## 1 0.7551546 0.4812842 ## 2 0.1745100 0.1331736 ## 3 0.3246576 0.5307556 ## 4 0.6262253 0.1858480 The first cell here is equivalent to: mean(mat2.df$x1[mat2.df$cat1==1 &amp; mat2.df$cat2==1]) ## [1] 0.7551546 9.1.5 sweep sweep(array, margin, stats, function, ...) This is used if you want, for instance, mean center your variables The name is horrible, but the idea is that you can alter each variable as a function of the variable. Easiest to understand by example. a &lt;- matrix(runif(100, 1, 2),20) a.df &lt;- data.frame(a) a ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1.791789 1.525150 1.267328 1.031245 1.807508 ## [2,] 1.722064 1.489433 1.448136 1.309382 1.420755 ## [3,] 1.997913 1.534616 1.773443 1.252746 1.560498 ## [4,] 1.426613 1.122802 1.132289 1.265866 1.387100 ## [5,] 1.924214 1.623027 1.077649 1.308449 1.126530 ## [6,] 1.730206 1.467400 1.435545 1.548422 1.960376 ## [7,] 1.294378 1.715798 1.863764 1.540219 1.649977 ## [8,] 1.153316 1.325771 1.167934 1.822147 1.084916 ## [9,] 1.520004 1.885161 1.748147 1.820758 1.014040 ## [10,] 1.523182 1.702126 1.380347 1.741188 1.461889 ## [11,] 1.072426 1.829897 1.857807 1.702611 1.240305 ## [12,] 1.471030 1.675648 1.428410 1.600730 1.326087 ## [13,] 1.313345 1.884099 1.244843 1.186602 1.511841 ## [14,] 1.693878 1.036697 1.849837 1.276845 1.864935 ## [15,] 1.823625 1.637818 1.901927 1.363731 1.390139 ## [16,] 1.126228 1.683903 1.088091 1.268413 1.289698 ## [17,] 1.143241 1.513913 1.258479 1.850939 1.078180 ## [18,] 1.710062 1.545605 1.624961 1.335398 1.940909 ## [19,] 1.478986 1.392595 1.989571 1.982486 1.925519 ## [20,] 1.773401 1.478046 1.197451 1.871149 1.934529 Now let’s subtract the mean from each column a1 &lt;- sweep(a, 2, colMeans(a), &quot;-&quot;) a1[1:5,] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.2572939 -0.02832551 -0.21947007 -0.4727211 0.30872160 ## [2,] 0.1875687 -0.06404213 -0.03866164 -0.1945843 -0.07803156 ## [3,] 0.4634175 -0.01885935 0.28664461 -0.2512201 0.06171170 ## [4,] -0.1078825 -0.43067354 -0.35450891 -0.2381007 -0.11168617 ## [5,] 0.3897186 0.06955155 -0.40914863 -0.1955171 -0.37225700 colMeans(a1) ## column means are all now about zero ## [1] 8.881784e-17 -8.881784e-17 -4.440892e-17 4.440892e-17 8.881784e-17 9.1.6 by by(data, INDICES, FUN, ..., simplify = TRUE) by is a wrapper for tapply that is supposed to make it easier to use tapply(mat2.df$x1, INDEX = mat2.df$cat1, FUN=mean) ## 1 2 3 4 ## 0.6182194 0.1538418 0.4277066 0.4060366 byOut &lt;- by(data=mat2.df$x1, INDICES=mat2.df$cat1, mean) byOut ## mat2.df$cat1: 1 ## [1] 0.6182194 ## ------------------------------------------------------------ ## mat2.df$cat1: 2 ## [1] 0.1538418 ## ------------------------------------------------------------ ## mat2.df$cat1: 3 ## [1] 0.4277066 ## ------------------------------------------------------------ ## mat2.df$cat1: 4 ## [1] 0.4060366 class(byOut) ## [1] &quot;by&quot; 9.1.7 vapply And more Base R comes with a variety of related functions that are a variety on a theme vapply is mucht the same except The argument FUN.VALUE provides a template for what the output should look like vapply(X, FUN, FUN.VALUE, ..., USE.NAMES = TRUE) l &lt;- list(a = 1:10, b = 11:20) l ## $a ## [1] 1 2 3 4 5 6 7 8 9 10 ## ## $b ## [1] 11 12 13 14 15 16 17 18 19 20 l.fivenum &lt;- vapply(X=l, FUN=fivenum, FUN.VALUE=c(Min.=0, &quot;1st Qu.&quot;=0, Median=0, &quot;3rd Qu.&quot;=0, Max.=0)) class(l.fivenum) ## [1] &quot;matrix&quot; &quot;array&quot; l.fivenum ## a b ## Min. 1.0 11.0 ## 1st Qu. 3.0 13.0 ## Median 5.5 15.5 ## 3rd Qu. 8.0 18.0 ## Max. 10.0 20.0 Other options include: - replicate, which executes the same function multiple times - mapply, which is a multivariate version of sapply - rapply, which allows for final handling of outputs - And more, including: + ave + colMeans + rowSums + aggregate + eapply 9.2 plyr The plyr package is designed to make all of this a bit easier to handle by adding the following features Consistent naming protocols for functions in terms of what is going in and coming out Easy to make parallel Built-in error recovery Better handling of labels Flesible handling of all basic data types This content taken from: https://www.r-bloggers.com/a-fast-intro-to-plyr-for-r/ library(plyr) ## ------------------------------------------------------------------------------ ## You have loaded plyr after dplyr - this is likely to cause problems. ## If you need functions from both plyr and dplyr, please load plyr first, then dplyr: ## library(plyr); library(dplyr) ## ------------------------------------------------------------------------------ ## ## Attaching package: &#39;plyr&#39; ## The following object is masked from &#39;package:faraway&#39;: ## ## ozone ## The following object is masked from &#39;package:purrr&#39;: ## ## compact ## The following objects are masked from &#39;package:dplyr&#39;: ## ## arrange, count, desc, failwith, id, mutate, rename, summarise, ## summarize Let’s make some example data dd&lt;-data.frame(matrix(rnorm(216),72,3),c(rep(&quot;A&quot;,24),rep(&quot;B&quot;,24),rep(&quot;C&quot;,24)),c(rep(&quot;J&quot;,36),rep(&quot;K&quot;,36))) colnames(dd) &lt;- c(&quot;v1&quot;, &quot;v2&quot;, &quot;v3&quot;, &quot;dim1&quot;, &quot;dim2&quot;) head(dd) ## v1 v2 v3 dim1 dim2 ## 1 0.8551940 0.3695775 0.05056537 A J ## 2 1.4588252 1.8064429 0.48527538 A J ## 3 0.1919591 0.4057030 1.17563859 A J ## 4 -0.4741837 -0.2961076 1.07583570 A J ## 5 -1.0205107 0.9330929 -0.15182294 A J ## 6 0.4337721 -0.1848640 -0.53081654 A J dd ## v1 v2 v3 dim1 dim2 ## 1 0.85519402 0.369577530 0.05056537 A J ## 2 1.45882515 1.806442926 0.48527538 A J ## 3 0.19195913 0.405702982 1.17563859 A J ## 4 -0.47418369 -0.296107567 1.07583570 A J ## 5 -1.02051067 0.933092930 -0.15182294 A J ## 6 0.43377212 -0.184864044 -0.53081654 A J ## 7 0.23159718 0.394476121 1.43322580 A J ## 8 -0.26324262 0.520440013 -0.59459369 A J ## 9 0.65168052 0.507995243 -0.65237095 A J ## 10 0.01158674 -0.972585282 -0.35773989 A J ## 11 0.22979067 0.893129826 -0.95733438 A J ## 12 -1.33530237 -0.615824369 -1.35199195 A J ## 13 -0.60099411 -0.382353835 -0.87411642 A J ## 14 -0.98330221 0.267899031 -0.17447483 A J ## 15 0.88356660 0.007036114 0.07901021 A J ## 16 -1.69499305 -0.065129940 -0.49541043 A J ## 17 -0.77913575 -1.377300052 -0.53318122 A J ## 18 -1.64228902 -0.963799735 0.18830207 A J ## 19 1.71105639 -1.933174725 1.32831880 A J ## 20 0.29732451 -1.453190231 -0.14990998 A J ## 21 1.03974471 -2.404054161 -1.62341270 A J ## 22 -0.19648730 -1.568305935 0.63239991 A J ## 23 -0.58138537 1.460733255 -0.17307378 A J ## 24 -0.52678596 -0.461033593 -0.11266158 A J ## 25 -1.12251290 0.143158133 0.11916201 B J ## 26 1.86475127 -1.338656092 0.53447925 B J ## 27 0.11635489 0.976720967 -0.12009826 B J ## 28 -1.06696466 -0.535537879 0.93410476 B J ## 29 -1.32621060 -1.669410100 1.16966031 B J ## 30 0.77484365 1.066130764 1.60058895 B J ## 31 3.73460137 -0.558471186 -1.19942065 B J ## 32 1.48132217 -1.001459826 1.14816285 B J ## 33 0.21613434 0.502681521 -0.62496678 B J ## 34 1.46135600 0.254098377 0.51648806 B J ## 35 -1.61791023 -1.023298584 0.63098358 B J ## 36 -0.78905699 -0.448320771 0.71385889 B J ## 37 -0.56351760 0.788019125 -0.33446140 B K ## 38 0.12331564 -1.265657614 -0.60389219 B K ## 39 0.45271832 -1.112659723 1.22022888 B K ## 40 -1.25525956 -0.046051603 -0.86896663 B K ## 41 1.05069185 -0.492566107 0.46031006 B K ## 42 -0.81745238 -1.197839054 -0.83174925 B K ## 43 -0.81346209 0.068413328 0.35531180 B K ## 44 -0.59279583 -1.158998300 -0.64495423 B K ## 45 -0.67273749 -0.128248266 0.66503425 B K ## 46 -0.55598129 -0.395669265 1.20721801 B K ## 47 0.85530651 0.634460688 -3.60633570 B K ## 48 -0.27297623 0.632905650 -0.57061948 B K ## 49 1.27315985 0.254025035 -0.90188016 C K ## 50 0.31188629 -0.015156662 -1.69973484 C K ## 51 0.14027612 -0.660252853 0.51326000 C K ## 52 0.24934190 0.258645715 0.30379182 C K ## 53 0.43379777 0.386684665 -0.21873901 C K ## 54 0.41394704 -0.429830970 -0.22652099 C K ## 55 -0.27778727 2.306581562 0.70927160 C K ## 56 2.02145979 1.255819064 0.93764571 C K ## 57 1.22263553 -0.492327739 -0.30218147 C K ## 58 0.36030492 -1.550914637 -0.85492192 C K ## 59 -0.42617645 -1.488111032 -0.66216164 C K ## 60 -1.34348788 -0.040394616 -1.27487745 C K ## 61 0.85107320 -0.854518534 0.04662172 C K ## 62 1.34090136 1.617514774 1.07328068 C K ## 63 -0.27188139 -1.769276040 1.45100843 C K ## 64 0.50574607 1.046174513 0.39715848 C K ## 65 -0.48429148 0.698995796 -0.48980961 C K ## 66 -0.06553340 -2.292778744 0.46208850 C K ## 67 -0.92143855 -0.561272034 -2.50548177 C K ## 68 1.79738428 0.152093596 -0.12909190 C K ## 69 0.83446135 -1.301686200 0.78992968 C K ## 70 -0.50058522 0.147775294 -0.52377153 C K ## 71 0.83023011 0.223246654 1.07183317 C K ## 72 -1.28033291 0.002926967 0.30381001 C K The main functions we want to use are a_ply, aaply, adply, alply, d_ply, daply, ddply, dlply, l_ply, laply, llply, m_ply, maply, mdply, mlply The first letter for each tells us what kind of input we have a=array d = data.frame l=list m=matrix The second letter tells us what we want in terms of output a=array d=data.frame l=list m=matrix _=discard the results Example: ddply(.data=dd, .variables=c(&quot;dim1&quot;,&quot;dim2&quot;), .fun=function(df) mean(df$v1)) ## dim1 dim2 V1 ## 1 A J -0.08760477 ## 2 B J 0.31055903 ## 3 B K -0.25517918 ## 4 C K 0.29229546 daply(.data=dd, .variables=c(&quot;dim1&quot;,&quot;dim2&quot;), .fun=function(df)mean(df$v1)) ## dim2 ## dim1 J K ## A -0.08760477 NA ## B 0.31055903 -0.2551792 ## C NA 0.2922955 Most combinations we want are here l_ply(1:100, identity) llply(1:100, identity) ## [[1]] ## [1] 1 ## ## [[2]] ## [1] 2 ## ## [[3]] ## [1] 3 ## ## [[4]] ## [1] 4 ## ## [[5]] ## [1] 5 ## ## [[6]] ## [1] 6 ## ## [[7]] ## [1] 7 ## ## [[8]] ## [1] 8 ## ## [[9]] ## [1] 9 ## ## [[10]] ## [1] 10 ## ## [[11]] ## [1] 11 ## ## [[12]] ## [1] 12 ## ## [[13]] ## [1] 13 ## ## [[14]] ## [1] 14 ## ## [[15]] ## [1] 15 ## ## [[16]] ## [1] 16 ## ## [[17]] ## [1] 17 ## ## [[18]] ## [1] 18 ## ## [[19]] ## [1] 19 ## ## [[20]] ## [1] 20 ## ## [[21]] ## [1] 21 ## ## [[22]] ## [1] 22 ## ## [[23]] ## [1] 23 ## ## [[24]] ## [1] 24 ## ## [[25]] ## [1] 25 ## ## [[26]] ## [1] 26 ## ## [[27]] ## [1] 27 ## ## [[28]] ## [1] 28 ## ## [[29]] ## [1] 29 ## ## [[30]] ## [1] 30 ## ## [[31]] ## [1] 31 ## ## [[32]] ## [1] 32 ## ## [[33]] ## [1] 33 ## ## [[34]] ## [1] 34 ## ## [[35]] ## [1] 35 ## ## [[36]] ## [1] 36 ## ## [[37]] ## [1] 37 ## ## [[38]] ## [1] 38 ## ## [[39]] ## [1] 39 ## ## [[40]] ## [1] 40 ## ## [[41]] ## [1] 41 ## ## [[42]] ## [1] 42 ## ## [[43]] ## [1] 43 ## ## [[44]] ## [1] 44 ## ## [[45]] ## [1] 45 ## ## [[46]] ## [1] 46 ## ## [[47]] ## [1] 47 ## ## [[48]] ## [1] 48 ## ## [[49]] ## [1] 49 ## ## [[50]] ## [1] 50 ## ## [[51]] ## [1] 51 ## ## [[52]] ## [1] 52 ## ## [[53]] ## [1] 53 ## ## [[54]] ## [1] 54 ## ## [[55]] ## [1] 55 ## ## [[56]] ## [1] 56 ## ## [[57]] ## [1] 57 ## ## [[58]] ## [1] 58 ## ## [[59]] ## [1] 59 ## ## [[60]] ## [1] 60 ## ## [[61]] ## [1] 61 ## ## [[62]] ## [1] 62 ## ## [[63]] ## [1] 63 ## ## [[64]] ## [1] 64 ## ## [[65]] ## [1] 65 ## ## [[66]] ## [1] 66 ## ## [[67]] ## [1] 67 ## ## [[68]] ## [1] 68 ## ## [[69]] ## [1] 69 ## ## [[70]] ## [1] 70 ## ## [[71]] ## [1] 71 ## ## [[72]] ## [1] 72 ## ## [[73]] ## [1] 73 ## ## [[74]] ## [1] 74 ## ## [[75]] ## [1] 75 ## ## [[76]] ## [1] 76 ## ## [[77]] ## [1] 77 ## ## [[78]] ## [1] 78 ## ## [[79]] ## [1] 79 ## ## [[80]] ## [1] 80 ## ## [[81]] ## [1] 81 ## ## [[82]] ## [1] 82 ## ## [[83]] ## [1] 83 ## ## [[84]] ## [1] 84 ## ## [[85]] ## [1] 85 ## ## [[86]] ## [1] 86 ## ## [[87]] ## [1] 87 ## ## [[88]] ## [1] 88 ## ## [[89]] ## [1] 89 ## ## [[90]] ## [1] 90 ## ## [[91]] ## [1] 91 ## ## [[92]] ## [1] 92 ## ## [[93]] ## [1] 93 ## ## [[94]] ## [1] 94 ## ## [[95]] ## [1] 95 ## ## [[96]] ## [1] 96 ## ## [[97]] ## [1] 97 ## ## [[98]] ## [1] 98 ## ## [[99]] ## [1] 99 ## ## [[100]] ## [1] 100 laply(1:100, identity) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## [19] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## [37] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## [55] 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 ## [73] 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 ## [91] 91 92 93 94 95 96 97 98 99 100 ldply(1:100, identity) ## V1 ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 ## 6 6 ## 7 7 ## 8 8 ## 9 9 ## 10 10 ## 11 11 ## 12 12 ## 13 13 ## 14 14 ## 15 15 ## 16 16 ## 17 17 ## 18 18 ## 19 19 ## 20 20 ## 21 21 ## 22 22 ## 23 23 ## 24 24 ## 25 25 ## 26 26 ## 27 27 ## 28 28 ## 29 29 ## 30 30 ## 31 31 ## 32 32 ## 33 33 ## 34 34 ## 35 35 ## 36 36 ## 37 37 ## 38 38 ## 39 39 ## 40 40 ## 41 41 ## 42 42 ## 43 43 ## 44 44 ## 45 45 ## 46 46 ## 47 47 ## 48 48 ## 49 49 ## 50 50 ## 51 51 ## 52 52 ## 53 53 ## 54 54 ## 55 55 ## 56 56 ## 57 57 ## 58 58 ## 59 59 ## 60 60 ## 61 61 ## 62 62 ## 63 63 ## 64 64 ## 65 65 ## 66 66 ## 67 67 ## 68 68 ## 69 69 ## 70 70 ## 71 71 ## 72 72 ## 73 73 ## 74 74 ## 75 75 ## 76 76 ## 77 77 ## 78 78 ## 79 79 ## 80 80 ## 81 81 ## 82 82 ## 83 83 ## 84 84 ## 85 85 ## 86 86 ## 87 87 ## 88 88 ## 89 89 ## 90 90 ## 91 91 ## 92 92 ## 93 93 ## 94 94 ## 95 95 ## 96 96 ## 97 97 ## 98 98 ## 99 99 ## 100 100 "],
["section-using-sql-syntax-on-an-r-dataframe.html", "10 Using SQL syntax on an R dataframe 10.1 SQL 10.2 The sqldf package", " 10 Using SQL syntax on an R dataframe 10.1 SQL SQL (Structured Query Language) is the universally most widely used querying language for databases. There are variations of SQL, the most popular being MySQL and PostgreSQL. The syntax is very similar for all versions of SQL. Here’s a helpful tutorial for SQl commands, but a few basic examples of each keyword are shown below: 10.1.1 SELECT and FROM SELECT col1, col2, col3 FROM table1 SELECT * FROM table1 The first example selects the three columns specified from a table called table1. It will select all rows from table1 and return them in a new, temporary table unless the result is stored or used somewhere examples The second example selects all columns (and rows) from table1. 10.1.2 WHERE SELECT movie_title, user, rating FROM movies WHERE rating &gt; 4 AND user NOT NULL This will select all of the rating data (movie_title, user, and rating) from the movies table with a rating &gt; 4 and a not-null user 10.1.3 Functions We can use aggregate functions like sum, min, max, count, or average with group by to apply that function to each group. Remember that your group by variable must be included in your select statement! -- valid SELECT movie_title, AVG(rating) FROM movies GROUP BY movie_title -- invalid (we wouldn&#39;t know which avg goes with which movie!!) SELECT AVG(rating) FROM movies GROUP BY movie_title We can also specify what we want the resulting aggregate column to be called. The above example would give us nicer output with this code: SELECT movie_title, AVG(rating) AS rating_avg FROM movies GROUP BY movie_title We can limit the number of results returned, find only unique values, and sort our results as well. SELECT DISTINCT student_id, student_full_name FROM assignments ORDER BY student_full_name DESC LIMIT 100 This query will grab the first 100 distinct students (uniqueness determined by id and full name) sorted in reverse alphabetical order. 10.1.4 HAVING HAVING is the equivalent function of WHERE for aggregate conditions. For example, SELECT movie_title, AVG(rating) AS rating_avg FROM movies GROUP BY movie_title HAVING rating_avg &lt; 2 This pulls all the movies and their average ratings that had an average rating less than 2. 10.1.5 WITH/AS and Subqueries Subqueries are very powerful and come in two syntatical forms. They allow us to create temporary tables which only exist for that query then disappear. SELECT employee_id, first_name, last_name, salary FROM employees WHERE salary = ( SELECT MAX(salary) FROM employees ) ORDER BY first_name, last_name This syntax is equivalent to the following: WITH temp_table AS (SELECT MAX(salary) FROM employees) SELECT employee_id, first_name, last_name, salary ORDER BY first_name, last_name 10.1.6 Joins Joins work the same way as they did in the tidyverse. There are a few types of joins: “Inner joins” produce results only where the linking variable appears on both tables “Outer joins” produce results with all data, and fills in missing values as necessary Left join keeps only observations from the first specified data if their is no match. Right join keeps only observations from the second data Full join keeps everything Here’s an example SQL query using two tables (movies, users) SELECT users.*, count(movie_id) FROM users LEFT JOIN movies ON users.user_id = movies.viewer_id GROUP BY users.user_id LIMIT 10 This will select the first 10 users from the users table (users.*) and add a new column with the count of the number of movies they saw. If both tables had a field called user_id that we wanted to join on, we could say LEFT JOIN movies ON user_id instead of specifying what the column is called in each table. If the column name is ambiguous (because it’s present in both tables), a GROUP BY or ORDER BY or SELECT should specify which table to take the column from by saying table.column 10.2 The sqldf package The main package we’ll be using to convert SQL commands to R manipulations is sqldf. Here’s an example using the same data from the Tidy chapter. Data comes from fivethirtyeight.com library(dplyr) library(tidyr) library(readr) library(tidyverse) mayors&lt;-read_csv(&quot;../Datasets/Mayors.csv&quot;) tweets&lt;-read_csv(&quot;C:/Users/maria/OneDrive/WUSTL/2019-20/PDS/Tweets.csv&quot;) mayors and tweets are tibbles, but we want to operate our SQL queries on data frames mayorsDF &lt;- as.data.frame(mayors) tweetsDF &lt;- as.data.frame(tweets) Now we can start using our newfound SQL powers to truly explore our data. For a warm-up, let’s count how many mayors are present in our dataset. library(sqldf) sqldf( &quot;SELECT COUNT(DISTINCT FullName) FROM mayorsDF&quot; ) ## COUNT(DISTINCT FullName) ## 1 1406 We can confirm this with our tidy skills: mayors %&gt;% select(&quot;FullName&quot;) %&gt;% filter(!is.na(FullName)) %&gt;% n_distinct() ## [1] 1406 Note that we need to filter out NA/null values from the dataset before taking unique values in tidy. Tidy counts null as a unique value. How about adding a column to the mayors dataset that counts the number of tweets they have made? With SQL: mayors_newDF &lt;- sqldf( &quot;SELECT mayorsDF.*, COUNT(TweetID) AS nTweets FROM mayorsDF INNER JOIN tweetsDF ON mayorsDF.TwitterHandle = tweetsDF.ScreenName GROUP BY MayorID ORDER BY nTweets DESC, FullName ASC&quot; ) head(mayors_newDF[c(&quot;MayorID&quot;, &quot;FullName&quot;, &quot;TwitterHandle&quot;, &quot;nTweets&quot;)], n=10) ## MayorID FullName TwitterHandle nTweets ## 1 902 Derek Armstead derek_armstead 3200 ## 2 1143 Ed Pawlowski ed_pawlowski 3200 ## 3 778 Emily Larson larsonforduluth 3200 ## 4 1071 Greg Peterson dublinohio 3200 ## 5 812 Jason L. Shelton tupmayorsoffice 3200 ## 6 1141 Jim Kenney phillymayor 3200 ## 7 498 Rahm Emmanuel chicagosmayor 3200 ## 8 443 Thomas A. Masters mayormasters 3200 ## 9 11 William Stimpson mayorstimpson 3200 ## 10 715 Andy Schor andyschor 3199 With tidy: tweetCounts &lt;- tweets %&gt;% filter(!is.na(ScreenName)) %&gt;% group_by(ScreenName) %&gt;% tally() mayors_new &lt;- mayors %&gt;% inner_join(tweetCounts, by=c(&quot;TwitterHandle&quot;=&quot;ScreenName&quot;)) %&gt;% group_by(MayorID) %&gt;% arrange(FullName) %&gt;% arrange(desc(n)) %&gt;% select(&quot;MayorID&quot;, &quot;FullName&quot;, &quot;TwitterHandle&quot;, &quot;n&quot;) mayors_new ## # A tibble: 572 x 4 ## # Groups: MayorID [572] ## MayorID FullName TwitterHandle n ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 902 Derek Armstead derek_armstead 3200 ## 2 1143 Ed Pawlowski ed_pawlowski 3200 ## 3 778 Emily Larson larsonforduluth 3200 ## 4 1071 Greg Peterson dublinohio 3200 ## 5 812 Jason L. Shelton tupmayorsoffice 3200 ## 6 1141 Jim Kenney phillymayor 3200 ## 7 498 Rahm Emmanuel chicagosmayor 3200 ## 8 443 Thomas A. Masters mayormasters 3200 ## 9 11 William Stimpson mayorstimpson 3200 ## 10 715 Andy Schor andyschor 3199 ## # ... with 562 more rows "],
["section-machine-learning.html", "11 Machine Learning 11.1 Supervised Learning 11.2 Classification 11.3 Putting that in action", " 11 Machine Learning 11.1 Supervised Learning 11.1.1 The Gist We want to build some function \\(f(X)\\) that explains/predicts/correlates with observed outcome \\(y\\). Note that the capital \\(X\\) denotes an entire dataset Typically we say that \\(X\\) is a n-by-d matrix, where n is the number of datapoints and d is the number of dimensions each datapoint has A function, \\(f(X)\\), can be something simple like a line or something more complex. We are going to use this function to: Predict \\(y\\) for a new \\(x_hat\\) Identify elements of \\(X\\) that seem important Explore interesting relationships Maybe for theory testing, but will need to be careful 11.1.2 What could go wrong? Problem 1: Infinity is a big number - The are an infinite number of potential functions, \\(f(\\cdot)\\). - We can’t try all possible functions. That problem isn’t clearly defined. Problem 2: Not enough data - Even if we knew a subset of \\(f(\\cdot)\\) to consider, we may not have enough data - If \\(f(\\cdot)\\) is complex, can be particularly hard to approximate unless large \\(n\\) Problem 3: What are the right features? - Even if we have some idea of \\(f(\\cdot)\\) and a lot of data, we don’t always know the right features to include. - And in some cases there are a lot of features. Problem 4: Is it signal or is it error? - A lot of outcomes we want to study are “noisy” - We are usually not interested in the noise - One way to think of this is that \\(f(\\cdot)\\) can be divided into two compoenents * Systematic component * Error component Example: The linear regression \\[f(X) = \\underbrace{\\beta_0 + \\beta_1x_1 + \\beta_2 x_2}_{systematic} + \\underbrace{\\epsilon}_{error}\\] \\[\\epsilon \\sim N(0, \\sigma^2)\\] Problem 5: Putting it all together We don’t know if we have the right “set” of functions to consider. Even if we did, we don’t have infinite data. And we don’t even know if we are using the right features. So we can’t ever be sure we are separating out the systematic and error portions. Problem 6: Meta problems - In many settings, the DGP is not static. - There may be unknown unknowns. - It is difficult or impossible to know if the data used to train your model is useful for the task at hand. 11.1.3 Example: Predicting presidential elections with vote share Today we are going to use the results of US presidential elections since 1948 library(tidyverse) electData&lt;-read.csv(&quot;http://politicaldatascience.com/PDS/Datasets/presElect.csv&quot;) Year: Year of the election q2gdp: GDP in the second quarter vote: Share of the two-party vote that went to the incumbent party. term: 1=Incumbent party has served more than one term; 0 = First term for incumbent party JuneApp: Approval as recorded in June prior to the election. Inc: Indicator if the incumbent party candidate is the current incumbent (meaning they are a first-term incument). 11.1.3.1 Linear regression We’ll use the lm function to generate a linear model Model1&lt;-lm(vote~q2gdp, data=electData) summary(Model1) ## ## Call: ## lm(formula = vote ~ q2gdp, data = electData) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.0116 -3.2421 0.1261 1.8912 8.4869 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 49.2711 1.3478 36.557 &lt; 2e-16 *** ## q2gdp 0.7536 0.2477 3.043 0.00775 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.339 on 16 degrees of freedom ## Multiple R-squared: 0.3666, Adjusted R-squared: 0.327 ## F-statistic: 9.259 on 1 and 16 DF, p-value: 0.007753 Inuition for the linear model \\[f(X) = \\underbrace{\\beta_0 + \\beta_1x_1 + \\beta_2 x_2}_{systematic} + \\underbrace{\\epsilon}_{error}\\] \\[\\epsilon \\sim N(0, \\sigma^2)\\] The “Multiple R-squared” is a fit statistic. Ranges from 0 to 1 Closer to 1 is better The “Estimate” is the coefficient The “Std. Error” is what we talked about in our previous lecture and is used to construct confidence intervals. Smaller p-values mean there is more evidence that that specific variable matters (sort of) 11.1.3.2 Prediction Let’s train our model using the data before 2016, then see what it predicts the vote will be for the 2016 election. electData$vote[electData$year==2016] ## [1] 50.5 Model2&lt;-lm(vote~q2gdp+JuneApp, data=electData[electData$year!=2016,]) predict(Model2, newdata=electData[electData$year==2016,]) ## 18 ## 50.88913 Pretty good! 11.1.3.3 Feature selection Model3&lt;-lm(vote~q2gdp+JuneApp, data=electData) summary(Model3) ## ## Call: ## lm(formula = vote ~ q2gdp + JuneApp, data = electData) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.3026 -1.6785 0.2673 1.1366 4.6499 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 49.34677 0.81828 60.305 &lt; 2e-16 *** ## q2gdp 0.45075 0.16072 2.805 0.0133 * ## JuneApp 0.14721 0.02761 5.331 8.39e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.634 on 15 degrees of freedom ## Multiple R-squared: 0.7812, Adjusted R-squared: 0.752 ## F-statistic: 26.77 on 2 and 15 DF, p-value: 1.124e-05 R-squared for the simpler model was 0.366 Adding June approval bumps it to 0.752 – way bigger What does that mean? The R-squared denotes the portion of the variation in the y value that is explained or predicted by the x values. Adding more predictors always increases \\(R^2\\). 11.1.3.4 Exploration Model4&lt;-lm(vote~JuneApp+Inc, data=electData) summary(Model4) ## ## Call: ## lm(formula = vote ~ JuneApp + Inc, data = electData) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.0471 -1.5652 -0.3451 1.2670 5.6167 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 49.47384 0.88618 55.828 &lt; 2e-16 *** ## JuneApp 0.14911 0.02963 5.033 0.000149 *** ## Inc 3.27994 1.43304 2.289 0.037016 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.8 on 15 degrees of freedom ## Multiple R-squared: 0.7528, Adjusted R-squared: 0.7198 ## F-statistic: 22.84 on 2 and 15 DF, p-value: 2.807e-05 So that’s interesting. When a party is running for a second term, they do better. Yes: Eisenhower, Kennedy, Nixon, Reagan, Clinton, Bush II, Obama No: Carter Might be worth looking into more. 11.1.3.5 Theory testing? Is this data by itself evidence that incumbent candidate is always at an advantage? Not really that convincing. Many other explanations. 11.1.4 The complexity of keeping it simple Simple models can be good, especially with small samples. But more complex models might be better: Maybe a more flexible \\(f(\\cdot)\\) than a line? Maybe more options for \\(X\\)? We want to aim for models that are: Complex enough to capture important aspects of reality Not so complex they confuse signal with noise 11.1.4.1 A new example library(readr) senateData&lt;-read_csv(&quot;http://politicaldatascience.com/PDS/Datasets/SenateForecast/CandidateLevel.csv&quot;) This is data on US Senate elections from 1992-2016. - VotePercentage: Percentage of the vote for that candidate - Republican: 1=Republican, 0=Any other - Democrat: 1=Democrat, 0=Any other - Experienced: 1=Candidate has held elected office, 0=otherwise - weightexperience: 1 = no experience, 4=held statewide office - pvi: Presidential vote index (Higher values mean more friendly to Democrats) - Generic Ballot: Generic ballot polling for that candidate’s party in that year - Incumbent: -1 = Running against incument, 0=open seat, 1 = Is the incumbent - PercentageRaised: Percent of money for that race raised by that candidate Model 1: Simple SimpleModelFull&lt;-lm(VotePercentage~pvi*Republican+Incumbent, data=senateData) summary(SimpleModelFull)$r.squared ## [1] 0.5895867 Model 2: Complex ComplexModelFull&lt;-lm(VotePercentage~pvi*Republican+weightexperience + GenericBallotSept*Republican + Incumbent, data=senateData) summary(ComplexModelFull)$r.squared ## [1] 0.6457033 Seems a little better So what’s that mean? Looks like the complex model is doing much better But is that real, or illusory? We can partially resolve that by doing a cross validation Several ways to do this, but here is a very easy one. Divide your data into two parts, training and validation Fit your model on your training data Test on your validation data (the data you didn’t use to fit the model) 11.1.4.2 Cross Validation library(rsample) split_senateData&lt;-initial_split(senateData, prop=.8) senate_train&lt;-training(split_senateData) senate_test&lt;-testing(split_senateData) Let’s look at those: dim(senate_train) ## [1] 665 14 dim(senate_test) ## [1] 165 14 Let’s test out the simple model SimpleModelTrain&lt;-lm(VotePercentage~pvi*Republican+Incumbent, data=senate_train) SimpleModelPredictions&lt;-predict(SimpleModelTrain, newdata=senate_test) Now we will calculate the root mean squared error (RMSE) comparing the predictions, \\(y^\\ast\\), with what we actually observed, \\(y\\). \\[RMSE=\\sqrt{\\frac{\\sum_i^n(y_i^\\ast-y_i)^2}{n}}\\] sqrt(mean((SimpleModelPredictions-senate_test$VotePercentage)^2)) ## [1] 6.947925 Let’s do the same for the more complex model Fit the model Make predictions for the training set ComplexModelTrain&lt;-lm(VotePercentage~pvi*Republican+weightexperience + GenericBallotSept*Republican + Incumbent, data=senate_train) ComplexModelPredictions&lt;-predict(ComplexModelTrain, newdata=senate_test) sqrt(mean((ComplexModelPredictions-senate_test$VotePercentage)^2)) ## [1] 6.512463 More on cross-validation One problem here is that the result may be somewhat sensitive to the particular way you partition your data. Maybe that 20% you pulled out were unusual? k-fold cross-validation tries to get around this by: Randomly dividing the data into k groups Each group serves as the test sample once So We have “out-of-sample” predictions for all cases You can also do Monte Carlo cross-validation, where you do this 90-10 random partitioning multiple times. Summary There is a tension between complexity and predictive accuracy More complex models may better explain the data you have, but may do worse in prediction. Cross validation is a fundamental tool for addressing this dilemma. 11.2 Classification 11.2.1 Classification basics: Binary outcomes The difference between regression and classification is that we look at a special type of function, \\(f(X)\\). We want a function that will: Take in a dataset \\(X\\) that can take on all kinds of values (binary, continuous, etc.) But it will “squash” all of these features so that \\(f(X) \\in [0, 1]\\). We then say that the probability that \\(y=1\\) is equal to \\(f(x)\\), or \\[Pr(y=1) = f(x)\\] Intuitively, we are imagining a weighted coin flip where the pobability of a “success” is determined by \\(f(x)\\). Motivating example: Turnout in the 2008 election Imagine we are trying to build a model to predict turnout (0 or 1) We have the following features: State Ethnicity (White/Black/Hispanic/Other) Age (Divided into quartiles) Income (Divided into quitiles) turnout&lt;-read.csv(&quot;http://politicaldatascience.com/PDS/Datasets/SimpleTurnout2008.csv&quot;) dim(turnout) ## [1] 73033 6 11.2.2 The linear classifier (AKA Logit) Logit is a member of a family of models called a “generalized linear model” These models have two basic parts: A linear component A squashing component The linear part: We might for instance, set up an equation of: \\[\\Lambda=\\beta_0 + \\beta_1 \\text{Income}+ \\beta_2 \\text{Age}\\] This is the same basic idea as normal regression models. But this linear combination can take on any value between \\(-\\infty\\) and \\(\\infty\\) (depending on what the \\(\\beta\\) values are) \\(\\beta_0 = 16\\) \\(\\beta_1=2\\) \\(\\beta_2 = 1.5\\) Income = 2 Age = 2 \\[23 = 16 + 2\\times 2 + 1.5\\times2\\] The squashing part: But we can’t use numbers like 23 to talk about turnout directly. And there is nothing keeping this from evaluating to -200 or 3,272 So the strategy will be to push \\(\\Lambda\\) through a squasher These are often shaped like an “s”, and are sometimes called a sigmoid. I am going to notate this as \\(\\sigma(\\cdot)\\) \\[f(X) = \\sigma(\\Lambda)\\] The logistic squasher: A very common choice for \\(\\sigma(\\cdot)\\) is the logistic function: \\[\\sigma(\\Lambda) = \\frac{1}{1+\\exp(-\\Lambda)} = \\frac{\\exp(\\Lambda)}{1+\\exp(\\Lambda)}\\] myLogistic&lt;-function(Lambda){ return(1/(1+exp(-1*Lambda))) } testValues&lt;-seq(-8, 8, by=.1) plot(testValues, myLogistic(testValues), type=&quot;l&quot;, ylab=&quot;Pred. Prob.&quot;) So let’s take a minute: Use the function from before and assume that: \\[\\Lambda = -2 + (1 \\times \\text{Income}) + (0.7\\times \\text{Age}) \\] What is \\(\\Lambda\\) when Income=2 and Age=4? Put that through our squasher and find out what the predicted probability will be. Do both calculations again but now assume Income=4 and Age=4 In general, Do the linear part first to get \\(\\Lambda\\). Then use the function myLogisticto calculate the predicted probability. 11.2.2.1 The linear classifier in theory Remember what we are trying to do: find some function \\(f(X)\\). We have a linear portion like \\(\\beta_1 \\text{Income}+ \\beta_2 \\text{Age}\\) We put it through a squasher like the logistic function on Slide 9. When we combine this we get: \\[Pr(y=1) = \\frac{\\exp(\\beta_0 + \\beta_1 \\text{Income}+ \\beta_2 \\text{Age})}{1+\\exp(\\beta_0 + \\beta_1 \\text{Income}+ \\beta_2 \\text{Age})} \\] But how do we figure out the \\(\\beta\\) coefficients? 11.2.2.2 The linear classifier in practice For this class we’ll use something maximum likelihood estimation. Basically, the computer will find the values of \\(\\beta\\) that minimize a specific form of loss All you really need to know is how to do it. We use the glm function. We specify that the data is binary by using the family=\"binomial\" argument. Model1&lt;-glm(turnout ~ inc + age, family=&quot;binomial&quot;, data=turnout) summary(Model1) ## ## Call: ## glm(formula = turnout ~ inc + age, family = &quot;binomial&quot;, data = turnout) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1708 -1.2979 0.6462 0.9014 1.3915 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.273471 0.030836 -41.30 &lt;2e-16 *** ## inc 0.398235 0.007284 54.67 &lt;2e-16 *** ## age 0.384711 0.008255 46.60 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 90693 on 73032 degrees of freedom ## Residual deviance: 85681 on 73030 degrees of freedom ## AIC: 85687 ## ## Number of Fisher Scoring iterations: 4 A couple of takeaways: The coefficients reported here are from the linear part of the model. You want to focus on the sign of the coefficients And again you can get standard errors and p-values The AIC number is a fit statistic, and we want it to be small (but hard to interpret) We can use the same predict approach we used before. If we don’t specify new data, it generates “in sample” predictions. You want to use the option type to make sure you get the predicted probability part back and not the linear part. Model1preds&lt;-predict(Model1, type=&quot;response&quot;) Let’s look at the predicted probabilities for each value of Age in the dataset. boxplot(Model1preds~turnout$inc, xlab=&quot;Income&quot;, ylab=&quot;Predicted Probabilities&quot;) Now it’s your turn: Fit your own linear classifier to this data using the data I’ve provided. You might try some new variables or even recoding a covariate if you want. Look at the output and make sure you understand what the coefficients are telling you (more or less). 11.2.2.3 Fit statistics When we talked about regression, I emphasized the importance of out-of-sample testing to keep from “overfitting the data.” Complex models might seem to fit the data well, but do poorly out of sample. Ths reflects a model that is confusing the “systematic” part of the data with the “error” part of the data. I introduced RMSE as a metric. But what should we use for binary cases? There are a lot of choices here, and this is again a large topic I will only touch on. The easiest place to start is a “confusion matrix” \\[\\begin{array}{lcc} &amp; \\text{Truth=0} &amp; \\text{Truth=1}\\\\ \\text{Prediction = 0} &amp; \\text{True negatives} &amp; \\text{False negatives}\\\\ \\text{Prediction = 1} &amp; \\text{False positives} &amp; \\text{True Positives}\\\\ \\end{array}\\] The main diagnonal of this matrix are your correct predictions. Should you weight positives or negatives more? Depends on the context You will want to balance or weight. One appproach is to look at precision and recall Precision \\[\\frac{\\text{True positives}}{\\text{True positives + False positives}}\\] Recall \\[\\frac{\\text{True positives}}{\\text{True positives + False negatives}}\\] You can also recombine these in a bunch of ways to get things like false discovery rate, specificity, sensitivity, F1 scores, and more. Another one is called the Brier Score, which is just RMSE. Let \\(p_i\\) be our predicted probabilty for unit \\(i\\). \\[\\sqrt{\\frac{\\sum_i^n{(y_i-p_i)^2}}{n}}\\] Going back to turnout: binaryPred&lt;-(Model1preds&gt;0.5)*1 table(binaryPred, turnout$turnout) ## ## binaryPred 0 1 ## 0 4283 3809 ## 1 18523 46418 This is the confusion matrix where columns are the “truth” and rows are the predictions Using this table and the formulas above, we can calculate the precision, recall, and accuracy (the percent of observations this model gets right) 11.2.3 More models Of course, logistic regression is only one way to build \\(f(X)\\). There are many more. Today I’ll introduce two pretty easy ones: Tree models (sometimes called decision trees) K-nearest neighbors Others you might look into include support vector machines, naive Bayes classifiers, neural networks, Gaussian process regression, and much, much, more. If you’re interested in learning about these, there’s tons of information on the internet, or you can take a introductory Machine Learning course. 11.2.3.1 Tree models map Here’s some code that will create a tree model for us. library(rpart) equation&lt;-as.formula(&quot;turnout~eth+inc+age&quot;) tree_mod1&lt;-rpart(equation, data=turnout) tree_mod1 ## n= 73033 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 73033 15684.380 0.6877302 ## 2) inc&lt; 2.5 26982 6555.365 0.5839449 ## 4) age&lt; 2.5 11531 2881.491 0.4895499 * ## 5) age&gt;=2.5 15451 3494.450 0.6543913 * ## 3) inc&gt;=2.5 46051 8668.089 0.7485397 ## 6) age&lt; 1.5 8495 2044.887 0.5963508 * ## 7) age&gt;=1.5 37556 6381.940 0.7829641 * This tells us how the tree is constructed Better to look at it visually plot(tree_mod1) text(tree_mod1, use.n=TRUE, all = TRUE, cex=0.8) Tuning parameters: Many ML models have tuning parameters, sometimes called hyperparamaters or hyperpriors. These are parameters that control how the function, but which typically are not estimated from the data. These are set by the analyst, often using some sort of cross-validation. Most often, these parameters control model complexity. tree_mod2&lt;-rpart(equation, data=turnout, control=rpart.control(cp=.0002)) plot(tree_mod2) text(tree_mod1, use.n=TRUE, all = TRUE, cex=0.7) Complex or simple: treePreds1&lt;-predict(tree_mod1) treePreds2&lt;-predict(tree_mod2) Compare the accuracy of these two models using a confusion matrix. Do the same use the Brier score Now make your own tree using a different value of cp. 11.2.4 Random forests These models sometimes do not end up performing very well. Especially when the relationship between variables is smooth and not discontinuous. And they can be very sensitive to seemingly small changes to the data. 11.2.4.1 Intuition for random forests One idea is to take advantage of this sensitivity by bootstrapping the sample to create many trees and average their predictions for each unit. Does better with smooth, additive functions, and on the whole less sensitive to small changes in the data. The model will: Randomly select rows of the data with replacement. Randomly select mtry variables from the dataset Build a tree Repeat this ntree times and average the results 11.2.4.2 Implementation for random forests library(randomForest) turnout$turnout&lt;-as.factor(turnout$turnout) # Leave as continuous for regression mod1_forest&lt;-randomForest(equation, data=turnout, ntree=201, mtry=2) mod1_forest # This confusion matrix is &quot;out of bag&quot; ## ## Call: ## randomForest(formula = equation, data = turnout, ntree = 201, mtry = 2) ## Type of random forest: classification ## Number of trees: 201 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 30.17% ## Confusion matrix: ## 0 1 class.error ## 0 3780 19026 0.83425414 ## 1 3009 47218 0.05990802 11.2.5 K Nearest Neighbors Another simple approach is just to look at observations that are “near” eachother. “Near” here just means that they have a similar in terms of their predictor variables. map 11.3 Putting that in action Weird problem in this data is that there can be too many ties. So many observations share the same value. So (just for this example) I’ll add a bit of noise library(class) turnoutX&lt;-turnout[,c(&quot;eth&quot;, &quot;inc&quot;, &quot;age&quot;)] turnoutX$inc&lt;-turnoutX$inc+rnorm(length(turnoutX$inc), 0, .001) mod1_knn&lt;-knn(turnoutX, test=turnoutX, cl=turnout$turnout, k=10) table(mod1_knn, turnout$turnout) ## ## mod1_knn 0 1 ## 0 8234 5133 ## 1 14572 45094 "],
["section-causality.html", "12 Causality 12.1 Correlation and causation 12.2 A formal definition 12.3 Formal presentation of an ATE 12.4 Estimating ATE 12.5 Experiments: A/B Testing 12.6 Panel data and fixed effects 12.7 Difference-in-differences", " 12 Causality 12.1 Correlation and causation What is causality? Understand how social scientists define causation Understand the fundamental problem of causal inference Understand why experiments are social scientists’ ideal method of determining causation Understand the term “average treatment effect” Understand the difficulties social scientists face when using correlation in the absence of an experiment to establish causality. Understand how to estimate the causal effect of a (simple) experimental treatment. Get a sense of how this can all get a lot more complicated. What’s the big deal here? In many cases we are not interested in describing or predicting. We are interested in explaining. Most of the sort of “why” questions we ask are fundamentally about causal inference. Does income inequality cause political polarization? Does being democratic make a country less likely to go to war with its neighbors? Does being appointed by a Republican make a judge more likely to vote consistent with conservative policies? So what, exactly, is causation? This is actually a deep question, and philosophers still disagree about it. But contemporary social science has partially converged on a particular definition often attributed to Rubin/Neyman/Holland. Sometimes also called the “potential outcomes” framework. The parable of the the accident: Imagine you’re driving a car down a four-lane road. All of the sudden, out of nowhere, a car drives out of its lane, into yours, right in front of your car. You can tell that the driver is texting, and isn’t looking at the road. You slam on the breaks, and your hood collides with his fender. Fortunately, you’re shaken, but fine, and so is the other driver. Chances are, you will be asking yourselves some questions. Why did that happen? What would have happened if I had been paying more attention to the cars around me? What if that driver hadn’t been texting? These are causal questions. What caused our accident? The other driver? Or maybe the fact the other driver was texting? The sky was blue? Some can be ruled out by other knowledge. One plausible causal argument is that if the driver had not been texting, the accident would not have happened. This is the kind of reasoning we use when we say that texting while driving caused the accident. We are comparing what happened (what we observed) to the counterfactual (what would have happened). 12.2 A formal definition We say that X caused Y if Y would not have occured but for X. So we are imagining two worlds: A world where X happens and then we observe Y. A world where X does not happen and then we observe Y. These are the “potential outcomes.” And the difference between potential outcomes is the causal effect of X. Go lose yourself in the philisophical background of this debate: https://plato.stanford.edu/entries/causal-models/ Example: Did the stimulus package work? In 2009, Congress passed a large stimulus package to address the 2008 financial crisis. Many question whether it really had an effect on the economy or unemployment rates. What was the causal effect of the stimulus? To answer this, we would need to observe two things: GDP growth/unemployment in the presence of the stimulus package. GDP growth/unemployment in the absence of the stimulus package. The difference between these two is the causal effect of the stimulus. The fundamental problem of causal inference Well, there’s a pretty big problem here: we only get to observe one timeline. The stimulus was passed, and we can never know (without some additional assumptions) what would have happened in its absence. A huge literature has developed around trying to make clear causal/reasonable/appropriate causal statements while making as few assumptions as possible. Moving away from the personal One approach is to shift to move away from trying to estimate the causal effect of X on all units. Instead we estimate the average treatment effect (ATE) or one of its cousins. The ATE is an estimate. We will talk later about how to estimate it. The ATE is the average difference in potential outcomes across all of your units. You still can’t observe it, but it is easier to approximate (estimate) it from the data you observe. 12.3 Formal presentation of an ATE Imagine we have a single covariate \\(T\\) that can take on two values \\(T \\in \\{0, 1\\}\\). \\[\\begin{array}{l c c c} \\text{Unit} &amp; y_0 &amp; y_1 &amp; y_1-y_0 \\\\ \\hline 1 &amp; 2.4 &amp; 3.2 &amp; 0.8 \\\\ 2 &amp; 42.3 &amp; 40.1 &amp; -2.2\\\\ 3 &amp; 13 &amp; 16.4 &amp; 2.4\\\\ \\vdots &amp; &amp; &amp; \\vdots\\\\ n &amp; -1.2 &amp; -1.8 &amp; -0.6 \\end{array}\\] The average causal effect would be (under reasonable assumptions): \\[\\text{mean}(y_1-y_0) = \\text{mean}(y_1) - \\text{mean}(y_0)\\] The problem is that when we get a real dataset it’s going to look like this: \\[\\begin{array}{l c c c} \\text{Unit} &amp; y_0 &amp; y_1 &amp; y_1-y_0 \\\\ \\hline 1 &amp; ? &amp; 3.2 &amp; ? \\\\ 2 &amp; ? &amp; 40.1 &amp; ?\\\\ 3 &amp; 13 &amp; ? &amp; ?\\\\ \\vdots &amp; &amp; &amp; \\vdots\\\\ n &amp; -1.2 &amp; ? &amp; ? \\end{array}\\] We will always be missing one of the counterfactual outcomes. And the goal of all of almost all approaches to causal inference is to fill in the missing values on this table. 12.3.1 Danger! So it turns out that doing this well is pretty hard. Why? Units where \\(T=0\\) might be fundamentally different than those where \\(T=1\\). There might be a “lurking variable” that is correlated with both \\(T\\) and \\(Y\\). We can’t really measure for all possible lurking variables. 12.3.2 Does drinking make you more educated? Education \\(\\rightarrow\\) Spending more on alcohol Spending more on alcohol \\(\\rightarrow\\) Education Income? 12.3.3 Problem: It can be way more subtle (Mount 2010) Rogers, Coffman, and Bergman: While the authors control for certain variables, their research only implies there is a relationship between parental involvement and student performance. This caveat is important; the existence of a relationship does not tell us what causes what. Think of it this way: If you had two children, and one was getting A’s and the other C’s, which of them would you help more? The C student. An outsider, noticing that you’ve spent the school year helping only one of your children, might infer that parental help caused that child to earn lower grades. This of course would not be the case, and inferring causation here would be a mistake. 12.4 Estimating ATE We’ve talked about how we can’t know the causal effect of an intervention on a single unit. And we’ve talked about the ATE. But how would we go about estimating it? \\[\\begin{array}{l c c c} \\text{Unit} &amp; y_0 &amp; y_1 &amp; y_1-y_0 \\\\ \\hline 1 &amp; ? &amp; 3.2 &amp; ? \\\\ 2 &amp; ? &amp; 40.1 &amp; ?\\\\ 3 &amp; 13 &amp; ? &amp; ?\\\\ 4 &amp; ? &amp; 18.3 &amp; ?\\\\ 5 &amp; 4.1 &amp; ? &amp; ?\\\\ 6 &amp; -5.1 &amp; ? &amp; ?\\\\ \\end{array}\\] 12.4.1 Filling in the potential values We are left with essentially two options (with shades of gray in between) We can fill in the missing values (or average values of missing values) of our table using a model. We can try to mine our data in some way to infer values for missing values (or averages of missing values). Mabe we could put extreme bounds on them based on possible (or plausible) values. Maybe even rely on other dependent variables in large/complex systems. We can use our research design. We can rely on randomization We can rely on “natural experiments” that help us fill in the missing counterfactuals. 12.4.2 Good causal inference \\[\\begin{array}{l c c c} \\text{Unit} &amp; y_0 &amp; y_1 &amp; y_1-y_0 \\\\ \\hline 1 &amp; ? &amp; 3.2 &amp; ? \\\\ 2 &amp; ? &amp; 40.1 &amp; ?\\\\ 3 &amp; 13 &amp; ? &amp; ?\\\\ 4 &amp; ? &amp; 18.3 &amp; ?\\\\ 5 &amp; 4.1 &amp; ? &amp; ?\\\\ 6 &amp; -5.1 &amp; ? &amp; ?\\\\ \\end{array}\\] An attractive approach here is to try something like this. Take the average of all of the cases where \\(T=1\\) Take the average of the cases where \\(T=0\\). Subtract them. \\[\\frac{3.2 + 40.1 + 18.3}{3} - \\frac{13+4.1-4.1}{3}\\] Can we use this to estimate the ATE? Yes, but only if: Assignment of T is unrelated to potential outcomes. Units don’t affect each other No hidden variation in treatment Hard to do in a lot of settings, but we’ll talk about this more next class. But much easier to do if we use our old friend randomization. 12.5 Experiments: A/B Testing Imagine that we had randomly assigned each unit to a treatment/control condition. Now we know for certain that T and the potential outcomes are independent. We can also take steps so that the treatment does not vary in intensity across units (?). And in many cases is easy to assure no between-unit contact. This all makes causal inference (relatively) easy, and is why randomized controlled trials are considered the gold standard. Example: Audit studies for employment resume&lt;-read.csv(&quot;../Datasets/resume.csv&quot;) Actual link: http://politicaldatascience.com/PDS/Datasets/resume.csv This study sent out resumes for job applications. Everything was the same, except for the first name of the applicant. Names were chosen to signal race and gender e.g., Aisha, Allison, Hakim, Brad, Rasheed, Todd, Tanisha, Jill, etc. Question: Does the perceived race/gender of the applicant affect the probability that they receive a call? Dataset includes four variables: Name: firstname Implied gender: sex Race: race Binary indicator of whether receivd a call: call So what was the causal effect? tapply(resume$call, INDEX=resume$race, FUN=mean) ## black white ## 0.06447639 0.09650924 A little over 3%. Do this a little easier with: summary(lm(call~race, data=resume)) ## ## Call: ## lm(formula = call ~ race, data = resume) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.09651 -0.09651 -0.06448 -0.06448 0.93552 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.064476 0.005505 11.713 &lt; 2e-16 *** ## racewhite 0.032033 0.007785 4.115 3.94e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2716 on 4868 degrees of freedom ## Multiple R-squared: 0.003466, Adjusted R-squared: 0.003261 ## F-statistic: 16.93 on 1 and 4868 DF, p-value: 3.941e-05 summary(lm(call~race+sex, data=resume)) ## ## Call: ## lm(formula = call ~ race + sex, data = resume) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.09866 -0.09866 -0.06653 -0.06653 0.94259 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.066534 0.005886 11.304 &lt; 2e-16 *** ## racewhite 0.032130 0.007786 4.127 3.74e-05 *** ## sexmale -0.009128 0.009239 -0.988 0.323 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2716 on 4867 degrees of freedom ## Multiple R-squared: 0.003666, Adjusted R-squared: 0.003256 ## F-statistic: 8.953 on 2 and 4867 DF, p-value: 0.0001314 12.5.1 What to do when you can’t experiment: We want to measure the causal effect of some variable \\(T\\) on outcome \\(Y\\). But what if the cases where \\(T=0\\) are fundamentally different from those where \\(T=1\\)? Just comparing \\(\\bar{Y} | T=1\\) to \\(\\bar{Y} | T=0\\) will not give us the causal effect of \\(T\\). More technically, for this to be a valid approach we need (at least): \\[(Y_0, Y_1) \\perp T\\] - So what do we do when this isn’t true? We are going to assume: \\[(Y_0, Y_1) \\perp T | X\\] How to do that? Matching, some forms of marginal structural models, and more. The simplest, though, is to just “control for” possible confounding. \\[Y=\\beta_0 + \\beta_1 T + \\beta_2 X + \\epsilon\\] \\[\\epsilon \\sim N(0, \\sigma^2) \\] This should look familliar – multiple linear regression. But if we have all of the right covariates and if we have the right model, then \\(E(\\beta_1) = ATE\\). 12.5.1.1 Example: New Hampshire 2008 primary: Clinton vs. Obama library(faraway) data(newhamp) N=276 wards in New Hampshire pObama: Proportion of vote going to Obama votesys: Hand counted (H) vs. machine count (D) Dean: Proportion of voters for Howard Dean in 2004 pci: Per capita annual income in USD white: Proportion white according to 2000 census Your data science friend says IT’S ALL RIGGED! summary(lm(pObama~votesys, data=newhamp)) ## ## Call: ## lm(formula = pObama ~ votesys, data = newhamp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.172147 -0.047643 -0.004519 0.040363 0.229107 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.352517 0.005173 68.148 &lt; 2e-16 *** ## votesysH 0.042487 0.008509 4.993 1.06e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.06823 on 274 degrees of freedom ## Multiple R-squared: 0.0834, Adjusted R-squared: 0.08006 ## F-statistic: 24.93 on 1 and 274 DF, p-value: 1.059e-06 Is this the effect of the voting machines? library(tidyverse) newhamp %&gt;% group_by(votesys) %&gt;% summarize(White = mean(white, na.rm=TRUE), Income=mean(pci, na.rm=TRUE), Dean = mean(Dean, na.rm=TRUE)) ## # A tibble: 2 x 4 ## votesys White Income Dean ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 D 0.954 23367. 0.251 ## 2 H 0.976 22054. 0.342 Probably not summary(lm(pObama~votesys + Dean + white + pci, data=newhamp)) ## ## Call: ## lm(formula = pObama ~ votesys + Dean + white + pci, data = newhamp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.156310 -0.034675 -0.001047 0.028146 0.216232 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.530e-01 1.150e-01 2.200 0.0287 * ## votesysH 6.650e-03 7.647e-03 0.870 0.3853 ## Dean 4.925e-01 3.950e-02 12.468 &lt; 2e-16 *** ## white -1.325e-01 1.200e-01 -1.105 0.2703 ## pci 4.373e-06 6.685e-07 6.542 3.02e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.05062 on 271 degrees of freedom ## Multiple R-squared: 0.501, Adjusted R-squared: 0.4937 ## F-statistic: 68.03 on 4 and 271 DF, p-value: &lt; 2.2e-16 congress &lt;- read.csv(&quot;https://jmontgomery.github.io/ProblemSets/incumbents.csv&quot;) This is a dataset of incumbent members of congress running for election. The dependent variable of interest is voteshare, which is the percent of the two-party vote received by the incumbent. Run a regression with voteshare as the dependent variable and incspend as the explanatory variable. incspend is the amount of money spent by the candidate in the election. What does the regression result mean? That is, what would be the naive causal interpretation? Is there imbalance in incspend depending on the quality of the challenger (challquality). What does that mean for your causal inference? Use the variables chalspend (spending by the challenger), presvote (share of vote going to the candidates’ presidential candidate), and chalquality to try and “adjust”. 12.6 Panel data and fixed effects As the Congress example illustrates, there is a fundamental limit to this kind of strategy. We can’t always control for everything. And even if we could, results can become highly model dependent. Another approach is to try and collect data on the same units over time. We can then try and “control” for factors (even unobserved factors) that make each unit different from the others using fixed effects. This is just a fancy way to say we are going to have a different intercept for each unit in our data. Requires we focus only on treatments that change over time. If treatment doesn’t change (much) will give us a false negative. 12.6.1 Example: Are elections fought over non-economic issues more when globalization increases? Panel data includes repeated observations of Canada, France, the United States and the United Kingdom. We have measures of (a) the degree to which each election is about non-economic issues and (b) level of globalization in the country. We want to see if globalization causes more focus on non-economic issues. With no fixed effects: What does that mean? Each dot is an election. Across these four countries, there is no relationship. But what if allowed for country-level fixed effects? The US and France may just be different on average in how much politics focuses on economics? So we have an “intercept shift” With fixed effects: 12.6.2 Naive regression library(AER) data(&quot;Fatalities&quot;) Fatalities$fatal_rate &lt;- Fatalities$fatal / Fatalities$pop * 10000 summary(lm(fatal_rate ~ beertax, data=Fatalities)) ## ## Call: ## lm(formula = fatal_rate ~ beertax, data = Fatalities) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.09060 -0.37768 -0.09436 0.28548 2.27643 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.85331 0.04357 42.539 &lt; 2e-16 *** ## beertax 0.36461 0.06217 5.865 1.08e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5437 on 334 degrees of freedom ## Multiple R-squared: 0.09336, Adjusted R-squared: 0.09065 ## F-statistic: 34.39 on 1 and 334 DF, p-value: 1.082e-08 12.6.3 Fixed effects The term that allows for the fixed effects in the lm call is the \\(-1\\) model2&lt;-summary(lm(fatal_rate ~ beertax + state - 1, data=Fatalities)) head(model2$coefficients) ## Estimate Std. Error t value Pr(&gt;|t|) ## beertax -0.6558737 0.18784999 -3.491476 5.559697e-04 ## stateal 3.4776301 0.31335679 11.097989 4.751833e-24 ## stateaz 2.9099032 0.09253893 31.445179 5.672947e-95 ## statear 2.8226785 0.13212532 21.363646 2.902574e-61 ## stateca 1.9681613 0.07400678 26.594335 2.045800e-79 ## stateco 1.9933497 0.08037085 24.801898 2.397281e-73 12.7 Difference-in-differences Difference-in-differences (DID) is an empirical strategy used to make claims of causal inference. A researcher considers some intervening factor (the introduction of a policy, a stock market shock, a terrorist attack, etc.) and seeks to prove that factor to be the cause of some outcome (bank closures, casualties, vote share, etc.). Let’s consider the task of estimating the effect of closing early-voting voting precincts in 17 counties in North Caroilna had on turnout in 2016. Identify the population (A) to which the intervention was applied and identify a comparable population (B) to which the intervention was NOT applied. In our example, we would divide all 100 counties in North Carolina into group A, counties where early voting precincts were reduced, and group B, counties where the number of early voting precincts was kept the same. Identify the outcome of interest of both populations before and after the intervention (A-pre, A-post, B-pre, B-post). This means we need to know the turnout rate for all counties in group A before the reform (e.g., in 2012) and after the reform (e.g., in 2016). Likewise, we need to calculate the turnout rate in for counties in group B before the reform (2012) and after the reform (2016). Find D1: the difference between A-pre and B-pre. This would be difference in the average turnout rate between group A and group B in 2012. Find D2: the difference between A-post and B-post. This would be the difference in the average turnout rate between group A and gropu B in 2016. Calculate D2 minus D1 to find the difference-in-differences (DD). This would be a measure of how the differences between turnout rates in group A and B changed as a result of the closed precincts. Here’s a visual illustration of how this research strategy works: 12.7.1 DID in practice A series of train bombings in Madrid on March 11, 2004 left 191 people dead and 1,500 injured. In the Spanish congressional elections which took place three days later, the Socialist party defeated the Conservative party. Did the terrorist attack increase the Socialist party’s vote share? Identifying our treatment/control group: Voters living outside of Spain voted earlier, while Spanish residents voted on election day. These groups are not the same, but could we assume a parallel trend? Are they consistently different? How to do it? Respndent ID# Voted Conservative (Y) Treatment group (X1) Post-treatment (X2) Treatment AND Post (X1 x X2) 1001 0 0 0 0 1002 0 1 1 1 1003 1 1 1 1 1004 0 1 0 0 1005 1 0 1 0 1006 1 0 0 0 1007 0 0 1 0 1008 1 0 1 0 \\[Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3(X_1 \\times X_2) + \\epsilon\\] One variable indicates whether or not respondents are affected by the treatment. One variable indicates whether pre-post treatment. One variable is the interaction. This is our treatment effect Effect of the treatment on the treated (ATT) 12.7.2 Regression discontinuity In many cases interventions occur at a discontinuity. Maybe only students above a specific score get free college? Maybe only politicians who win the most votes get into office? Regression discontinuity seeks to: Compare respondents just above this cutoff to Individuals just below this cutoff 12.7.2.1 Example: Is voting a habit? Some have argued that voting can be habitual Prof. Marc Meredith tested this using data from California. He looked at the turnout rates in the 2006 election for people around the voting eligibiltiy threshold for the 2004 election. So how to do that? Well … that’s not so easy. In essence we want to compare means just to the left and just to the right. But what if there is a “trend” on the left/right? A more advanced class would take you through this. But, in essence, fit a “smooth” function to the left and right. A simpler version \\[y= \\beta_0 + \\beta_1 X + \\beta_2*I(X&gt;D) + \\epsilon\\] \\(I(x)\\) is an indicator function. That means that it has the value 1 when \\(x\\) is true and 0 otherwise. This is fitting single regression where \\(y\\) is a funciton of \\(x\\). BUT, you allow an intercept shift for before/after the discontinuity. The basic components: \\(Y\\): An outcome variable (violence at \\(t+1\\)) \\(X\\): A variable that determines the cutoff (violence at \\(t\\)) \\(D\\): A cutoff/threshold Does the relationship between \\(Y\\) and \\(X\\) “shift” right at \\(D\\)? \\[y= \\beta_0 + \\beta_1 X + \\beta_2*I(X&gt;D) + \\epsilon\\] Machine learning and statistics is really just a subset of artificial intelligence (AI) research. For fun, just start telling people you are in an AI class and see how they react.↩︎ "]
]
