---
title: "Supervised Learning: Introduction and Basic Concepts"
author: |
  | Jacob M. Montgomery
  | *Washington University in St. Louis*
  | *Department of Political Science*
date: '2020'
header-includes:
  - \usepackage{amsmath}
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      countIncrementalSlides: false
#   beamer_presentation: default
#   slidy_presentation: default


---

<style type="text/css">
body, td {
   font-size: 14px;
}
code.r{
  font-size: 14px;
}
pre {
  font-size: 4px;
}
</style>


## Orientation for this component

Last time


1. We talked about basic ideas of probability theory as applied to machine learning

--

This time

1. Introduction to machine learning:
    + Basic idea
    + Basic problems
    + Example: Prediction presidential elections
    + Signal or noise?: Cross validation

--

Next time

1. Supervised learning 2
    + Classification
    + Fit statistics/Diagnostics
    + Trees, neighbors, nets, and ensembles


---
## Finding a function

- We have some outcome, $y$, we are interested in modeling.
    - Election outcomes
    - Political attitudes
    - Economic growth

--
    
- We have some set of variables (or features), we'll call it $X$, related to $y$.
    - Elections $\leftarrow$ polls, incumbency, fundraising
    - Political attitudes  $\leftarrow$ demographics, news sources, party ID
    - Economic growth $\leftarrow$ business confidence, savings rate, productivity


---

- More formally, we say that we want to model $y$ as a **function** of $X$.
    - $y$: Outcome or *dependent* variable
    - $x$: Predictor or *independent* variable
    - $f(\cdot)$: The function

- Formal statement: 

$$y \sim f(x)$$


---
## What's a function?


- Let's not get scared by terminology

- You have been working with functions for most of your life.
    - $y = a + bx$
    - $y = log(x)$
    - $y = a + bx + cx^2$
    - $y = \beta_0 + \beta_1x_1 + \beta_2x_2$

---
##OK, but why are we doing this?

The are probably as many purposes for supervised learning as there are projects. But here are some broad categories:

1. Prediction
2. Feature selection
3. Exploration
4. Theory testing (?)


---
##Prediction

1. Collect outcomes $y$ and predictors $X$. This is called a "training sample."
2. Estimate $f_x(\cdot)$.  The sub-script indicates the model was estimated with the training sample.
3. Collect predictors for "new" observations $X^\prime$.
4. Predict "new" observations as:

$$y^\prime \sim f_x(X^\prime)$$

<br>
<br>


--

> Example: Can we predict who will be president in 2020?


---
##Feature selection

1. Collect outcomes $y$ and predictors $X$.
2. Estimate a different $f(\cdot)$ for different subsets of variables.
3. Use some criteria/algorithm to see what "features" matter.
4. Use this information for inference or further investigation.


<br>
<br>

--

> Example: Is the condition of the economy an important predictor of whether incumbents get re-elected?


---
## Exploration

1. Collect outcomes $y$ and predictors $X$.
2. Estimate a different $f(\cdot)$ using lot's of variables.
3. Find some patterns in the data.
4. Use that to develop theory for later testing.



<br>
<br>

--

> Example: Do voters punish incumbents for a bad economy?  Or do they only care about their own pocketbook?


---
## Summary

1. We want to build some $f(X)$ that explains/predicts/correlates with observed outcome $y$.

--

2. A function, $f(X)$, can be something simple like a line or something more complex.

--

3. We are going to use this function to:
    * Predict
    * Identify elements of $X$ that seem important
    * Explore interesting relationships
    * Maybe for theory testing, but will need to be careful.



---

## End Part A



---
## Sounds good, so what's the problem?


**Problem 1**: Infinity is a big number
- The are an infinite number of potential functions, $f(\cdot)$.
- We can't try all possible functions.  That problem isn't clearly defined.
    

--

**Problem 2**: Not enough data
- Even if we knew a subset of $f(\cdot)$ to consider, we may not have enough data
- If $f(\cdot)$ is complex, can be particularly hard to approximate unless large $n$

--

**Problem 3**: What are the right features?
- Even if we have some idea of $f(\cdot)$ and a lot of data, we don't always know the right features to include.
- And in some cases there are *a lot* of features.

---

**Problem 4**: Is it signal or is it error?
- A lot of outcomes we want to study are "noisy"
- We are usually not interested in the noise
- One way to think of this is that $f(\cdot)$ can be divided into two compoenents
    * Systematic component
    * Error component
   


Example: The linear regression
    
$$f(X) = \underbrace{\beta_0 + \beta_1x_1 + \beta_2 x_2}_{systematic} + \underbrace{\epsilon}_{error}$$
$$\epsilon  \sim N(0, \sigma^2)$$

---

**Problem 5**: Putting it all together  

- We don't know if we have the right "set" of functions to consider. 
- Even if we did, we don't have infinite data.
- And we don't even know if we are using the right features.
- So we can't ever be sure we are separating out the systematic and error portions.

--

**Problem 6**: Meta problems
- In many settings, the DGP is not static.
- There may be unknown unknowns.
- It is difficult or impossible to know if the data used to train your model is useful for the task at hand.


---
##All I heard was womp, womp, womp

So what does it all mean?

**Problem 1**: Too many options for $f(\cdot)$

> Basic approach: Assume the DGP can be represented as some subset of all possible functions (e.g., a line).

--

**Problem 2**: Not enough data

**Problem 3**: What are the right features?

**Problem 4**: Is it signal or is it error

> Basic approach: Use cross-validation or related methods to build models that are fitting signal rather than noise.  

--

**Problem 6**: Meta problems


> Basic approach: Humility.



---


## End Part B



---


## Example: Predicting presidential elections with vote share

Today we are going to use the results of US presidential elections since 1948
```{r, eval=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
electData<-read.csv("http://politicaldatascience.com/PDS/Datasets/presElect.csv")
```

- **Year**: Year of the election
- **q2gdp**: GDP in the second quarter
- **vote**: Share of the two-party vote that went to the **incumbent party.**
- **term**: 1=Incumbent party has served more than one term; 0 = First term for incumbent party
- **JuneApp**: Approval as recorded in June prior to the election.
- **Inc**: Indicator if the **incumbent party** candidate is the current incumbent (meaning they are a first-term incument).

---

##Fancy machine learning: Linear regression

```{r, eval=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.width=12}
par(mar=c(2,2,.5,.5), mgp=c(1,0,0), tcl=0, cex.lab=1)
plot(electData$q2gdp, electData$vote, ylab="Incumbent party share of vote", xlab="Q2 GDP Growth", ylim=c(44, 64), pch="", xlim=c(-8, 10.5), cex=.8)
abline(h=seq(45, 65, by=5), col="gray80")
abline(lm(electData$vote~electData$q2gdp), lwd=2)
points(electData$q2gdp[1:17], electData$vote[1:17], ylab="Incumbent party share of vote", xlab="Q2 GDP Growth", ylim=c(44, 64), pch=19, xlim=c(-8, 10.5), cex=.8)
text(electData$q2gdp[1:17]+.65, electData$vote[1:17], electData$year[1:17], cex=1.2)
points(electData$q2gdp[18], electData$vote[18], ylab="Incumbent party share of vote", xlab="Q2 GDP Growth", ylim=c(44, 64), pch=19, xlim=c(-8, 10.5), cex=.8, col="red")
text(electData$q2gdp[18]+.65, electData$vote[18], electData$year[18], cex=1.2, col="red")
```

---

##So how do we do it?

```{r, eval=TRUE, message=FALSE, warning=FALSE, echo=TRUE}
Model1<-lm(vote~q2gdp, data=electData)
summary(Model1)
```


---
## Inuition for the linear model

    
$$f(X) = \underbrace{\beta_0 + \beta_1x_1 + \beta_2 x_2}_{systematic} + \underbrace{\epsilon}_{error}$$
$$\epsilon  \sim N(0, \sigma^2)$$

- The "Multiple R-squared" is a fit statistic.
    * Ranges from 0 to 1
    * Closer to 1 is better

--

- The "Estimate" is the coefficient

- The "Std. Error" is what we talked about in our previous lecture and is used to construct confidence intervals.

--

- Smaller p-values mean there is more evidence that that specific variable matters (sort of)


---

## Example: Prediction


```{r, eval=TRUE, message=FALSE, warning=FALSE, echo=TRUE}
electData$vote[electData$year==2016]
Model2<-lm(vote~q2gdp+JuneApp, data=electData[electData$year!=2016,])
predict(Model2, newdata=electData[electData$year==2016,])
```

Pretty good!


---

## Example: Feature selection


```{r, eval=TRUE, message=FALSE, warning=FALSE, echo=TRUE}
Model3<-lm(vote~q2gdp+JuneApp, data=electData)
summary(Model3)
```

---

- R-squared for the simpler model was 0.366
- Adding June approval bumps it to 0.752 -- way bigger


--


What does that mean? Maybe not much.




---

## Example: Exploration


```{r, eval=TRUE, message=FALSE, warning=FALSE, echo=TRUE}
Model4<-lm(vote~JuneApp+Inc, data=electData)
summary(Model4)
```


---


- So that's interesting.
- When a party is running for a second term, they do better.
    - Yes: Eisenhower, Kennedy, Nixon, Reagan, Clinton, Bush II, Obama
    - No: Carter
- Might be worth looking into more.


---

## Theory testing?


- Is this data by itself evidence that incumbent candidate is always at an advantage?
- Not really that convincing.  Many other explanations.

---

## Your turn

1. Subset the data as I did above and fit a linear model of your own.
2. Make a prediction for 2016.  How did you do?
3. Put your answer in slack. 



---



## End Part C


---

## The complexity of keeping it simple


- Simple models can be good, especially with small samples.

--

- But more complex models *might* be better:
    * Maybe a more flexible $f(\cdot)$ than a line?
    * Maybe more options for $X$?

--

- We want to aim for models that are:
    * Complex enough to capture important aspects of reality
    * Not so complex they confuse signal with noise


---

## A new example

```{r, eval=TRUE, message=FALSE, warning=FALSE}
library(readr)
senateData<-read_csv("http://politicaldatascience.com/PDS/Datasets/SenateForecast/CandidateLevel.csv")
```

This is data on US Senate elections from 1992-2016.
- **VotePercentage**: Percentage of the vote for that candidate
- **Republican**: 1=Republican, 0=Any other
- **Democrat**: 1=Democrat, 0=Any other
- **Experienced**: 1=Candidate has held elected office, 0=otherwise
- **weightexperience**: 1 = no experience, 4=held statewide office
- **pvi**: Presidential vote index (Higher values mean more friendly to Democrats)
- **Generic Ballot**: Generic ballot polling for that candidate's party in that year
- **Incumbent**: -1 = Running against incument, 0=open seat, 1 = Is the incumbent 
- **PercentageRaised**: Percent of money for that race raised by that candidate



---

## Model 1: Simple


```{r, eval=TRUE, message=FALSE, warning=FALSE}
SimpleModelFull<-lm(VotePercentage~pvi*Republican+Incumbent, data=senateData)
summary(SimpleModelFull)$r.squared
```

---

##Model 2: Complex


```{r, eval=TRUE, message=FALSE, warning=FALSE, size=2}
ComplexModelFull<-lm(VotePercentage~pvi*Republican+weightexperience 
                 + GenericBallotSept*Republican + Incumbent, data=senateData)
summary(ComplexModelFull)$r.squared
```

--

Seems a little better


---

## So what's that mean?

- Looks like the complex model is doing much better

- But is that real, or illusory?

--

- We can partially resolve that by doing a **cross validation**

- Several ways to do this, but here is a very easy one.
    - Divide your data so you can fit on your training data
    - And test on your validation data (the data you didn't use to fit the model)


---

## Simple cross validation

```{r, eval=TRUE, message=FALSE, warning=FALSE, size=2}
library(rsample)
split_senateData<-initial_split(senateData, prop=.8)
senate_train<-training(split_senateData)
senate_test<-testing(split_senateData)
```

--

Let's look at those:
```{r, eval=TRUE, message=FALSE, warning=FALSE, size=2}
dim(senate_train)
dim(senate_test)
```




---

### Let's test out the simple model


```{r, eval=TRUE, message=FALSE, warning=FALSE, size=2}
SimpleModelTrain<-lm(VotePercentage~pvi*Republican+Incumbent, data=senate_train)
SimpleModelPredictions<-predict(SimpleModelTrain, newdata=senate_test)
```

Now we will calculate the root mean squared error (RMSE) comparing the predictions, $y^\ast$, with what we actually observed, $y$. 

$$RMSE=\sqrt{\frac{\sum_i^n(y_i^\ast-y_i)^2}{n}}$$

--

```{r, eval=TRUE, message=FALSE, warning=FALSE, size=2}
sqrt(mean((SimpleModelPredictions-senate_test$VotePercentage)^2))
```

---

### Let's do the same for the more complex model

- Fit the model
- Make predictions for the training set

```{r, eval=TRUE, message=FALSE, warning=FALSE, size=2}
ComplexModelTrain<-lm(VotePercentage~pvi*Republican+weightexperience 
                 + GenericBallotSept*Republican + Incumbent, data=senate_train)
ComplexModelPredictions<-predict(ComplexModelTrain, newdata=senate_test)
```


--

```{r, eval=TRUE, message=FALSE, warning=FALSE, size=2}
sqrt(mean((ComplexModelPredictions-senate_test$VotePercentage)^2))
```




---

### More on cross-validation

- One problem here is that the result may be somewhat sensitive to the particular way you partition your data.  Maybe that 20% you pulled out were unusual?
- k-fold validation tries to get around this by:
    - Randomly dividing the data into k groups
    - Each group serves as the test sample once
    - So We have "out-of-sample" predictions for all cases
- You can also do Monte Carlo cross-validation, where you do this 90-10 random partitioning multiple times.    



---


## Summary

- There is a tension between complexity and predictive accuracy
- More complex models may better explain the data you have, but may do worse in prediction.
- Cross validation is a fundamental tool for addressing this dilemma.

---

## Group assignment

1. Make your own model to predict senate races.
2. How does your model do in an out-of sample test compared to my simple model above?  Use RMSE.  And remember that *smaller* values of RMSE are better.
3. Now go get the data at this link and use it to make predictions for 2018 (you will need to change some of the variable names)

http://politicaldatascience.com/PDS/Datasets/SenateForecast/CandidateLevel2018.csv

Use this data to make predicitons for 2018. Add into slack your predictions for: 2018MOMcCaskill, 2018OHBrown, and 2018WVManchin.  Please put your predictions for these races up BEFORE the next scheduled class session.




