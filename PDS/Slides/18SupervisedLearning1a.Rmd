---
title: "Supervised Learning: Introduction and Basic Concepts"
author: |
  | Jacob M. Montgomery
  | *Washington University in St. Louis*
  | *Department of Politcal Science*
date: '2020'
header-includes:
  - \usepackage{amsmath}
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      countIncrementalSlides: false
#   beamer_presentation: default
#   slidy_presentation: default


---


## Orientation for this component

Last time


1. We talked about basic ideas of probability theory as applied to machine learning

--

This time

1. Introduction to machine learning:
    + Basic idea
    + Signal or noise?
    + Example: Regression trees
    + Regularization and the LASSO

--

Next time

1. Supervised learning 2
    + Classificaiton
    + Fit statistics/Diagnostics
    + Trees, neighbors, nets, and ensembles


---
## Finding a function

- We have some outcome, $y$, we are interested in modeling.
    - Election outcomes
    - Political attitudes
    - Economic growth

--
    
- We have some set of variables (or features), we'll call it $X$, related to $y$.
    - Elections $\leftarrow$ polls, incumbency, fundraising
    - Political attitudes  $\leftarrow$ demographics, news sources, party ID
    - Economic growth $\leftarrow$ business confidence, savings rate, productivity


---

- More formally, we say that we want to model $y$ as a *function$ of $X$.
    - $y$: Outcome or *dependent* variable
    - $x$: Predictor or *independent* variable
    - $f(\cdot)$: The function

- Formal statement: 

$$y \sim f(x)$$


---
## What's a function?


- Let's not get scared by terminology

- You have been working with functions for most of your life.
    - $y = a + bx$
    - $y = log(x)$
    - $y = a + bx + cx^2$
    - $y = \beta_ + \beta_1x_1 + \beta_2x_2$

---
##OK, but why are we doing this?

The are probably as many purposes for supervised learning as there are projects. But here are some broad categories:

1. Prediction
2. Feature selection
3. Exploration
4. Theory testing (?)


---
##Prediction

1. Collect outcomes $y$ and predictors $X$. This is called a "training sample."
2. Estimate $f_x(\cdot)$.  The sub-script indicates the model was estimated with the training sample.
3. Collect predictors for "new" observations $X^\prime$.
4. Predict "new" observations as:

$$y^\prime \sim f_x(X^\prime)$$

<br>
<br>


--

> Example: Can we predict who will be president in 2020?


---
##Feature selection

1. Collect outcomes $y$ and predictors $X$.
2. Estimate a different $f(\cdot)$ for different subsets of variables.
3. Use some criteria/algorithm to see what "features" matter.
4. Use this information for inference or further investigation.


<br>
<br>

--

> Example: Is the condition of the economy an important predictor of whether incumbents get re-elected?


---
## Exploration

1. Collect outcomes $y$ and predictors $X$.
2. Estimate a different $f(\cdot)$ using lot's of variables.
3. Find some patterns in the data.
4. Use that to develop theory for later testing.



<br>
<br>

--

> Example: Do voters punish incumbents for a bad economy?  Or do they only care about their own pocketbook?


---
## Sounds good, so what's the problem?


**Problem 1**: Infinity is a big number
- The are an infinite number of potential functions, $f(\cdot)$.
- We can't try all possible functions.  That problem isn't clearly defined.
    

--

**Problem 2**: Not enough data
- Even if we knew a subset of $f(\cdot)$ to consider, we may not have enough data
- If $f(\cdot)$ is complex, can be particularly hard to approximate unless large $n$

--

**Problem 3**: What are the right features?
- Even if we have some idea of $f(\cdot)$ and a lot of data, we don't always know the right features to include.
- And in some cases there are *a lot* of features.

---

**Problem 4**: Is it noise or is it error
- A lot of outcomes we want to study are "noisy"
- One way to think of this is that $f(\cdot)$ can be divided into two compoenents
    * Systematic component
    * Error component
   


Example: The linear regression
    
$$f(X) = \underbrace{\beta_0 + \beta_1x_1 + \beta_2 x_2}_{systematic} + \underbrace{\epsilon}_{error}$$
$$\epsilon  \sim N(0, \sigma^2)$$

---

**Problem 5**: Putting it all together  

- We don't know if we have the right "set" of functions to consider. 
- Even if we did, we don't have infinite data.
- And we don't even know if we are using the right features.
- So we can't ever be sure we are separating out the systematic and error portions.

--

**Problem 6**: Meta problems
- In many settings, the DGP is not static.
- There may be unknown unknowns.
- It is difficult or impossible to know if the data used to train your model is useful for the task at hand.


---
##All I heard was womp, womp, womp

So what does it all mean?

**Problem 1**: To many options for $f(\cdot)$

> Basic approach: Assume the DGP can be represented as some subset of all possible functions (e.g., a line).

--

**Problem 2**: Not enough data

**Problem 3**: What are the right features?

**Problem 4**: Is it noise or is it error

> Basic approach: Use cross-validation or related methods to build models that are fitting signal rather than noise.  

--

**Problem 6**: Meta problems


> Basic approach: Humility.



---


## Get set up

Today we are going to use the 2019 data release from the Voter Study Group

- Download: https://www.voterstudygroup.org/publication/2019-voter-survey-full-data-set
- Codebook: https://www.voterstudygroup.org/publication/2019-voter-survey-full-data-set


```{r, eval=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
VSG<-read_csv("~/Downloads/VOTER_Survey_Jan217_Release1-csv.csv")
```

---

## Recode a bit

```{r, eval=TRUE, message=FALSE, warning=FALSE}
with(VSG, table(fav_biden_2019))
VSG<-VSG %>% 
  mutate(fav_biden_2019=na_if(fav_biden_2019, 8)) %>%
  mutate(fav_biden_2019=na_if(fav_biden_2019, 98))
with(VSG, table(fav_biden_2019))
```

Here is the mean value:

```{r, eval=TRUE, message=FALSE, warning=FALSE}
mean(VSG$fav_biden_2019, na.rm=TRUE)
```


---
